apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/db-migrations-script: 37898f38b90abd06081105d992362ec9e0d0015123b69e758e59031a9e6ddfc9
      checksum/secret-config-envs: 7b5b4b568e7c694c0f44651459436d8876c0d6f337d3876162036cffceec3b68
      checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      cni.projectcalico.org/containerID: 2bae1791283031cdf9eb8ec9c8d655bb57d8b581f4e509c868de6b87bb36a767
      cni.projectcalico.org/podIP: 10.42.133.226/32
      cni.projectcalico.org/podIPs: 10.42.133.226/32
    creationTimestamp: "2024-03-14T05:12:14Z"
    generateName: airflow-db-migrations-77b99788cf-
    labels:
      app: airflow
      component: db-migrations
      pod-template-hash: 77b99788cf
      release: airflow
    name: airflow-db-migrations-77b99788cf-knfp9
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: airflow-db-migrations-77b99788cf
      uid: f80ef275-9f6f-402f-8664-6b360abb2498
    resourceVersion: "13231685"
    uid: 2c991b1a-4dce-4451-9d5d-6c6f718a4f94
  spec:
    affinity: {}
    containers:
    - args:
      - python
      - -u
      - /mnt/scripts/db_migrations.py
      command:
      - /usr/bin/dumb-init
      - --
      - /entrypoint
      env:
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: CONNECTION_CHECK_MAX_COUNT
        value: "0"
      - name: AIRFLOW__METRICS__STATSD_HOST
        value: prometheus-statsd-exporter
      - name: AIRFLOW__METRICS__STATSD_PORT
        value: "9125"
      - name: AIRFLOW__METRICS__STATSD_ON
        value: "true"
      envFrom:
      - secretRef:
          name: airflow-config-envs
      image: neinver2024/apacheairflow:2.6.3-python3.11
      imagePullPolicy: IfNotPresent
      name: db-migrations
      resources: {}
      securityContext:
        runAsGroup: 0
        runAsUser: 50000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/airflow/dags
        name: dags-data
      - mountPath: /opt/airflow/logs
        name: logs-data
      - mountPath: /mnt/scripts
        name: scripts
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vhdkn
        readOnly: true
    - env:
      - name: GIT_SYNC_ROOT
        value: /dags
      - name: GIT_SYNC_DEST
        value: repo
      - name: GIT_SYNC_REPO
        value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
      - name: GIT_SYNC_BRANCH
        value: main
      - name: GIT_SYNC_REV
        value: HEAD
      - name: GIT_SYNC_DEPTH
        value: "1"
      - name: GIT_SYNC_WAIT
        value: "60"
      - name: GIT_SYNC_TIMEOUT
        value: "120"
      - name: GIT_SYNC_ADD_USER
        value: "true"
      - name: GIT_SYNC_MAX_SYNC_FAILURES
        value: "0"
      - name: GIT_SYNC_SUBMODULES
        value: recursive
      - name: GIT_SYNC_SSH
        value: "true"
      - name: GIT_SSH_KEY_FILE
        value: /etc/git-secret/id_rsa
      - name: GIT_KNOWN_HOSTS
        value: "false"
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: CONNECTION_CHECK_MAX_COUNT
        value: "0"
      - name: AIRFLOW__METRICS__STATSD_HOST
        value: prometheus-statsd-exporter
      - name: AIRFLOW__METRICS__STATSD_PORT
        value: "9125"
      - name: AIRFLOW__METRICS__STATSD_ON
        value: "true"
      envFrom:
      - secretRef:
          name: airflow-config-envs
      image: registry.k8s.io/git-sync/git-sync:v3.6.5
      imagePullPolicy: IfNotPresent
      name: dags-git-sync
      resources: {}
      securityContext:
        runAsGroup: 65533
        runAsUser: 65533
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /dags
        name: dags-data
      - mountPath: /etc/git-secret/id_rsa
        name: git-secret
        readOnly: true
        subPath: gitSshKey
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vhdkn
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - env:
      - name: GIT_SYNC_ONE_TIME
        value: "true"
      - name: GIT_SYNC_ROOT
        value: /dags
      - name: GIT_SYNC_DEST
        value: repo
      - name: GIT_SYNC_REPO
        value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
      - name: GIT_SYNC_BRANCH
        value: main
      - name: GIT_SYNC_REV
        value: HEAD
      - name: GIT_SYNC_DEPTH
        value: "1"
      - name: GIT_SYNC_WAIT
        value: "60"
      - name: GIT_SYNC_TIMEOUT
        value: "120"
      - name: GIT_SYNC_ADD_USER
        value: "true"
      - name: GIT_SYNC_MAX_SYNC_FAILURES
        value: "0"
      - name: GIT_SYNC_SUBMODULES
        value: recursive
      - name: GIT_SYNC_SSH
        value: "true"
      - name: GIT_SSH_KEY_FILE
        value: /etc/git-secret/id_rsa
      - name: GIT_KNOWN_HOSTS
        value: "false"
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: CONNECTION_CHECK_MAX_COUNT
        value: "0"
      - name: AIRFLOW__METRICS__STATSD_HOST
        value: prometheus-statsd-exporter
      - name: AIRFLOW__METRICS__STATSD_PORT
        value: "9125"
      - name: AIRFLOW__METRICS__STATSD_ON
        value: "true"
      envFrom:
      - secretRef:
          name: airflow-config-envs
      image: registry.k8s.io/git-sync/git-sync:v3.6.5
      imagePullPolicy: IfNotPresent
      name: dags-git-clone
      resources: {}
      securityContext:
        runAsGroup: 65533
        runAsUser: 65533
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /dags
        name: dags-data
      - mountPath: /etc/git-secret/id_rsa
        name: git-secret
        readOnly: true
        subPath: gitSshKey
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vhdkn
        readOnly: true
    - args:
      - bash
      - -c
      - exec timeout 60s airflow db check
      command:
      - /usr/bin/dumb-init
      - --
      - /entrypoint
      env:
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: CONNECTION_CHECK_MAX_COUNT
        value: "0"
      - name: AIRFLOW__METRICS__STATSD_HOST
        value: prometheus-statsd-exporter
      - name: AIRFLOW__METRICS__STATSD_PORT
        value: "9125"
      - name: AIRFLOW__METRICS__STATSD_ON
        value: "true"
      envFrom:
      - secretRef:
          name: airflow-config-envs
      image: neinver2024/apacheairflow:2.6.3-python3.11
      imagePullPolicy: IfNotPresent
      name: check-db
      resources: {}
      securityContext:
        runAsGroup: 0
        runAsUser: 50000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/airflow/dags
        name: dags-data
      - mountPath: /opt/airflow/logs
        name: logs-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vhdkn
        readOnly: true
    nodeName: worker-2
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 0
    serviceAccount: airflow
    serviceAccountName: airflow
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: dags-data
    - name: logs-data
      persistentVolumeClaim:
        claimName: airflow-logs
    - name: git-secret
      secret:
        defaultMode: 420
        secretName: airflow-ssh-secret
    - name: scripts
      secret:
        defaultMode: 420
        secretName: airflow-db-migrations
    - name: kube-api-access-vhdkn
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-14T05:13:27Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-14T05:13:28Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-14T05:13:28Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-14T05:12:14Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://317ab8e3720193d00d40944c53404ef076352d16dd610f4fa510b444f57e5db0
      image: registry.k8s.io/git-sync/git-sync:v3.6.5
      imageID: docker-pullable://registry.k8s.io/git-sync/git-sync@sha256:7231f6c2284758b91caed71e4e596413df31ac4467de9b596dc6b386b82f624f
      lastState: {}
      name: dags-git-sync
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-14T05:13:27Z"
    - containerID: docker://7f7346b64595cd03e59cb13f9f92d4ca7cbd9278560659c93a8569fad5f37475
      image: neinver2024/apacheairflow:2.6.3-python3.11
      imageID: docker-pullable://neinver2024/apacheairflow@sha256:35afb4acf4f67c73c84428d57f7e58fa3d247c690f7c01d91c9f7e2f59aff800
      lastState: {}
      name: db-migrations
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-14T05:13:27Z"
    hostIP: 10.2.1.184
    initContainerStatuses:
    - containerID: docker://313bcd4e2ade88f653714727ff90007480e9edeeb61ee2ada98f5f2d93f1f47d
      image: registry.k8s.io/git-sync/git-sync:v3.6.5
      imageID: docker-pullable://registry.k8s.io/git-sync/git-sync@sha256:7231f6c2284758b91caed71e4e596413df31ac4467de9b596dc6b386b82f624f
      lastState: {}
      name: dags-git-clone
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://313bcd4e2ade88f653714727ff90007480e9edeeb61ee2ada98f5f2d93f1f47d
          exitCode: 0
          finishedAt: "2024-03-14T05:12:16Z"
          reason: Completed
          startedAt: "2024-03-14T05:12:15Z"
    - containerID: docker://40975660e54d3224663142b31c0e2e30752d4acfdd160499d1180dd3d43bdc3c
      image: neinver2024/apacheairflow:2.6.3-python3.11
      imageID: docker-pullable://neinver2024/apacheairflow@sha256:35afb4acf4f67c73c84428d57f7e58fa3d247c690f7c01d91c9f7e2f59aff800
      lastState: {}
      name: check-db
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://40975660e54d3224663142b31c0e2e30752d4acfdd160499d1180dd3d43bdc3c
          exitCode: 0
          finishedAt: "2024-03-14T05:13:25Z"
          reason: Completed
          startedAt: "2024-03-14T05:13:19Z"
    phase: Running
    podIP: 10.42.133.226
    podIPs:
    - ip: 10.42.133.226
    qosClass: BestEffort
    startTime: "2024-03-14T05:12:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/secret-config-envs: 7b5b4b568e7c694c0f44651459436d8876c0d6f337d3876162036cffceec3b68
      checksum/secret-pgbouncer: cbb28505a66ba6d70bfda4f3495790f613816b1e9ac5761a955018cdb67ad44e
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      cni.projectcalico.org/containerID: 0a012688b746aa5dcff5371b3aac4aefd6d564aa1f34dad35cfcea926342cc57
      cni.projectcalico.org/podIP: 10.42.38.92/32
      cni.projectcalico.org/podIPs: 10.42.38.92/32
    creationTimestamp: "2024-03-14T05:12:12Z"
    generateName: airflow-pgbouncer-55f8877f8c-
    labels:
      app: airflow
      component: pgbouncer
      pod-template-hash: 55f8877f8c
      release: airflow
    name: airflow-pgbouncer-55f8877f8c-28btr
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: airflow-pgbouncer-55f8877f8c
      uid: b0e83978-4a5b-47ec-818a-345bc9dd8911
    resourceVersion: "13231425"
    uid: 0e301ce7-8531-4f64-a26a-a70f2ac6f335
  spec:
    affinity: {}
    containers:
    - args:
      - /bin/sh
      - -c
      - |-
        /home/pgbouncer/config/gen_self_signed_cert.sh && \
        /home/pgbouncer/config/gen_auth_file.sh && \
        exec pgbouncer /home/pgbouncer/config/pgbouncer.ini
      command:
      - /usr/bin/dumb-init
      - --rewrite=15:2
      - --
      env:
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: CONNECTION_CHECK_MAX_COUNT
        value: "0"
      - name: AIRFLOW__METRICS__STATSD_HOST
        value: prometheus-statsd-exporter
      - name: AIRFLOW__METRICS__STATSD_PORT
        value: "9125"
      - name: AIRFLOW__METRICS__STATSD_ON
        value: "true"
      envFrom:
      - secretRef:
          name: airflow-config-envs
      image: ghcr.io/airflow-helm/pgbouncer:1.18.0-patch.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - psql $(eval $DATABASE_PSQL_CMD) --tuples-only --command="SELECT 1;" |
            grep -q "1"
        failureThreshold: 3
        initialDelaySeconds: 5
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 60
      name: pgbouncer
      ports:
      - containerPort: 6432
        name: pgbouncer
        protocol: TCP
      resources: {}
      securityContext:
        runAsGroup: 1001
        runAsUser: 1001
      startupProbe:
        failureThreshold: 30
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 6432
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /home/pgbouncer/config
        name: pgbouncer-config
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hn6rk
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker-4
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 0
    serviceAccount: airflow
    serviceAccountName: airflow
    terminationGracePeriodSeconds: 120
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: pgbouncer-config
      secret:
        defaultMode: 420
        items:
        - key: gen_auth_file.sh
          mode: 493
          path: gen_auth_file.sh
        - key: gen_self_signed_cert.sh
          mode: 493
          path: gen_self_signed_cert.sh
        - key: pgbouncer.ini
          path: pgbouncer.ini
        secretName: airflow-pgbouncer
    - name: kube-api-access-hn6rk
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-14T05:12:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-14T05:12:24Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-14T05:12:24Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-14T05:12:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://33055f6930153d11ce01d3aa5feb352e76dff60cf3a6b57aa91b6c5b3dd621ae
      image: ghcr.io/airflow-helm/pgbouncer:1.18.0-patch.1
      imageID: docker-pullable://ghcr.io/airflow-helm/pgbouncer@sha256:2347043c74de12f2954331b5c08c9a13510c2db3e7a8d9efa601bb3678f34e84
      lastState: {}
      name: pgbouncer
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-14T05:12:15Z"
    hostIP: 10.2.1.186
    phase: Running
    podIP: 10.42.38.92
    podIPs:
    - ip: 10.42.38.92
    qosClass: BestEffort
    startTime: "2024-03-14T05:12:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config-pod-template: 1a8683c073a2d13ce4ef577bf3256def776cd1e7666f47cc1d08d00f446fe1c9
      checksum/secret-config-envs: 7b5b4b568e7c694c0f44651459436d8876c0d6f337d3876162036cffceec3b68
      checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      cni.projectcalico.org/containerID: 33e939539be4e8d26240c6398bea6b006e8cf7be8ba44c496b0524f8feb07458
      cni.projectcalico.org/podIP: 10.42.38.94/32
      cni.projectcalico.org/podIPs: 10.42.38.94/32
    creationTimestamp: "2024-03-14T05:12:12Z"
    generateName: airflow-scheduler-54bf954c6d-
    labels:
      app: airflow
      component: scheduler
      pod-template-hash: 54bf954c6d
      release: airflow
    name: airflow-scheduler-54bf954c6d-hm8mt
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: airflow-scheduler-54bf954c6d
      uid: 203f54e9-1bc1-4a69-b996-4fb12840c8dd
    resourceVersion: "13231811"
    uid: 7269085e-941a-4bc8-a854-019725aa0d83
  spec:
    affinity: {}
    containers:
    - args:
      - bash
      - -c
      - exec airflow scheduler -n -1
      command:
      - /usr/bin/dumb-init
      - --
      - /entrypoint
      env:
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: CONNECTION_CHECK_MAX_COUNT
        value: "0"
      - name: AIRFLOW__METRICS__STATSD_HOST
        value: prometheus-statsd-exporter
      - name: AIRFLOW__METRICS__STATSD_PORT
        value: "9125"
      - name: AIRFLOW__METRICS__STATSD_ON
        value: "true"
      envFrom:
      - secretRef:
          name: airflow-config-envs
      image: neinver2024/apacheairflow:2.6.3-python3.11
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          - python
          - -Wignore
          - -c
          - |
            import os
            import sys

            # suppress logs triggered from importing airflow packages
            os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

            # shared imports
            try:
                from airflow.jobs.job import Job
            except ImportError:
                # `BaseJob` was renamed to `Job` in airflow 2.6.0
                from airflow.jobs.base_job import BaseJob as Job
            from airflow.utils.db import create_session
            from airflow.utils.net import get_hostname

            # heartbeat check imports
            try:
                from airflow.jobs.scheduler_job_runner import SchedulerJobRunner
            except ImportError:
                # `SchedulerJob` is wrapped by `SchedulerJobRunner` since airflow 2.6.0
                from airflow.jobs.scheduler_job import SchedulerJob as SchedulerJobRunner

            with create_session() as session:
                ########################
                # heartbeat check
                ########################
                # ensure the SchedulerJob with most recent heartbeat for this `hostname` is alive
                hostname = get_hostname()
                scheduler_job = session \
                    .query(Job) \
                    .filter_by(job_type=SchedulerJobRunner.job_type) \
                    .filter_by(hostname=hostname) \
                    .order_by(Job.latest_heartbeat.desc()) \
                    .limit(1) \
                    .first()
                if (scheduler_job is not None) and scheduler_job.is_alive():
                    pass
                else:
                    sys.exit(f"The SchedulerJob (id={scheduler_job.id}) for hostname '{hostname}' is not alive")
        failureThreshold: 5
        initialDelaySeconds: 10
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 60
      name: airflow-scheduler
      resources: {}
      securityContext:
        runAsGroup: 0
        runAsUser: 50000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/airflow/dags
        name: dags-data
      - mountPath: /opt/airflow/logs
        name: logs-data
      - mountPath: /opt/airflow/pod_templates/pod_template.yaml
        name: pod-template
        readOnly: true
        subPath: pod_template.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jzc79
        readOnly: true
    - env:
      - name: GIT_SYNC_ROOT
        value: /dags
      - name: GIT_SYNC_DEST
        value: repo
      - name: GIT_SYNC_REPO
        value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
      - name: GIT_SYNC_BRANCH
        value: main
      - name: GIT_SYNC_REV
        value: HEAD
      - name: GIT_SYNC_DEPTH
        value: "1"
      - name: GIT_SYNC_WAIT
        value: "60"
      - name: GIT_SYNC_TIMEOUT
        value: "120"
      - name: GIT_SYNC_ADD_USER
        value: "true"
      - name: GIT_SYNC_MAX_SYNC_FAILURES
        value: "0"
      - name: GIT_SYNC_SUBMODULES
        value: recursive
      - name: GIT_SYNC_SSH
        value: "true"
      - name: GIT_SSH_KEY_FILE
        value: /etc/git-secret/id_rsa
      - name: GIT_KNOWN_HOSTS
        value: "false"
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: CONNECTION_CHECK_MAX_COUNT
        value: "0"
      - name: AIRFLOW__METRICS__STATSD_HOST
        value: prometheus-statsd-exporter
      - name: AIRFLOW__METRICS__STATSD_PORT
        value: "9125"
      - name: AIRFLOW__METRICS__STATSD_ON
        value: "true"
      envFrom:
      - secretRef:
          name: airflow-config-envs
      image: registry.k8s.io/git-sync/git-sync:v3.6.5
      imagePullPolicy: IfNotPresent
      name: dags-git-sync
      resources: {}
      securityContext:
        runAsGroup: 65533
        runAsUser: 65533
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /dags
        name: dags-data
      - mountPath: /etc/git-secret/id_rsa
        name: git-secret
        readOnly: true
        subPath: gitSshKey
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jzc79
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - env:
      - name: GIT_SYNC_ONE_TIME
        value: "true"
      - name: GIT_SYNC_ROOT
        value: /dags
      - name: GIT_SYNC_DEST
        value: repo
      - name: GIT_SYNC_REPO
        value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
      - name: GIT_SYNC_BRANCH
        value: main
      - name: GIT_SYNC_REV
        value: HEAD
      - name: GIT_SYNC_DEPTH
        value: "1"
      - name: GIT_SYNC_WAIT
        value: "60"
      - name: GIT_SYNC_TIMEOUT
        value: "120"
      - name: GIT_SYNC_ADD_USER
        value: "true"
      - name: GIT_SYNC_MAX_SYNC_FAILURES
        value: "0"
      - name: GIT_SYNC_SUBMODULES
        value: recursive
      - name: GIT_SYNC_SSH
        value: "true"
      - name: GIT_SSH_KEY_FILE
        value: /etc/git-secret/id_rsa
      - name: GIT_KNOWN_HOSTS
        value: "false"
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: CONNECTION_CHECK_MAX_COUNT
        value: "0"
      - name: AIRFLOW__METRICS__STATSD_HOST
        value: prometheus-statsd-exporter
      - name: AIRFLOW__METRICS__STATSD_PORT
        value: "9125"
      - name: AIRFLOW__METRICS__STATSD_ON
        value: "true"
      envFrom:
      - secretRef:
          name: airflow-config-envs
      image: registry.k8s.io/git-sync/git-sync:v3.6.5
      imagePullPolicy: IfNotPresent
      name: dags-git-clone
      resources: {}
      securityContext:
        runAsGroup: 65533
        runAsUser: 65533
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /dags
        name: dags-data
      - mountPath: /etc/git-secret/id_rsa
        name: git-secret
        readOnly: true
        subPath: gitSshKey
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jzc79
        readOnly: true
    - args:
      - bash
      - -c
      - exec timeout 60s airflow db check
      command:
      - /usr/bin/dumb-init
      - --
      - /entrypoint
      env:
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: CONNECTION_CHECK_MAX_COUNT
        value: "0"
      - name: AIRFLOW__METRICS__STATSD_HOST
        value: prometheus-statsd-exporter
      - name: AIRFLOW__METRICS__STATSD_PORT
        value: "9125"
      - name: AIRFLOW__METRICS__STATSD_ON
        value: "true"
      envFrom:
      - secretRef:
          name: airflow-config-envs
      image: neinver2024/apacheairflow:2.6.3-python3.11
      imagePullPolicy: IfNotPresent
      name: check-db
      resources: {}
      securityContext:
        runAsGroup: 0
        runAsUser: 50000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/airflow/dags
        name: dags-data
      - mountPath: /opt/airflow/logs
        name: logs-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jzc79
        readOnly: true
    - args:
      - bash
      - -c
      - exec airflow db check-migrations -t 60
      command:
      - /usr/bin/dumb-init
      - --
      - /entrypoint
      env:
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: CONNECTION_CHECK_MAX_COUNT
        value: "0"
      - name: AIRFLOW__METRICS__STATSD_HOST
        value: prometheus-statsd-exporter
      - name: AIRFLOW__METRICS__STATSD_PORT
        value: "9125"
      - name: AIRFLOW__METRICS__STATSD_ON
        value: "true"
      envFrom:
      - secretRef:
          name: airflow-config-envs
      image: neinver2024/apacheairflow:2.6.3-python3.11
      imagePullPolicy: IfNotPresent
      name: wait-for-db-migrations
      resources: {}
      securityContext:
        runAsGroup: 0
        runAsUser: 50000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/airflow/dags
        name: dags-data
      - mountPath: /opt/airflow/logs
        name: logs-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jzc79
        readOnly: true
    nodeName: worker-4
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 0
    serviceAccount: airflow
    serviceAccountName: airflow
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: dags-data
    - name: logs-data
      persistentVolumeClaim:
        claimName: airflow-logs
    - name: git-secret
      secret:
        defaultMode: 420
        secretName: airflow-ssh-secret
    - configMap:
        defaultMode: 420
        name: airflow-pod-template
      name: pod-template
    - name: kube-api-access-jzc79
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-14T05:13:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-14T05:13:44Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-14T05:13:44Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-14T05:12:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://b6407ece5103e4ec4831bf1437de6f9716d67dd98356650aa222968f8297f959
      image: neinver2024/apacheairflow:2.6.3-python3.11
      imageID: docker-pullable://neinver2024/apacheairflow@sha256:35afb4acf4f67c73c84428d57f7e58fa3d247c690f7c01d91c9f7e2f59aff800
      lastState: {}
      name: airflow-scheduler
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-14T05:13:43Z"
    - containerID: docker://a470a001634bdaa7beba7c6a136246f7b5304880375609260ece241679523d06
      image: registry.k8s.io/git-sync/git-sync:v3.6.5
      imageID: docker-pullable://registry.k8s.io/git-sync/git-sync@sha256:7231f6c2284758b91caed71e4e596413df31ac4467de9b596dc6b386b82f624f
      lastState: {}
      name: dags-git-sync
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-14T05:13:44Z"
    hostIP: 10.2.1.186
    initContainerStatuses:
    - containerID: docker://0d6b2bbe9380982baaacd6c4a1da252e310c2a0d9ae9e2c3b42695f1597d4282
      image: registry.k8s.io/git-sync/git-sync:v3.6.5
      imageID: docker-pullable://registry.k8s.io/git-sync/git-sync@sha256:7231f6c2284758b91caed71e4e596413df31ac4467de9b596dc6b386b82f624f
      lastState: {}
      name: dags-git-clone
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://0d6b2bbe9380982baaacd6c4a1da252e310c2a0d9ae9e2c3b42695f1597d4282
          exitCode: 0
          finishedAt: "2024-03-14T05:12:21Z"
          reason: Completed
          startedAt: "2024-03-14T05:12:20Z"
    - containerID: docker://3cd0af63ded39081a5f85fee0e6d3d27759b76a16d0c3182cb04e1335625e1fe
      image: neinver2024/apacheairflow:2.6.3-python3.11
      imageID: docker-pullable://neinver2024/apacheairflow@sha256:35afb4acf4f67c73c84428d57f7e58fa3d247c690f7c01d91c9f7e2f59aff800
      lastState: {}
      name: check-db
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://3cd0af63ded39081a5f85fee0e6d3d27759b76a16d0c3182cb04e1335625e1fe
          exitCode: 0
          finishedAt: "2024-03-14T05:13:29Z"
          reason: Completed
          startedAt: "2024-03-14T05:13:22Z"
    - containerID: docker://0dc1d98b4684a76a93074ba08297137da05a634d1f9d69ffdf05fb1250fe84fd
      image: neinver2024/apacheairflow:2.6.3-python3.11
      imageID: docker-pullable://neinver2024/apacheairflow@sha256:35afb4acf4f67c73c84428d57f7e58fa3d247c690f7c01d91c9f7e2f59aff800
      lastState: {}
      name: wait-for-db-migrations
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://0dc1d98b4684a76a93074ba08297137da05a634d1f9d69ffdf05fb1250fe84fd
          exitCode: 0
          finishedAt: "2024-03-14T05:13:43Z"
          reason: Completed
          startedAt: "2024-03-14T05:13:30Z"
    phase: Running
    podIP: 10.42.38.94
    podIPs:
    - ip: 10.42.38.94
    qosClass: BestEffort
    startTime: "2024-03-14T05:12:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/secret-config-envs: 7b5b4b568e7c694c0f44651459436d8876c0d6f337d3876162036cffceec3b68
      checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
      checksum/sync-users-script: 0e7bdc9596783274f1496d1dc37fa2eda2d72d60a2b56e3b61274b347f7e10ee
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      cni.projectcalico.org/containerID: ba9744eb032f4d259892b0755307cbe149103759663f1fba4476afc30a4212e2
      cni.projectcalico.org/podIP: 10.42.226.94/32
      cni.projectcalico.org/podIPs: 10.42.226.94/32
    creationTimestamp: "2024-03-14T05:12:13Z"
    generateName: airflow-sync-users-7ffb469495-
    labels:
      app: airflow
      component: sync-users
      pod-template-hash: 7ffb469495
      release: airflow
    name: airflow-sync-users-7ffb469495-l9dsj
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: airflow-sync-users-7ffb469495
      uid: 40b8488f-f22b-4808-b5d8-2766d956ba4c
    resourceVersion: "13231782"
    uid: a26fdf20-a178-4c92-9dc2-834c8bf0f6f3
  spec:
    affinity: {}
    containers:
    - args:
      - python
      - -u
      - /mnt/scripts/sync_users.py
      command:
      - /usr/bin/dumb-init
      - --
      - /entrypoint
      env:
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: CONNECTION_CHECK_MAX_COUNT
        value: "0"
      - name: AIRFLOW__METRICS__STATSD_HOST
        value: prometheus-statsd-exporter
      - name: AIRFLOW__METRICS__STATSD_PORT
        value: "9125"
      - name: AIRFLOW__METRICS__STATSD_ON
        value: "true"
      envFrom:
      - secretRef:
          name: airflow-config-envs
      image: neinver2024/apacheairflow:2.6.3-python3.11
      imagePullPolicy: IfNotPresent
      name: sync-airflow-users
      resources: {}
      securityContext:
        runAsGroup: 0
        runAsUser: 50000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/airflow/dags
        name: dags-data
      - mountPath: /opt/airflow/logs
        name: logs-data
      - mountPath: /mnt/scripts
        name: scripts
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vxl58
        readOnly: true
    - env:
      - name: GIT_SYNC_ROOT
        value: /dags
      - name: GIT_SYNC_DEST
        value: repo
      - name: GIT_SYNC_REPO
        value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
      - name: GIT_SYNC_BRANCH
        value: main
      - name: GIT_SYNC_REV
        value: HEAD
      - name: GIT_SYNC_DEPTH
        value: "1"
      - name: GIT_SYNC_WAIT
        value: "60"
      - name: GIT_SYNC_TIMEOUT
        value: "120"
      - name: GIT_SYNC_ADD_USER
        value: "true"
      - name: GIT_SYNC_MAX_SYNC_FAILURES
        value: "0"
      - name: GIT_SYNC_SUBMODULES
        value: recursive
      - name: GIT_SYNC_SSH
        value: "true"
      - name: GIT_SSH_KEY_FILE
        value: /etc/git-secret/id_rsa
      - name: GIT_KNOWN_HOSTS
        value: "false"
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: CONNECTION_CHECK_MAX_COUNT
        value: "0"
      - name: AIRFLOW__METRICS__STATSD_HOST
        value: prometheus-statsd-exporter
      - name: AIRFLOW__METRICS__STATSD_PORT
        value: "9125"
      - name: AIRFLOW__METRICS__STATSD_ON
        value: "true"
      envFrom:
      - secretRef:
          name: airflow-config-envs
      image: registry.k8s.io/git-sync/git-sync:v3.6.5
      imagePullPolicy: IfNotPresent
      name: dags-git-sync
      resources: {}
      securityContext:
        runAsGroup: 65533
        runAsUser: 65533
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /dags
        name: dags-data
      - mountPath: /etc/git-secret/id_rsa
        name: git-secret
        readOnly: true
        subPath: gitSshKey
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vxl58
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - env:
      - name: GIT_SYNC_ONE_TIME
        value: "true"
      - name: GIT_SYNC_ROOT
        value: /dags
      - name: GIT_SYNC_DEST
        value: repo
      - name: GIT_SYNC_REPO
        value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
      - name: GIT_SYNC_BRANCH
        value: main
      - name: GIT_SYNC_REV
        value: HEAD
      - name: GIT_SYNC_DEPTH
        value: "1"
      - name: GIT_SYNC_WAIT
        value: "60"
      - name: GIT_SYNC_TIMEOUT
        value: "120"
      - name: GIT_SYNC_ADD_USER
        value: "true"
      - name: GIT_SYNC_MAX_SYNC_FAILURES
        value: "0"
      - name: GIT_SYNC_SUBMODULES
        value: recursive
      - name: GIT_SYNC_SSH
        value: "true"
      - name: GIT_SSH_KEY_FILE
        value: /etc/git-secret/id_rsa
      - name: GIT_KNOWN_HOSTS
        value: "false"
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: CONNECTION_CHECK_MAX_COUNT
        value: "0"
      - name: AIRFLOW__METRICS__STATSD_HOST
        value: prometheus-statsd-exporter
      - name: AIRFLOW__METRICS__STATSD_PORT
        value: "9125"
      - name: AIRFLOW__METRICS__STATSD_ON
        value: "true"
      envFrom:
      - secretRef:
          name: airflow-config-envs
      image: registry.k8s.io/git-sync/git-sync:v3.6.5
      imagePullPolicy: IfNotPresent
      name: dags-git-clone
      resources: {}
      securityContext:
        runAsGroup: 65533
        runAsUser: 65533
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /dags
        name: dags-data
      - mountPath: /etc/git-secret/id_rsa
        name: git-secret
        readOnly: true
        subPath: gitSshKey
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vxl58
        readOnly: true
    - args:
      - bash
      - -c
      - exec timeout 60s airflow db check
      command:
      - /usr/bin/dumb-init
      - --
      - /entrypoint
      env:
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: CONNECTION_CHECK_MAX_COUNT
        value: "0"
      - name: AIRFLOW__METRICS__STATSD_HOST
        value: prometheus-statsd-exporter
      - name: AIRFLOW__METRICS__STATSD_PORT
        value: "9125"
      - name: AIRFLOW__METRICS__STATSD_ON
        value: "true"
      envFrom:
      - secretRef:
          name: airflow-config-envs
      image: neinver2024/apacheairflow:2.6.3-python3.11
      imagePullPolicy: IfNotPresent
      name: check-db
      resources: {}
      securityContext:
        runAsGroup: 0
        runAsUser: 50000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/airflow/dags
        name: dags-data
      - mountPath: /opt/airflow/logs
        name: logs-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vxl58
        readOnly: true
    - args:
      - bash
      - -c
      - exec airflow db check-migrations -t 60
      command:
      - /usr/bin/dumb-init
      - --
      - /entrypoint
      env:
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: CONNECTION_CHECK_MAX_COUNT
        value: "0"
      - name: AIRFLOW__METRICS__STATSD_HOST
        value: prometheus-statsd-exporter
      - name: AIRFLOW__METRICS__STATSD_PORT
        value: "9125"
      - name: AIRFLOW__METRICS__STATSD_ON
        value: "true"
      envFrom:
      - secretRef:
          name: airflow-config-envs
      image: neinver2024/apacheairflow:2.6.3-python3.11
      imagePullPolicy: IfNotPresent
      name: wait-for-db-migrations
      resources: {}
      securityContext:
        runAsGroup: 0
        runAsUser: 50000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/airflow/dags
        name: dags-data
      - mountPath: /opt/airflow/logs
        name: logs-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vxl58
        readOnly: true
    nodeName: worker-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 0
    serviceAccount: airflow
    serviceAccountName: airflow
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: dags-data
    - name: logs-data
      persistentVolumeClaim:
        claimName: airflow-logs
    - name: git-secret
      secret:
        defaultMode: 420
        secretName: airflow-ssh-secret
    - name: scripts
      secret:
        defaultMode: 420
        secretName: airflow-sync-users
    - name: kube-api-access-vxl58
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-14T05:13:40Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-14T05:13:41Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-14T05:13:41Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-14T05:12:13Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://f8cf7a595bf27f9fe85549cec65b331fc8cb2bde673c1784e8cd127a2b266770
      image: registry.k8s.io/git-sync/git-sync:v3.6.5
      imageID: docker-pullable://registry.k8s.io/git-sync/git-sync@sha256:7231f6c2284758b91caed71e4e596413df31ac4467de9b596dc6b386b82f624f
      lastState: {}
      name: dags-git-sync
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-14T05:13:41Z"
    - containerID: docker://0621513ec9ac6c5a05bf19747be6eabbad0fa4ed55f62ef8c82260de0f06e424
      image: neinver2024/apacheairflow:2.6.3-python3.11
      imageID: docker-pullable://neinver2024/apacheairflow@sha256:35afb4acf4f67c73c84428d57f7e58fa3d247c690f7c01d91c9f7e2f59aff800
      lastState: {}
      name: sync-airflow-users
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-14T05:13:40Z"
    hostIP: 10.2.1.183
    initContainerStatuses:
    - containerID: docker://cb54a78510415db46249a1e33508efb69ee93aa8cd4353fbdb46d8350bea1b69
      image: registry.k8s.io/git-sync/git-sync:v3.6.5
      imageID: docker-pullable://registry.k8s.io/git-sync/git-sync@sha256:7231f6c2284758b91caed71e4e596413df31ac4467de9b596dc6b386b82f624f
      lastState: {}
      name: dags-git-clone
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://cb54a78510415db46249a1e33508efb69ee93aa8cd4353fbdb46d8350bea1b69
          exitCode: 0
          finishedAt: "2024-03-14T05:12:15Z"
          reason: Completed
          startedAt: "2024-03-14T05:12:14Z"
    - containerID: docker://71bb8ddd3a8750e46a7eac0847a7b54e510c19a47f52d52e6bbb6045dc36e7f3
      image: neinver2024/apacheairflow:2.6.3-python3.11
      imageID: docker-pullable://neinver2024/apacheairflow@sha256:35afb4acf4f67c73c84428d57f7e58fa3d247c690f7c01d91c9f7e2f59aff800
      lastState: {}
      name: check-db
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://71bb8ddd3a8750e46a7eac0847a7b54e510c19a47f52d52e6bbb6045dc36e7f3
          exitCode: 0
          finishedAt: "2024-03-14T05:13:27Z"
          reason: Completed
          startedAt: "2024-03-14T05:13:21Z"
    - containerID: docker://e2c29040985b5fc0c41028fccd9f60c921283f162163cba7a2013775575b30ba
      image: neinver2024/apacheairflow:2.6.3-python3.11
      imageID: docker-pullable://neinver2024/apacheairflow@sha256:35afb4acf4f67c73c84428d57f7e58fa3d247c690f7c01d91c9f7e2f59aff800
      lastState: {}
      name: wait-for-db-migrations
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://e2c29040985b5fc0c41028fccd9f60c921283f162163cba7a2013775575b30ba
          exitCode: 0
          finishedAt: "2024-03-14T05:13:39Z"
          reason: Completed
          startedAt: "2024-03-14T05:13:29Z"
    phase: Running
    podIP: 10.42.226.94
    podIPs:
    - ip: 10.42.226.94
    qosClass: BestEffort
    startTime: "2024-03-14T05:12:13Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/secret-config-envs: 7b5b4b568e7c694c0f44651459436d8876c0d6f337d3876162036cffceec3b68
      checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      cni.projectcalico.org/containerID: ee09b7063995403038018def936f25d47a8a318286c3c2d1b0ef39800fe75da3
      cni.projectcalico.org/podIP: 10.42.97.218/32
      cni.projectcalico.org/podIPs: 10.42.97.218/32
    creationTimestamp: "2024-03-14T05:12:12Z"
    generateName: airflow-triggerer-54b68d597f-
    labels:
      app: airflow
      component: triggerer
      pod-template-hash: 54b68d597f
      release: airflow
    name: airflow-triggerer-54b68d597f-292d9
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: airflow-triggerer-54b68d597f
      uid: c7ba2757-c2af-4a34-9b4b-f344c079726c
    resourceVersion: "13231738"
    uid: af1c403b-7a93-4dab-add1-392b0ec7c46b
  spec:
    affinity: {}
    containers:
    - args:
      - bash
      - -c
      - exec airflow triggerer
      command:
      - /usr/bin/dumb-init
      - --
      - /entrypoint
      env:
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: CONNECTION_CHECK_MAX_COUNT
        value: "0"
      - name: AIRFLOW__METRICS__STATSD_HOST
        value: prometheus-statsd-exporter
      - name: AIRFLOW__METRICS__STATSD_PORT
        value: "9125"
      - name: AIRFLOW__METRICS__STATSD_ON
        value: "true"
      envFrom:
      - secretRef:
          name: airflow-config-envs
      image: neinver2024/apacheairflow:2.6.3-python3.11
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          - python
          - -Wignore
          - -c
          - |
            import os
            import sys

            # suppress logs triggered from importing airflow packages
            os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

            # shared imports
            try:
                from airflow.jobs.job import Job
            except ImportError:
                # `BaseJob` was renamed to `Job` in airflow 2.6.0
                from airflow.jobs.base_job import BaseJob as Job
            from airflow.utils.db import create_session
            from airflow.utils.net import get_hostname

            # heartbeat check imports
            try:
                from airflow.jobs.triggerer_job_runner import TriggererJobRunner
            except ImportError:
                # `TriggererJob` is wrapped by `TriggererJobRunner` since airflow 2.6.0
                from airflow.jobs.triggerer_job import TriggererJob as TriggererJobRunner

            with create_session() as session:
                # ensure the TriggererJob with most recent heartbeat for this `hostname` is alive
                hostname = get_hostname()
                triggerer_job = session \
                    .query(Job) \
                    .filter_by(job_type=TriggererJobRunner.job_type) \
                    .filter_by(hostname=hostname) \
                    .order_by(Job.latest_heartbeat.desc()) \
                    .limit(1) \
                    .first()
                if (triggerer_job is not None) and triggerer_job.is_alive():
                    pass
                else:
                    sys.exit(f"The TriggererJob (id={triggerer_job.id}) for hostname '{hostname}' is not alive")
        failureThreshold: 5
        initialDelaySeconds: 10
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 60
      name: airflow-triggerer
      resources: {}
      securityContext:
        runAsGroup: 0
        runAsUser: 50000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/airflow/dags
        name: dags-data
      - mountPath: /opt/airflow/logs
        name: logs-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w569d
        readOnly: true
    - env:
      - name: GIT_SYNC_ROOT
        value: /dags
      - name: GIT_SYNC_DEST
        value: repo
      - name: GIT_SYNC_REPO
        value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
      - name: GIT_SYNC_BRANCH
        value: main
      - name: GIT_SYNC_REV
        value: HEAD
      - name: GIT_SYNC_DEPTH
        value: "1"
      - name: GIT_SYNC_WAIT
        value: "60"
      - name: GIT_SYNC_TIMEOUT
        value: "120"
      - name: GIT_SYNC_ADD_USER
        value: "true"
      - name: GIT_SYNC_MAX_SYNC_FAILURES
        value: "0"
      - name: GIT_SYNC_SUBMODULES
        value: recursive
      - name: GIT_SYNC_SSH
        value: "true"
      - name: GIT_SSH_KEY_FILE
        value: /etc/git-secret/id_rsa
      - name: GIT_KNOWN_HOSTS
        value: "false"
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: CONNECTION_CHECK_MAX_COUNT
        value: "0"
      - name: AIRFLOW__METRICS__STATSD_HOST
        value: prometheus-statsd-exporter
      - name: AIRFLOW__METRICS__STATSD_PORT
        value: "9125"
      - name: AIRFLOW__METRICS__STATSD_ON
        value: "true"
      envFrom:
      - secretRef:
          name: airflow-config-envs
      image: registry.k8s.io/git-sync/git-sync:v3.6.5
      imagePullPolicy: IfNotPresent
      name: dags-git-sync
      resources: {}
      securityContext:
        runAsGroup: 65533
        runAsUser: 65533
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /dags
        name: dags-data
      - mountPath: /etc/git-secret/id_rsa
        name: git-secret
        readOnly: true
        subPath: gitSshKey
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w569d
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - env:
      - name: GIT_SYNC_ONE_TIME
        value: "true"
      - name: GIT_SYNC_ROOT
        value: /dags
      - name: GIT_SYNC_DEST
        value: repo
      - name: GIT_SYNC_REPO
        value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
      - name: GIT_SYNC_BRANCH
        value: main
      - name: GIT_SYNC_REV
        value: HEAD
      - name: GIT_SYNC_DEPTH
        value: "1"
      - name: GIT_SYNC_WAIT
        value: "60"
      - name: GIT_SYNC_TIMEOUT
        value: "120"
      - name: GIT_SYNC_ADD_USER
        value: "true"
      - name: GIT_SYNC_MAX_SYNC_FAILURES
        value: "0"
      - name: GIT_SYNC_SUBMODULES
        value: recursive
      - name: GIT_SYNC_SSH
        value: "true"
      - name: GIT_SSH_KEY_FILE
        value: /etc/git-secret/id_rsa
      - name: GIT_KNOWN_HOSTS
        value: "false"
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: CONNECTION_CHECK_MAX_COUNT
        value: "0"
      - name: AIRFLOW__METRICS__STATSD_HOST
        value: prometheus-statsd-exporter
      - name: AIRFLOW__METRICS__STATSD_PORT
        value: "9125"
      - name: AIRFLOW__METRICS__STATSD_ON
        value: "true"
      envFrom:
      - secretRef:
          name: airflow-config-envs
      image: registry.k8s.io/git-sync/git-sync:v3.6.5
      imagePullPolicy: IfNotPresent
      name: dags-git-clone
      resources: {}
      securityContext:
        runAsGroup: 65533
        runAsUser: 65533
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /dags
        name: dags-data
      - mountPath: /etc/git-secret/id_rsa
        name: git-secret
        readOnly: true
        subPath: gitSshKey
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w569d
        readOnly: true
    - args:
      - bash
      - -c
      - exec timeout 60s airflow db check
      command:
      - /usr/bin/dumb-init
      - --
      - /entrypoint
      env:
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: CONNECTION_CHECK_MAX_COUNT
        value: "0"
      - name: AIRFLOW__METRICS__STATSD_HOST
        value: prometheus-statsd-exporter
      - name: AIRFLOW__METRICS__STATSD_PORT
        value: "9125"
      - name: AIRFLOW__METRICS__STATSD_ON
        value: "true"
      envFrom:
      - secretRef:
          name: airflow-config-envs
      image: neinver2024/apacheairflow:2.6.3-python3.11
      imagePullPolicy: IfNotPresent
      name: check-db
      resources: {}
      securityContext:
        runAsGroup: 0
        runAsUser: 50000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/airflow/dags
        name: dags-data
      - mountPath: /opt/airflow/logs
        name: logs-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w569d
        readOnly: true
    - args:
      - bash
      - -c
      - exec airflow db check-migrations -t 60
      command:
      - /usr/bin/dumb-init
      - --
      - /entrypoint
      env:
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: CONNECTION_CHECK_MAX_COUNT
        value: "0"
      - name: AIRFLOW__METRICS__STATSD_HOST
        value: prometheus-statsd-exporter
      - name: AIRFLOW__METRICS__STATSD_PORT
        value: "9125"
      - name: AIRFLOW__METRICS__STATSD_ON
        value: "true"
      envFrom:
      - secretRef:
          name: airflow-config-envs
      image: neinver2024/apacheairflow:2.6.3-python3.11
      imagePullPolicy: IfNotPresent
      name: wait-for-db-migrations
      resources: {}
      securityContext:
        runAsGroup: 0
        runAsUser: 50000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/airflow/dags
        name: dags-data
      - mountPath: /opt/airflow/logs
        name: logs-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w569d
        readOnly: true
    nodeName: worker-3
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 0
    serviceAccount: airflow
    serviceAccountName: airflow
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: dags-data
    - name: logs-data
      persistentVolumeClaim:
        claimName: airflow-logs
    - name: git-secret
      secret:
        defaultMode: 420
        secretName: airflow-ssh-secret
    - name: kube-api-access-w569d
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-14T05:13:35Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-14T05:13:36Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-14T05:13:36Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-14T05:12:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://e533dcd797dce83ab29ec1b1480c76264b3c2eb7bb70cf09f7ba8a05fa55dd96
      image: neinver2024/apacheairflow:2.6.3-python3.11
      imageID: docker-pullable://neinver2024/apacheairflow@sha256:35afb4acf4f67c73c84428d57f7e58fa3d247c690f7c01d91c9f7e2f59aff800
      lastState: {}
      name: airflow-triggerer
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-14T05:13:35Z"
    - containerID: docker://e5fbc7648ecffbe4d351da0da76f6139946f8cce9632cfed90630dd03baa6770
      image: registry.k8s.io/git-sync/git-sync:v3.6.5
      imageID: docker-pullable://registry.k8s.io/git-sync/git-sync@sha256:7231f6c2284758b91caed71e4e596413df31ac4467de9b596dc6b386b82f624f
      lastState: {}
      name: dags-git-sync
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-14T05:13:35Z"
    hostIP: 10.2.1.185
    initContainerStatuses:
    - containerID: docker://b0f173d78b45c90d52dc2c08be844ea9d8ca1d0e4ef586a9bef8b5681e584c16
      image: registry.k8s.io/git-sync/git-sync:v3.6.5
      imageID: docker-pullable://registry.k8s.io/git-sync/git-sync@sha256:7231f6c2284758b91caed71e4e596413df31ac4467de9b596dc6b386b82f624f
      lastState: {}
      name: dags-git-clone
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://b0f173d78b45c90d52dc2c08be844ea9d8ca1d0e4ef586a9bef8b5681e584c16
          exitCode: 0
          finishedAt: "2024-03-14T05:12:14Z"
          reason: Completed
          startedAt: "2024-03-14T05:12:13Z"
    - containerID: docker://cd652d83d1264ff98a98577cc6005dd903feb4ca8d99eba5ff1259ad6dbc1372
      image: neinver2024/apacheairflow:2.6.3-python3.11
      imageID: docker-pullable://neinver2024/apacheairflow@sha256:35afb4acf4f67c73c84428d57f7e58fa3d247c690f7c01d91c9f7e2f59aff800
      lastState: {}
      name: check-db
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://cd652d83d1264ff98a98577cc6005dd903feb4ca8d99eba5ff1259ad6dbc1372
          exitCode: 0
          finishedAt: "2024-03-14T05:13:23Z"
          reason: Completed
          startedAt: "2024-03-14T05:13:17Z"
    - containerID: docker://3fd8f39ba6b69c1d0c4c4fdcb97ee82cc4a0a7e181525109f4cfa3a2b89110a6
      image: neinver2024/apacheairflow:2.6.3-python3.11
      imageID: docker-pullable://neinver2024/apacheairflow@sha256:35afb4acf4f67c73c84428d57f7e58fa3d247c690f7c01d91c9f7e2f59aff800
      lastState: {}
      name: wait-for-db-migrations
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://3fd8f39ba6b69c1d0c4c4fdcb97ee82cc4a0a7e181525109f4cfa3a2b89110a6
          exitCode: 0
          finishedAt: "2024-03-14T05:13:34Z"
          reason: Completed
          startedAt: "2024-03-14T05:13:24Z"
    phase: Running
    podIP: 10.42.97.218
    podIPs:
    - ip: 10.42.97.218
    qosClass: BestEffort
    startTime: "2024-03-14T05:12:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config-webserver-config: 08f3293f0aaa3978bcbcae61eee58760bcba270cf691c5ff9c788d678ba8d579
      checksum/secret-config-envs: 7b5b4b568e7c694c0f44651459436d8876c0d6f337d3876162036cffceec3b68
      checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      cni.projectcalico.org/containerID: 3e1fe099c5f50bc80b9ca765c8fb47311bb3f5860c3b83cac206924f47c1d43e
      cni.projectcalico.org/podIP: 10.42.38.93/32
      cni.projectcalico.org/podIPs: 10.42.38.93/32
    creationTimestamp: "2024-03-14T05:12:12Z"
    generateName: airflow-web-5c757c7656-
    labels:
      app: airflow
      component: web
      pod-template-hash: 5c757c7656
      release: airflow
    name: airflow-web-5c757c7656-hwb6f
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: airflow-web-5c757c7656
      uid: 0f27fcb2-ba16-47b9-81bc-2141939535a1
    resourceVersion: "13231984"
    uid: 8c1e3df2-c26f-4b1b-b34f-8de54ccf0508
  spec:
    affinity: {}
    containers:
    - args:
      - bash
      - -c
      - exec airflow webserver
      command:
      - /usr/bin/dumb-init
      - --
      - /entrypoint
      env:
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: CONNECTION_CHECK_MAX_COUNT
        value: "0"
      - name: AIRFLOW__METRICS__STATSD_HOST
        value: prometheus-statsd-exporter
      - name: AIRFLOW__METRICS__STATSD_PORT
        value: "9125"
      - name: AIRFLOW__METRICS__STATSD_ON
        value: "true"
      envFrom:
      - secretRef:
          name: airflow-config-envs
      image: neinver2024/apacheairflow:2.6.3-python3.11
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 6
        httpGet:
          path: /airflow/health
          port: web
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: airflow-web
      ports:
      - containerPort: 8080
        name: web
        protocol: TCP
      readinessProbe:
        failureThreshold: 6
        httpGet:
          path: /airflow/health
          port: web
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      securityContext:
        runAsGroup: 0
        runAsUser: 50000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/airflow/dags
        name: dags-data
      - mountPath: /opt/airflow/logs
        name: logs-data
      - mountPath: /opt/airflow/webserver_config.py
        name: webserver-config
        readOnly: true
        subPath: webserver_config.py
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kht4c
        readOnly: true
    - env:
      - name: GIT_SYNC_ROOT
        value: /dags
      - name: GIT_SYNC_DEST
        value: repo
      - name: GIT_SYNC_REPO
        value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
      - name: GIT_SYNC_BRANCH
        value: main
      - name: GIT_SYNC_REV
        value: HEAD
      - name: GIT_SYNC_DEPTH
        value: "1"
      - name: GIT_SYNC_WAIT
        value: "60"
      - name: GIT_SYNC_TIMEOUT
        value: "120"
      - name: GIT_SYNC_ADD_USER
        value: "true"
      - name: GIT_SYNC_MAX_SYNC_FAILURES
        value: "0"
      - name: GIT_SYNC_SUBMODULES
        value: recursive
      - name: GIT_SYNC_SSH
        value: "true"
      - name: GIT_SSH_KEY_FILE
        value: /etc/git-secret/id_rsa
      - name: GIT_KNOWN_HOSTS
        value: "false"
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: CONNECTION_CHECK_MAX_COUNT
        value: "0"
      - name: AIRFLOW__METRICS__STATSD_HOST
        value: prometheus-statsd-exporter
      - name: AIRFLOW__METRICS__STATSD_PORT
        value: "9125"
      - name: AIRFLOW__METRICS__STATSD_ON
        value: "true"
      envFrom:
      - secretRef:
          name: airflow-config-envs
      image: registry.k8s.io/git-sync/git-sync:v3.6.5
      imagePullPolicy: IfNotPresent
      name: dags-git-sync
      resources: {}
      securityContext:
        runAsGroup: 65533
        runAsUser: 65533
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /dags
        name: dags-data
      - mountPath: /etc/git-secret/id_rsa
        name: git-secret
        readOnly: true
        subPath: gitSshKey
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kht4c
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - env:
      - name: GIT_SYNC_ONE_TIME
        value: "true"
      - name: GIT_SYNC_ROOT
        value: /dags
      - name: GIT_SYNC_DEST
        value: repo
      - name: GIT_SYNC_REPO
        value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
      - name: GIT_SYNC_BRANCH
        value: main
      - name: GIT_SYNC_REV
        value: HEAD
      - name: GIT_SYNC_DEPTH
        value: "1"
      - name: GIT_SYNC_WAIT
        value: "60"
      - name: GIT_SYNC_TIMEOUT
        value: "120"
      - name: GIT_SYNC_ADD_USER
        value: "true"
      - name: GIT_SYNC_MAX_SYNC_FAILURES
        value: "0"
      - name: GIT_SYNC_SUBMODULES
        value: recursive
      - name: GIT_SYNC_SSH
        value: "true"
      - name: GIT_SSH_KEY_FILE
        value: /etc/git-secret/id_rsa
      - name: GIT_KNOWN_HOSTS
        value: "false"
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: CONNECTION_CHECK_MAX_COUNT
        value: "0"
      - name: AIRFLOW__METRICS__STATSD_HOST
        value: prometheus-statsd-exporter
      - name: AIRFLOW__METRICS__STATSD_PORT
        value: "9125"
      - name: AIRFLOW__METRICS__STATSD_ON
        value: "true"
      envFrom:
      - secretRef:
          name: airflow-config-envs
      image: registry.k8s.io/git-sync/git-sync:v3.6.5
      imagePullPolicy: IfNotPresent
      name: dags-git-clone
      resources: {}
      securityContext:
        runAsGroup: 65533
        runAsUser: 65533
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /dags
        name: dags-data
      - mountPath: /etc/git-secret/id_rsa
        name: git-secret
        readOnly: true
        subPath: gitSshKey
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kht4c
        readOnly: true
    - args:
      - bash
      - -c
      - exec timeout 60s airflow db check
      command:
      - /usr/bin/dumb-init
      - --
      - /entrypoint
      env:
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: CONNECTION_CHECK_MAX_COUNT
        value: "0"
      - name: AIRFLOW__METRICS__STATSD_HOST
        value: prometheus-statsd-exporter
      - name: AIRFLOW__METRICS__STATSD_PORT
        value: "9125"
      - name: AIRFLOW__METRICS__STATSD_ON
        value: "true"
      envFrom:
      - secretRef:
          name: airflow-config-envs
      image: neinver2024/apacheairflow:2.6.3-python3.11
      imagePullPolicy: IfNotPresent
      name: check-db
      resources: {}
      securityContext:
        runAsGroup: 0
        runAsUser: 50000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/airflow/dags
        name: dags-data
      - mountPath: /opt/airflow/logs
        name: logs-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kht4c
        readOnly: true
    - args:
      - bash
      - -c
      - exec airflow db check-migrations -t 60
      command:
      - /usr/bin/dumb-init
      - --
      - /entrypoint
      env:
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: CONNECTION_CHECK_MAX_COUNT
        value: "0"
      - name: AIRFLOW__METRICS__STATSD_HOST
        value: prometheus-statsd-exporter
      - name: AIRFLOW__METRICS__STATSD_PORT
        value: "9125"
      - name: AIRFLOW__METRICS__STATSD_ON
        value: "true"
      envFrom:
      - secretRef:
          name: airflow-config-envs
      image: neinver2024/apacheairflow:2.6.3-python3.11
      imagePullPolicy: IfNotPresent
      name: wait-for-db-migrations
      resources: {}
      securityContext:
        runAsGroup: 0
        runAsUser: 50000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/airflow/dags
        name: dags-data
      - mountPath: /opt/airflow/logs
        name: logs-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kht4c
        readOnly: true
    nodeName: worker-4
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 0
    serviceAccount: airflow
    serviceAccountName: airflow
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: dags-data
    - name: logs-data
      persistentVolumeClaim:
        claimName: airflow-logs
    - name: git-secret
      secret:
        defaultMode: 420
        secretName: airflow-ssh-secret
    - name: webserver-config
      secret:
        defaultMode: 420
        secretName: airflow-webserver-config
    - name: kube-api-access-kht4c
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-14T05:13:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-14T05:14:29Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-14T05:14:29Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-14T05:12:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://e533959f492ff51df82d3391e9aa4a929a2221e5397a4db9955b37c55c280520
      image: neinver2024/apacheairflow:2.6.3-python3.11
      imageID: docker-pullable://neinver2024/apacheairflow@sha256:35afb4acf4f67c73c84428d57f7e58fa3d247c690f7c01d91c9f7e2f59aff800
      lastState: {}
      name: airflow-web
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-14T05:13:43Z"
    - containerID: docker://82300cb774b6d10ab5a7ccf01d6bc052f68716deba512fb4ce1055481d37a60a
      image: registry.k8s.io/git-sync/git-sync:v3.6.5
      imageID: docker-pullable://registry.k8s.io/git-sync/git-sync@sha256:7231f6c2284758b91caed71e4e596413df31ac4467de9b596dc6b386b82f624f
      lastState: {}
      name: dags-git-sync
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-14T05:13:43Z"
    hostIP: 10.2.1.186
    initContainerStatuses:
    - containerID: docker://734296f340b8480071f2420c8549a95d8dc65375c8704da39bf5cd9a97741e08
      image: registry.k8s.io/git-sync/git-sync:v3.6.5
      imageID: docker-pullable://registry.k8s.io/git-sync/git-sync@sha256:7231f6c2284758b91caed71e4e596413df31ac4467de9b596dc6b386b82f624f
      lastState: {}
      name: dags-git-clone
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://734296f340b8480071f2420c8549a95d8dc65375c8704da39bf5cd9a97741e08
          exitCode: 0
          finishedAt: "2024-03-14T05:12:21Z"
          reason: Completed
          startedAt: "2024-03-14T05:12:20Z"
    - containerID: docker://736edbdc115df0fdd17e981de8b05452d75aa6e5a588f98e638915d195d537f0
      image: neinver2024/apacheairflow:2.6.3-python3.11
      imageID: docker-pullable://neinver2024/apacheairflow@sha256:35afb4acf4f67c73c84428d57f7e58fa3d247c690f7c01d91c9f7e2f59aff800
      lastState: {}
      name: check-db
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://736edbdc115df0fdd17e981de8b05452d75aa6e5a588f98e638915d195d537f0
          exitCode: 0
          finishedAt: "2024-03-14T05:13:29Z"
          reason: Completed
          startedAt: "2024-03-14T05:13:23Z"
    - containerID: docker://59ef6e3e6ab0734da40fa85c216ada649b89ade31186803abad3c39898a9e934
      image: neinver2024/apacheairflow:2.6.3-python3.11
      imageID: docker-pullable://neinver2024/apacheairflow@sha256:35afb4acf4f67c73c84428d57f7e58fa3d247c690f7c01d91c9f7e2f59aff800
      lastState: {}
      name: wait-for-db-migrations
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://59ef6e3e6ab0734da40fa85c216ada649b89ade31186803abad3c39898a9e934
          exitCode: 0
          finishedAt: "2024-03-14T05:13:43Z"
          reason: Completed
          startedAt: "2024-03-14T05:13:30Z"
    phase: Running
    podIP: 10.42.38.93
    podIPs:
    - ip: 10.42.38.93
    qosClass: BestEffort
    startTime: "2024-03-14T05:12:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: c34dd871b6f2cee3ecb0cc52d359ad495500ad49dad06bc61a06e01b59f4ac0f
      cni.projectcalico.org/podIP: 10.42.133.218/32
      cni.projectcalico.org/podIPs: 10.42.133.218/32
    creationTimestamp: "2024-03-13T16:56:14Z"
    generateName: postgresql-ha-pgpool-76bcdbc66f-
    labels:
      app.kubernetes.io/component: pgpool
      app.kubernetes.io/instance: postgresql-ha
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql-ha
      app.kubernetes.io/version: 4.5.0
      helm.sh/chart: postgresql-ha-13.2.3
      pod-template-hash: 76bcdbc66f
    name: postgresql-ha-pgpool-76bcdbc66f-fn9hd
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: postgresql-ha-pgpool-76bcdbc66f
      uid: ebef0d66-d9ee-4174-8113-e9c1141107b4
    resourceVersion: "13087967"
    uid: 3edbe222-f2a2-4361-b533-87f045cff576
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: pgpool
              app.kubernetes.io/instance: postgresql-ha
              app.kubernetes.io/name: postgresql-ha
          topologyKey: kubernetes.io/hostname
    automountServiceAccountToken: false
    containers:
    - env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: PGPOOL_BACKEND_NODES
        value: 0:postgresql-ha-postgresql-0.postgresql-ha-postgresql-headless:5432,1:postgresql-ha-postgresql-1.postgresql-ha-postgresql-headless:5432,2:postgresql-ha-postgresql-2.postgresql-ha-postgresql-headless:5432,
      - name: PGPOOL_SR_CHECK_USER
        value: repmgr
      - name: PGPOOL_SR_CHECK_PASSWORD
        valueFrom:
          secretKeyRef:
            key: repmgr-password
            name: postgresql-ha-postgresql
      - name: PGPOOL_SR_CHECK_DATABASE
        value: postgres
      - name: PGPOOL_ENABLE_LDAP
        value: "no"
      - name: PGPOOL_POSTGRES_USERNAME
        value: postgres
      - name: PGPOOL_POSTGRES_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: PGPOOL_ADMIN_USERNAME
        value: admin
      - name: PGPOOL_ADMIN_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: postgresql-ha-pgpool
      - name: PGPOOL_AUTHENTICATION_METHOD
        value: scram-sha-256
      - name: PGPOOL_ENABLE_LOAD_BALANCING
        value: "yes"
      - name: PGPOOL_DISABLE_LOAD_BALANCE_ON_WRITE
        value: transaction
      - name: PGPOOL_ENABLE_LOG_CONNECTIONS
        value: "no"
      - name: PGPOOL_ENABLE_LOG_HOSTNAME
        value: "yes"
      - name: PGPOOL_ENABLE_LOG_PER_NODE_STATEMENT
        value: "no"
      - name: PGPOOL_RESERVED_CONNECTIONS
        value: "1"
      - name: PGPOOL_CHILD_LIFE_TIME
      - name: PGPOOL_ENABLE_TLS
        value: "no"
      - name: PGPOOL_HEALTH_CHECK_PSQL_TIMEOUT
        value: "6"
      image: docker.io/bitnami/pgpool:4.5.0-debian-11-r4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /opt/bitnami/scripts/pgpool/healthcheck.sh
        failureThreshold: 5
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: pgpool
      ports:
      - containerPort: 5432
        name: postgresql
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - bash
          - -ec
          - PGPASSWORD=${PGPOOL_POSTGRES_PASSWORD} psql -U "postgres" -d "airflow"
            -h /opt/bitnami/pgpool/tmp -tA -c "SELECT 1" >/dev/null
        failureThreshold: 5
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: false
        runAsNonRoot: true
        runAsUser: 1001
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker-2
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: Always
    serviceAccount: postgresql-ha
    serviceAccountName: postgresql-ha
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:56:14Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:56:19Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:56:19Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:56:14Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://e6e811f7273feee5a76d045cdf316eab0cd2dbe2fc6efd0e95132726e65bbba2
      image: bitnami/pgpool:4.5.0-debian-11-r4
      imageID: docker-pullable://bitnami/pgpool@sha256:b8330250f32f169231c0f56e6473ecd65239bd3e78fadc8b402b4814bc73ab52
      lastState: {}
      name: pgpool
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:56:14Z"
    hostIP: 10.2.1.184
    phase: Running
    podIP: 10.42.133.218
    podIPs:
    - ip: 10.42.133.218
    qosClass: BestEffort
    startTime: "2024-03-13T16:56:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 130366d421afb616e2a370c406692228f02290c6a9e54ff264f110a55b5aa006
      cni.projectcalico.org/podIP: 10.42.133.210/32
      cni.projectcalico.org/podIPs: 10.42.133.210/32
    creationTimestamp: "2024-03-13T16:50:38Z"
    generateName: postgresql-ha-postgresql-
    labels:
      app.kubernetes.io/component: postgresql
      app.kubernetes.io/instance: postgresql-ha
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql-ha
      app.kubernetes.io/version: 16.2.0
      controller-revision-hash: postgresql-ha-postgresql-5bf76cd696
      helm.sh/chart: postgresql-ha-13.2.3
      role: data
      statefulset.kubernetes.io/pod-name: postgresql-ha-postgresql-0
    name: postgresql-ha-postgresql-0
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: postgresql-ha-postgresql
      uid: 6b9ab1b3-c15a-4c34-9379-7698e6a5fce2
    resourceVersion: "13087265"
    uid: 0403a737-8071-414b-a4f0-8490ed15f904
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: postgresql
              app.kubernetes.io/instance: postgresql-ha
              app.kubernetes.io/name: postgresql-ha
          topologyKey: kubernetes.io/hostname
    automountServiceAccountToken: false
    containers:
    - env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: POSTGRESQL_VOLUME_DIR
        value: /bitnami/postgresql
      - name: PGDATA
        value: /bitnami/postgresql/data
      - name: POSTGRES_USER
        value: postgres
      - name: POSTGRES_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: POSTGRES_DB
        value: airflow
      - name: POSTGRESQL_LOG_HOSTNAME
        value: "true"
      - name: POSTGRESQL_LOG_CONNECTIONS
        value: "false"
      - name: POSTGRESQL_LOG_DISCONNECTIONS
        value: "false"
      - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
        value: "off"
      - name: POSTGRESQL_CLIENT_MIN_MESSAGES
        value: error
      - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
        value: pgaudit, repmgr
      - name: POSTGRESQL_ENABLE_TLS
        value: "no"
      - name: POSTGRESQL_PORT_NUMBER
        value: "5432"
      - name: REPMGR_PORT_NUMBER
        value: "5432"
      - name: REPMGR_PRIMARY_PORT
        value: "5432"
      - name: MY_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: REPMGR_UPGRADE_EXTENSION
        value: "no"
      - name: REPMGR_PGHBA_TRUST_ALL
        value: "no"
      - name: REPMGR_MOUNTED_CONF_DIR
        value: /bitnami/repmgr/conf
      - name: REPMGR_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: REPMGR_PARTNER_NODES
        value: postgresql-ha-postgresql-0.postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local,postgresql-ha-postgresql-1.postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local,postgresql-ha-postgresql-2.postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local,
      - name: REPMGR_PRIMARY_HOST
        value: postgresql-ha-postgresql-0.postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local
      - name: REPMGR_NODE_NAME
        value: $(MY_POD_NAME)
      - name: REPMGR_NODE_NETWORK_NAME
        value: $(MY_POD_NAME).postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local
      - name: REPMGR_NODE_TYPE
        value: data
      - name: REPMGR_LOG_LEVEL
        value: NOTICE
      - name: REPMGR_CONNECT_TIMEOUT
        value: "5"
      - name: REPMGR_RECONNECT_ATTEMPTS
        value: "2"
      - name: REPMGR_RECONNECT_INTERVAL
        value: "3"
      - name: REPMGR_USERNAME
        value: repmgr
      - name: REPMGR_PASSWORD
        valueFrom:
          secretKeyRef:
            key: repmgr-password
            name: postgresql-ha-postgresql
      - name: REPMGR_DATABASE
        value: repmgr
      - name: REPMGR_FENCE_OLD_PRIMARY
        value: "no"
      - name: REPMGR_CHILD_NODES_CHECK_INTERVAL
        value: "5"
      - name: REPMGR_CHILD_NODES_CONNECTED_MIN_COUNT
        value: "1"
      - name: REPMGR_CHILD_NODES_DISCONNECT_TIMEOUT
        value: "30"
      image: docker.io/bitnami/postgresql-repmgr:16.2.0-debian-11-r1
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /pre-stop.sh
            - "25"
      livenessProbe:
        exec:
          command:
          - bash
          - -ec
          - PGPASSWORD=$POSTGRES_PASSWORD psql -w -U "postgres" -d "airflow" -h 127.0.0.1
            -p 5432 -c "SELECT 1"
        failureThreshold: 6
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: postgresql
      ports:
      - containerPort: 5432
        name: postgresql
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - bash
          - -ec
          - PGPASSWORD=$POSTGRES_PASSWORD psql -w -U "postgres" -d "airflow" -h 127.0.0.1
            -p 5432 -c "SELECT 1"
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: false
        runAsNonRoot: true
        runAsUser: 1001
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bitnami/postgresql
        name: data
      - mountPath: /pre-stop.sh
        name: hooks-scripts
        subPath: pre-stop.sh
      - mountPath: /readiness-probe.sh
        name: hooks-scripts
        subPath: readiness-probe.sh
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: postgresql-ha-postgresql-0
    nodeName: worker-2
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: Always
    serviceAccount: postgresql-ha
    serviceAccountName: postgresql-ha
    subdomain: postgresql-ha-postgresql-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-postgresql-ha-postgresql-0
    - configMap:
        defaultMode: 493
        name: postgresql-ha-postgresql-hooks-scripts
      name: hooks-scripts
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:38Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:54:01Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:54:01Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:38Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://f2788671e2b1b018c72bbd3618c9cf9ee8f2870cc94ee47f0df093d06a50607d
      image: bitnami/postgresql-repmgr:16.2.0-debian-11-r1
      imageID: docker-pullable://bitnami/postgresql-repmgr@sha256:9b9e7875d8ab37ae7d9f5a974f98b92c98ad53fdc496022a62efa9a018d281d7
      lastState:
        terminated:
          containerID: docker://2366ef52a5a715d0dcb00c8def86c8a9551903ba6a8e24d6e8fdea8fe708da6f
          exitCode: 6
          finishedAt: "2024-03-13T16:52:31Z"
          reason: Error
          startedAt: "2024-03-13T16:52:30Z"
      name: postgresql
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:53:56Z"
    hostIP: 10.2.1.184
    phase: Running
    podIP: 10.42.133.210
    podIPs:
    - ip: 10.42.133.210
    qosClass: BestEffort
    startTime: "2024-03-13T16:50:38Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: ee1505db59ae55f74274268d60ae13e254471df165f3f6e62124081c6a350a80
      cni.projectcalico.org/podIP: 10.42.38.82/32
      cni.projectcalico.org/podIPs: 10.42.38.82/32
    creationTimestamp: "2024-03-13T16:50:48Z"
    generateName: postgresql-ha-postgresql-
    labels:
      app.kubernetes.io/component: postgresql
      app.kubernetes.io/instance: postgresql-ha
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql-ha
      app.kubernetes.io/version: 16.2.0
      controller-revision-hash: postgresql-ha-postgresql-5bf76cd696
      helm.sh/chart: postgresql-ha-13.2.3
      role: data
      statefulset.kubernetes.io/pod-name: postgresql-ha-postgresql-1
    name: postgresql-ha-postgresql-1
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: postgresql-ha-postgresql
      uid: 6b9ab1b3-c15a-4c34-9379-7698e6a5fce2
    resourceVersion: "13086914"
    uid: ec7627ec-e54b-4e3a-b1ce-f0ea2cbf89b7
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: postgresql
              app.kubernetes.io/instance: postgresql-ha
              app.kubernetes.io/name: postgresql-ha
          topologyKey: kubernetes.io/hostname
    automountServiceAccountToken: false
    containers:
    - env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: POSTGRESQL_VOLUME_DIR
        value: /bitnami/postgresql
      - name: PGDATA
        value: /bitnami/postgresql/data
      - name: POSTGRES_USER
        value: postgres
      - name: POSTGRES_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: POSTGRES_DB
        value: airflow
      - name: POSTGRESQL_LOG_HOSTNAME
        value: "true"
      - name: POSTGRESQL_LOG_CONNECTIONS
        value: "false"
      - name: POSTGRESQL_LOG_DISCONNECTIONS
        value: "false"
      - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
        value: "off"
      - name: POSTGRESQL_CLIENT_MIN_MESSAGES
        value: error
      - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
        value: pgaudit, repmgr
      - name: POSTGRESQL_ENABLE_TLS
        value: "no"
      - name: POSTGRESQL_PORT_NUMBER
        value: "5432"
      - name: REPMGR_PORT_NUMBER
        value: "5432"
      - name: REPMGR_PRIMARY_PORT
        value: "5432"
      - name: MY_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: REPMGR_UPGRADE_EXTENSION
        value: "no"
      - name: REPMGR_PGHBA_TRUST_ALL
        value: "no"
      - name: REPMGR_MOUNTED_CONF_DIR
        value: /bitnami/repmgr/conf
      - name: REPMGR_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: REPMGR_PARTNER_NODES
        value: postgresql-ha-postgresql-0.postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local,postgresql-ha-postgresql-1.postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local,postgresql-ha-postgresql-2.postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local,
      - name: REPMGR_PRIMARY_HOST
        value: postgresql-ha-postgresql-0.postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local
      - name: REPMGR_NODE_NAME
        value: $(MY_POD_NAME)
      - name: REPMGR_NODE_NETWORK_NAME
        value: $(MY_POD_NAME).postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local
      - name: REPMGR_NODE_TYPE
        value: data
      - name: REPMGR_LOG_LEVEL
        value: NOTICE
      - name: REPMGR_CONNECT_TIMEOUT
        value: "5"
      - name: REPMGR_RECONNECT_ATTEMPTS
        value: "2"
      - name: REPMGR_RECONNECT_INTERVAL
        value: "3"
      - name: REPMGR_USERNAME
        value: repmgr
      - name: REPMGR_PASSWORD
        valueFrom:
          secretKeyRef:
            key: repmgr-password
            name: postgresql-ha-postgresql
      - name: REPMGR_DATABASE
        value: repmgr
      - name: REPMGR_FENCE_OLD_PRIMARY
        value: "no"
      - name: REPMGR_CHILD_NODES_CHECK_INTERVAL
        value: "5"
      - name: REPMGR_CHILD_NODES_CONNECTED_MIN_COUNT
        value: "1"
      - name: REPMGR_CHILD_NODES_DISCONNECT_TIMEOUT
        value: "30"
      image: docker.io/bitnami/postgresql-repmgr:16.2.0-debian-11-r1
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /pre-stop.sh
            - "25"
      livenessProbe:
        exec:
          command:
          - bash
          - -ec
          - PGPASSWORD=$POSTGRES_PASSWORD psql -w -U "postgres" -d "airflow" -h 127.0.0.1
            -p 5432 -c "SELECT 1"
        failureThreshold: 6
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: postgresql
      ports:
      - containerPort: 5432
        name: postgresql
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - bash
          - -ec
          - PGPASSWORD=$POSTGRES_PASSWORD psql -w -U "postgres" -d "airflow" -h 127.0.0.1
            -p 5432 -c "SELECT 1"
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: false
        runAsNonRoot: true
        runAsUser: 1001
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bitnami/postgresql
        name: data
      - mountPath: /pre-stop.sh
        name: hooks-scripts
        subPath: pre-stop.sh
      - mountPath: /readiness-probe.sh
        name: hooks-scripts
        subPath: readiness-probe.sh
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: postgresql-ha-postgresql-1
    nodeName: worker-4
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: Always
    serviceAccount: postgresql-ha
    serviceAccountName: postgresql-ha
    subdomain: postgresql-ha-postgresql-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-postgresql-ha-postgresql-1
    - configMap:
        defaultMode: 493
        name: postgresql-ha-postgresql-hooks-scripts
      name: hooks-scripts
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:48Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:52:43Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:52:43Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:48Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://8cdbf82f9e4f06d599a0e34e8ba6e26d7fa67ab989ec6a6af249a78a6415bf58
      image: bitnami/postgresql-repmgr:16.2.0-debian-11-r1
      imageID: docker-pullable://bitnami/postgresql-repmgr@sha256:9b9e7875d8ab37ae7d9f5a974f98b92c98ad53fdc496022a62efa9a018d281d7
      lastState: {}
      name: postgresql
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:52:34Z"
    hostIP: 10.2.1.186
    phase: Running
    podIP: 10.42.38.82
    podIPs:
    - ip: 10.42.38.82
    qosClass: BestEffort
    startTime: "2024-03-13T16:50:48Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: ccd47076bd89f32d5463fd45166ffd17d117d826d8bdf9f547b2af2a76af5c68
      cni.projectcalico.org/podIP: 10.42.226.85/32
      cni.projectcalico.org/podIPs: 10.42.226.85/32
    creationTimestamp: "2024-03-13T16:46:56Z"
    generateName: postgresql-ha-postgresql-
    labels:
      app.kubernetes.io/component: postgresql
      app.kubernetes.io/instance: postgresql-ha
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql-ha
      app.kubernetes.io/version: 16.2.0
      controller-revision-hash: postgresql-ha-postgresql-5bf76cd696
      helm.sh/chart: postgresql-ha-13.2.3
      role: data
      statefulset.kubernetes.io/pod-name: postgresql-ha-postgresql-2
    name: postgresql-ha-postgresql-2
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: postgresql-ha-postgresql
      uid: 6b9ab1b3-c15a-4c34-9379-7698e6a5fce2
    resourceVersion: "13087204"
    uid: 5faba08d-990a-40a7-9906-ddb0a9da7373
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: postgresql
              app.kubernetes.io/instance: postgresql-ha
              app.kubernetes.io/name: postgresql-ha
          topologyKey: kubernetes.io/hostname
    automountServiceAccountToken: false
    containers:
    - env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: POSTGRESQL_VOLUME_DIR
        value: /bitnami/postgresql
      - name: PGDATA
        value: /bitnami/postgresql/data
      - name: POSTGRES_USER
        value: postgres
      - name: POSTGRES_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: postgresql-ha-postgresql
      - name: POSTGRES_DB
        value: airflow
      - name: POSTGRESQL_LOG_HOSTNAME
        value: "true"
      - name: POSTGRESQL_LOG_CONNECTIONS
        value: "false"
      - name: POSTGRESQL_LOG_DISCONNECTIONS
        value: "false"
      - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
        value: "off"
      - name: POSTGRESQL_CLIENT_MIN_MESSAGES
        value: error
      - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
        value: pgaudit, repmgr
      - name: POSTGRESQL_ENABLE_TLS
        value: "no"
      - name: POSTGRESQL_PORT_NUMBER
        value: "5432"
      - name: REPMGR_PORT_NUMBER
        value: "5432"
      - name: REPMGR_PRIMARY_PORT
        value: "5432"
      - name: MY_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: REPMGR_UPGRADE_EXTENSION
        value: "no"
      - name: REPMGR_PGHBA_TRUST_ALL
        value: "no"
      - name: REPMGR_MOUNTED_CONF_DIR
        value: /bitnami/repmgr/conf
      - name: REPMGR_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: REPMGR_PARTNER_NODES
        value: postgresql-ha-postgresql-0.postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local,postgresql-ha-postgresql-1.postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local,postgresql-ha-postgresql-2.postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local,
      - name: REPMGR_PRIMARY_HOST
        value: postgresql-ha-postgresql-0.postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local
      - name: REPMGR_NODE_NAME
        value: $(MY_POD_NAME)
      - name: REPMGR_NODE_NETWORK_NAME
        value: $(MY_POD_NAME).postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local
      - name: REPMGR_NODE_TYPE
        value: data
      - name: REPMGR_LOG_LEVEL
        value: NOTICE
      - name: REPMGR_CONNECT_TIMEOUT
        value: "5"
      - name: REPMGR_RECONNECT_ATTEMPTS
        value: "2"
      - name: REPMGR_RECONNECT_INTERVAL
        value: "3"
      - name: REPMGR_USERNAME
        value: repmgr
      - name: REPMGR_PASSWORD
        valueFrom:
          secretKeyRef:
            key: repmgr-password
            name: postgresql-ha-postgresql
      - name: REPMGR_DATABASE
        value: repmgr
      - name: REPMGR_FENCE_OLD_PRIMARY
        value: "no"
      - name: REPMGR_CHILD_NODES_CHECK_INTERVAL
        value: "5"
      - name: REPMGR_CHILD_NODES_CONNECTED_MIN_COUNT
        value: "1"
      - name: REPMGR_CHILD_NODES_DISCONNECT_TIMEOUT
        value: "30"
      image: docker.io/bitnami/postgresql-repmgr:16.2.0-debian-11-r1
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /pre-stop.sh
            - "25"
      livenessProbe:
        exec:
          command:
          - bash
          - -ec
          - PGPASSWORD=$POSTGRES_PASSWORD psql -w -U "postgres" -d "airflow" -h 127.0.0.1
            -p 5432 -c "SELECT 1"
        failureThreshold: 6
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: postgresql
      ports:
      - containerPort: 5432
        name: postgresql
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - bash
          - -ec
          - PGPASSWORD=$POSTGRES_PASSWORD psql -w -U "postgres" -d "airflow" -h 127.0.0.1
            -p 5432 -c "SELECT 1"
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: false
        runAsNonRoot: true
        runAsUser: 1001
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bitnami/postgresql
        name: data
      - mountPath: /pre-stop.sh
        name: hooks-scripts
        subPath: pre-stop.sh
      - mountPath: /readiness-probe.sh
        name: hooks-scripts
        subPath: readiness-probe.sh
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: postgresql-ha-postgresql-2
    nodeName: worker-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: Always
    serviceAccount: postgresql-ha
    serviceAccountName: postgresql-ha
    subdomain: postgresql-ha-postgresql-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-postgresql-ha-postgresql-2
    - configMap:
        defaultMode: 493
        name: postgresql-ha-postgresql-hooks-scripts
      name: hooks-scripts
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:46:56Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:53:52Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:53:52Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:46:56Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://3b9bff4e9ccad2bf6f87aec44c546575db8f8f44959a3c5e463554cde771f87c
      image: bitnami/postgresql-repmgr:16.2.0-debian-11-r1
      imageID: docker-pullable://bitnami/postgresql-repmgr@sha256:9b9e7875d8ab37ae7d9f5a974f98b92c98ad53fdc496022a62efa9a018d281d7
      lastState:
        terminated:
          containerID: docker://004cd98c84fa687957045754e5a462576e674bea281e5c9be4133b3dd3aeeeb2
          exitCode: 1
          finishedAt: "2024-03-13T16:53:35Z"
          reason: Error
          startedAt: "2024-03-13T16:52:24Z"
      name: postgresql
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:53:35Z"
    hostIP: 10.2.1.183
    phase: Running
    podIP: 10.42.226.85
    podIPs:
    - ip: 10.42.226.85
    qosClass: BestEffort
    startTime: "2024-03-13T16:46:56Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: c9b319ae5b95eecc387f48ff0bf4a5a72831e38b89b377b603d9d922bdee09c7
      cni.projectcalico.org/podIP: 10.42.226.70/32
      cni.projectcalico.org/podIPs: 10.42.226.70/32
    creationTimestamp: "2024-03-13T16:45:22Z"
    generateName: prometheus-statsd-exporter-56d8b89dfd-
    labels:
      app.kubernetes.io/instance: prometheus-statsd-exporter
      app.kubernetes.io/name: prometheus-statsd-exporter
      pod-template-hash: 56d8b89dfd
    name: prometheus-statsd-exporter-56d8b89dfd-cvxzj
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: prometheus-statsd-exporter-56d8b89dfd
      uid: ab6afa04-eec8-44db-ae8d-f492f25a8629
    resourceVersion: "13082246"
    uid: ef4a25b4-39c8-4022-bb0b-e847321eed95
  spec:
    containers:
    - args:
      - --web.listen-address=:9102
      - --web.telemetry-path=/metrics
      - --statsd.listen-udp=:9125
      - --statsd.listen-tcp=:9125
      - --statsd.cache-size=1000
      - --statsd.event-queue-size=10000
      - --statsd.event-flush-threshold=1000
      - --statsd.event-flush-interval=200ms
      - --statsd.mapping-config=/etc/prometheus-statsd-exporter/statsd-mapping.conf
      image: prom/statsd-exporter:v0.26.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /metrics
          port: web
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 1
      name: prometheus-statsd-exporter
      ports:
      - containerPort: 9102
        name: web
        protocol: TCP
      - containerPort: 9125
        name: statsd-tcp
        protocol: TCP
      - containerPort: 9125
        name: statsd-udp
        protocol: UDP
      resources: {}
      securityContext: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/prometheus-statsd-exporter
        name: statsd-mapping-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vgsfd
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: prometheus-statsd-exporter
    serviceAccountName: prometheus-statsd-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: statsd.mappingConf
          path: statsd-mapping.conf
        name: prometheus-statsd-exporter
      name: statsd-mapping-config
    - name: kube-api-access-vgsfd
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:45:22Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:45:28Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:45:28Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:45:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://9c61bccb1047f75257a98b89d5d10f82c4b49755e382cca2474ebba2d0644f0b
      image: prom/statsd-exporter:v0.26.0
      imageID: docker-pullable://prom/statsd-exporter@sha256:a3924f9429c8237293336ff40c5a246238ff9f64aaf712521b2d29f45d6214d5
      lastState: {}
      name: prometheus-statsd-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:45:28Z"
    hostIP: 10.2.1.183
    phase: Running
    podIP: 10.42.226.70
    podIPs:
    - ip: 10.42.226.70
    qosClass: BestEffort
    startTime: "2024-03-13T16:45:22Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: d0a44666aae984996a463f147bf3d42e6c7e4fc0f3f688c9c66ee252f1351bb1
      cni.projectcalico.org/podIP: 10.42.38.69/32
      cni.projectcalico.org/podIPs: 10.42.38.69/32
      tigera-operator.hash.operator.tigera.io/calico-apiserver-certs: f486d634d991c86be36e698588a498a9965d8a2f
    creationTimestamp: "2024-03-13T16:45:36Z"
    generateName: calico-apiserver-fd8fdffff-
    labels:
      apiserver: "true"
      app.kubernetes.io/name: calico-apiserver
      k8s-app: calico-apiserver
      pod-template-hash: fd8fdffff
    name: calico-apiserver-fd8fdffff-l2bbj
    namespace: calico-apiserver
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: calico-apiserver-fd8fdffff
      uid: b1650dc9-c891-4ae6-b6c0-0b0795c9f729
    resourceVersion: "13082592"
    uid: 7b50bb01-4312-45fd-bbf1-10f2101032bc
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                k8s-app: calico-apiserver
            namespaces:
            - calico-apiserver
            topologyKey: kubernetes.io/hostname
          weight: 100
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                k8s-app: calico-apiserver
            namespaces:
            - calico-apiserver
            topologyKey: topology.kubernetes.io/zone
          weight: 100
    containers:
    - args:
      - --secure-port=5443
      - --tls-private-key-file=/calico-apiserver-certs/tls.key
      - --tls-cert-file=/calico-apiserver-certs/tls.crt
      env:
      - name: DATASTORE_TYPE
        value: kubernetes
      - name: KUBERNETES_SERVICE_HOST
        value: 10.43.0.1
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      - name: MULTI_INTERFACE_MODE
        value: none
      image: docker.io/calico/apiserver:master
      imagePullPolicy: IfNotPresent
      name: calico-apiserver
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: 5443
          scheme: HTTPS
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /calico-apiserver-certs
        name: calico-apiserver-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-85hf8
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker-4
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: calico-apiserver
    serviceAccountName: calico-apiserver
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: calico-apiserver-certs
      secret:
        defaultMode: 420
        secretName: calico-apiserver-certs
    - name: kube-api-access-85hf8
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:45:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:45:54Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:45:54Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:45:36Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://08ecba4ba4863fae89c7e6137e23842a43591707cee6a3c6d2fc4883fc06574e
      image: calico/apiserver:master
      imageID: docker-pullable://calico/apiserver@sha256:7c4d765d3c5c91d59bb5eae098b4e6c3ab9cac9d0d1b5495c61cf7cf4641a48f
      lastState: {}
      name: calico-apiserver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:45:53Z"
    hostIP: 10.2.1.186
    phase: Running
    podIP: 10.42.38.69
    podIPs:
    - ip: 10.42.38.69
    qosClass: BestEffort
    startTime: "2024-03-13T16:45:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 5b5d3e2bb655ba2ea87934df47d100da0c590c094439e9ff0da4439da7e5af39
      cni.projectcalico.org/podIP: 10.42.97.203/32
      cni.projectcalico.org/podIPs: 10.42.97.203/32
      tigera-operator.hash.operator.tigera.io/calico-apiserver-certs: f486d634d991c86be36e698588a498a9965d8a2f
    creationTimestamp: "2024-03-13T16:45:36Z"
    generateName: calico-apiserver-fd8fdffff-
    labels:
      apiserver: "true"
      app.kubernetes.io/name: calico-apiserver
      k8s-app: calico-apiserver
      pod-template-hash: fd8fdffff
    name: calico-apiserver-fd8fdffff-vb29t
    namespace: calico-apiserver
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: calico-apiserver-fd8fdffff
      uid: b1650dc9-c891-4ae6-b6c0-0b0795c9f729
    resourceVersion: "13082579"
    uid: 4ad54b62-7d8f-4ebc-bc39-f24a386400b9
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                k8s-app: calico-apiserver
            namespaces:
            - calico-apiserver
            topologyKey: kubernetes.io/hostname
          weight: 100
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                k8s-app: calico-apiserver
            namespaces:
            - calico-apiserver
            topologyKey: topology.kubernetes.io/zone
          weight: 100
    containers:
    - args:
      - --secure-port=5443
      - --tls-private-key-file=/calico-apiserver-certs/tls.key
      - --tls-cert-file=/calico-apiserver-certs/tls.crt
      env:
      - name: DATASTORE_TYPE
        value: kubernetes
      - name: KUBERNETES_SERVICE_HOST
        value: 10.43.0.1
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      - name: MULTI_INTERFACE_MODE
        value: none
      image: docker.io/calico/apiserver:master
      imagePullPolicy: IfNotPresent
      name: calico-apiserver
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: 5443
          scheme: HTTPS
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /calico-apiserver-certs
        name: calico-apiserver-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-c2hgw
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker-3
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: calico-apiserver
    serviceAccountName: calico-apiserver
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: calico-apiserver-certs
      secret:
        defaultMode: 420
        secretName: calico-apiserver-certs
    - name: kube-api-access-c2hgw
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:45:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:45:54Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:45:54Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:45:36Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://ac586ee5921ba54a962d78c0b1a07b9cd2625fa59bd6bd74b6e68cfb04550b93
      image: calico/apiserver:master
      imageID: docker-pullable://calico/apiserver@sha256:7c4d765d3c5c91d59bb5eae098b4e6c3ab9cac9d0d1b5495c61cf7cf4641a48f
      lastState: {}
      name: calico-apiserver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:45:53Z"
    hostIP: 10.2.1.185
    phase: Running
    podIP: 10.42.97.203
    podIPs:
    - ip: 10.42.97.203
    qosClass: BestEffort
    startTime: "2024-03-13T16:45:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: f00cabe6ae2313b464a9c699e26563c686ffc4d6cb63c50f80fd498d1368c7f7
      cni.projectcalico.org/podIP: 10.42.38.66/32
      cni.projectcalico.org/podIPs: 10.42.38.66/32
      hash.operator.tigera.io/system: fdde45054a8ae4f629960ce37570929502e59449
      tigera-operator.hash.operator.tigera.io/tigera-ca-private: 4f5267a1bedc8badce3a9288fba115ecfdb158ed
    creationTimestamp: "2024-03-13T16:18:28Z"
    generateName: calico-kube-controllers-55bd969cb-
    labels:
      app.kubernetes.io/name: calico-kube-controllers
      k8s-app: calico-kube-controllers
      pod-template-hash: 55bd969cb
    name: calico-kube-controllers-55bd969cb-qtvss
    namespace: calico-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: calico-kube-controllers-55bd969cb
      uid: b918bf76-f7dd-49a1-b41b-f01366d9d9c6
    resourceVersion: "13075258"
    uid: 0151213f-b37b-452e-b214-01ca81b27b3b
  spec:
    containers:
    - env:
      - name: KUBE_CONTROLLERS_CONFIG_NAME
        value: default
      - name: DATASTORE_TYPE
        value: kubernetes
      - name: ENABLED_CONTROLLERS
        value: node
      - name: FIPS_MODE_ENABLED
        value: "false"
      - name: DISABLE_KUBE_CONTROLLERS_CONFIG_API
        value: "false"
      - name: KUBERNETES_SERVICE_HOST
        value: 10.43.0.1
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      - name: CA_CRT_PATH
        value: /etc/pki/tls/certs/tigera-ca-bundle.crt
      image: docker.io/calico/kube-controllers:master
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /usr/bin/check-status
          - -l
        failureThreshold: 6
        initialDelaySeconds: 10
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 10
      name: calico-kube-controllers
      readinessProbe:
        exec:
          command:
          - /usr/bin/check-status
          - -r
        failureThreshold: 3
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        runAsGroup: 0
        runAsNonRoot: true
        runAsUser: 999
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/pki/tls/certs
        name: tigera-ca-bundle
        readOnly: true
      - mountPath: /etc/pki/tls/cert.pem
        name: tigera-ca-bundle
        readOnly: true
        subPath: ca-bundle.crt
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wnpm9
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker-4
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: calico-kube-controllers
    serviceAccountName: calico-kube-controllers
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: tigera-ca-bundle
      name: tigera-ca-bundle
    - name: kube-api-access-wnpm9
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:18:28Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:18:40Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:18:40Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:18:28Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://6adac98aa835bd0feecd4a12b5a69bd833226f569b102fa12bedf6d66ef5636a
      image: calico/kube-controllers:master
      imageID: docker-pullable://calico/kube-controllers@sha256:9e1914c5e2a9b37bf1463af399738a7f7ac32c3bf2a5bda2ea8c7712787d5e36
      lastState: {}
      name: calico-kube-controllers
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:18:39Z"
    hostIP: 10.2.1.186
    phase: Running
    podIP: 10.42.38.66
    podIPs:
    - ip: 10.42.38.66
    qosClass: BestEffort
    startTime: "2024-03-13T16:18:28Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      hash.operator.tigera.io/cni-config: 67d9927589bcf84d027e873bcb62d5ee6677ab82
      hash.operator.tigera.io/system: fdde45054a8ae4f629960ce37570929502e59449
      tigera-operator.hash.operator.tigera.io/tigera-ca-private: 4f5267a1bedc8badce3a9288fba115ecfdb158ed
    creationTimestamp: "2024-03-13T16:12:07Z"
    generateName: calico-node-
    labels:
      app.kubernetes.io/name: calico-node
      controller-revision-hash: 6ccf5bfd78
      k8s-app: calico-node
      pod-template-generation: "1"
    name: calico-node-54jk8
    namespace: calico-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: calico-node
      uid: 82403527-ae8d-4ba4-a2a0-e2ebe48bf965
    resourceVersion: "13074473"
    uid: 322e8808-98c0-4a20-b7e8-b60c3e32a5ee
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - worker-4
    containers:
    - env:
      - name: DATASTORE_TYPE
        value: kubernetes
      - name: WAIT_FOR_DATASTORE
        value: "true"
      - name: CLUSTER_TYPE
        value: k8s,operator,bgp
      - name: CALICO_DISABLE_FILE_LOGGING
        value: "false"
      - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
        value: ACCEPT
      - name: FELIX_HEALTHENABLED
        value: "true"
      - name: FELIX_HEALTHPORT
        value: "9099"
      - name: NODENAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: FELIX_TYPHAK8SNAMESPACE
        value: calico-system
      - name: FELIX_TYPHAK8SSERVICENAME
        value: calico-typha
      - name: FELIX_TYPHACAFILE
        value: /etc/pki/tls/certs/tigera-ca-bundle.crt
      - name: FELIX_TYPHACERTFILE
        value: /node-certs/tls.crt
      - name: FELIX_TYPHAKEYFILE
        value: /node-certs/tls.key
      - name: FIPS_MODE_ENABLED
        value: "false"
      - name: FELIX_TYPHACN
        value: typha-server
      - name: CALICO_MANAGE_CNI
        value: "true"
      - name: CALICO_IPV4POOL_CIDR
        value: 10.42.0.0/16
      - name: CALICO_IPV4POOL_VXLAN
        value: CrossSubnet
      - name: CALICO_IPV4POOL_BLOCK_SIZE
        value: "26"
      - name: CALICO_IPV4POOL_NODE_SELECTOR
        value: all()
      - name: CALICO_IPV4POOL_DISABLE_BGP_EXPORT
        value: "false"
      - name: CALICO_NETWORKING_BACKEND
        value: bird
      - name: IP
        value: autodetect
      - name: IP_AUTODETECTION_METHOD
        value: first-found
      - name: IP6
        value: none
      - name: FELIX_IPV6SUPPORT
        value: "false"
      - name: KUBERNETES_SERVICE_HOST
        value: 10.43.0.1
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: docker.io/calico/node:master
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/calico-node
            - -shutdown
      livenessProbe:
        failureThreshold: 3
        httpGet:
          host: localhost
          path: /liveness
          port: 9099
          scheme: HTTP
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 10
      name: calico-node
      readinessProbe:
        exec:
          command:
          - /bin/calico-node
          - -bird-ready
          - -felix-ready
        failureThreshold: 3
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/pki/tls/certs
        name: tigera-ca-bundle
        readOnly: true
      - mountPath: /etc/pki/tls/cert.pem
        name: tigera-ca-bundle
        readOnly: true
        subPath: ca-bundle.crt
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/nodeagent
        name: policysync
      - mountPath: /node-certs
        name: node-certs
        readOnly: true
      - mountPath: /var/run/calico
        name: var-run-calico
      - mountPath: /var/lib/calico
        name: var-lib-calico
      - mountPath: /var/log/calico/cni
        name: cni-log-dir
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t8mgm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - image: docker.io/calico/pod2daemon-flexvol:master
      imagePullPolicy: IfNotPresent
      name: flexvol-driver
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/driver
        name: flexvol-driver-host
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t8mgm
        readOnly: true
    - command:
      - /opt/cni/bin/install
      env:
      - name: CNI_CONF_NAME
        value: 10-calico.conflist
      - name: SLEEP
        value: "false"
      - name: CNI_NET_DIR
        value: /etc/cni/net.d
      - name: CNI_NETWORK_CONFIG
        valueFrom:
          configMapKeyRef:
            key: config
            name: cni-config
      - name: KUBERNETES_SERVICE_HOST
        value: 10.43.0.1
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: docker.io/calico/cni:master
      imagePullPolicy: IfNotPresent
      name: install-cni
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cni-bin-dir
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t8mgm
        readOnly: true
    nodeName: worker-4
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: calico-node
    serviceAccountName: calico-node
    terminationGracePeriodSeconds: 5
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /var/run/nodeagent
        type: DirectoryOrCreate
      name: policysync
    - configMap:
        defaultMode: 420
        name: tigera-ca-bundle
      name: tigera-ca-bundle
    - name: node-certs
      secret:
        defaultMode: 420
        secretName: node-certs
    - hostPath:
        path: /var/run/calico
        type: ""
      name: var-run-calico
    - hostPath:
        path: /var/lib/calico
        type: ""
      name: var-lib-calico
    - hostPath:
        path: /opt/cni/bin
        type: ""
      name: cni-bin-dir
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni-net-dir
    - hostPath:
        path: /var/log/calico/cni
        type: ""
      name: cni-log-dir
    - hostPath:
        path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds
        type: DirectoryOrCreate
      name: flexvol-driver-host
    - name: kube-api-access-t8mgm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:13:57Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:16:37Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:16:37Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://9c3c0699c27fcaffd9511ea1603d7a25c041c70394aa827349cb6f850cb262ff
      image: calico/node:master
      imageID: docker-pullable://calico/node@sha256:48f83d5dfa9111eb4cab0f99abe7b3e40dae57610d16bd3f539447493a7f8618
      lastState: {}
      name: calico-node
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:15:53Z"
    hostIP: 10.2.1.186
    initContainerStatuses:
    - containerID: docker://8a9acc64a5310513487475232ab8e9f615d591ded19c814cff1fbc241cd4d38d
      image: calico/pod2daemon-flexvol:master
      imageID: docker-pullable://calico/pod2daemon-flexvol@sha256:81e82599290cdd04e348497d1b69516891094a1b91f26d4390755b3bc34eb297
      lastState: {}
      name: flexvol-driver
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://8a9acc64a5310513487475232ab8e9f615d591ded19c814cff1fbc241cd4d38d
          exitCode: 0
          finishedAt: "2024-03-13T16:12:18Z"
          reason: Completed
          startedAt: "2024-03-13T16:12:18Z"
    - containerID: docker://89024f08cfd6ac1d001ef2e58cbcf15260e6cb333867cf7297f4d82dcfd23fdb
      image: calico/cni:master
      imageID: docker-pullable://calico/cni@sha256:44da8846472e8c262a666e19ebbb6208f2607e11e40fd4b96aa704c92ab794b9
      lastState: {}
      name: install-cni
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://89024f08cfd6ac1d001ef2e58cbcf15260e6cb333867cf7297f4d82dcfd23fdb
          exitCode: 0
          finishedAt: "2024-03-13T16:13:56Z"
          reason: Completed
          startedAt: "2024-03-13T16:13:56Z"
    phase: Running
    podIP: 10.2.1.186
    podIPs:
    - ip: 10.2.1.186
    qosClass: BestEffort
    startTime: "2024-03-13T16:12:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      hash.operator.tigera.io/cni-config: 67d9927589bcf84d027e873bcb62d5ee6677ab82
      hash.operator.tigera.io/system: fdde45054a8ae4f629960ce37570929502e59449
      tigera-operator.hash.operator.tigera.io/tigera-ca-private: 4f5267a1bedc8badce3a9288fba115ecfdb158ed
    creationTimestamp: "2024-03-13T16:12:07Z"
    generateName: calico-node-
    labels:
      app.kubernetes.io/name: calico-node
      controller-revision-hash: 6ccf5bfd78
      k8s-app: calico-node
      pod-template-generation: "1"
    name: calico-node-59hfq
    namespace: calico-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: calico-node
      uid: 82403527-ae8d-4ba4-a2a0-e2ebe48bf965
    resourceVersion: "13074475"
    uid: b2e21ad2-23a2-4d57-9e0d-3969a33fcd80
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - master-2
    containers:
    - env:
      - name: DATASTORE_TYPE
        value: kubernetes
      - name: WAIT_FOR_DATASTORE
        value: "true"
      - name: CLUSTER_TYPE
        value: k8s,operator,bgp
      - name: CALICO_DISABLE_FILE_LOGGING
        value: "false"
      - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
        value: ACCEPT
      - name: FELIX_HEALTHENABLED
        value: "true"
      - name: FELIX_HEALTHPORT
        value: "9099"
      - name: NODENAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: FELIX_TYPHAK8SNAMESPACE
        value: calico-system
      - name: FELIX_TYPHAK8SSERVICENAME
        value: calico-typha
      - name: FELIX_TYPHACAFILE
        value: /etc/pki/tls/certs/tigera-ca-bundle.crt
      - name: FELIX_TYPHACERTFILE
        value: /node-certs/tls.crt
      - name: FELIX_TYPHAKEYFILE
        value: /node-certs/tls.key
      - name: FIPS_MODE_ENABLED
        value: "false"
      - name: FELIX_TYPHACN
        value: typha-server
      - name: CALICO_MANAGE_CNI
        value: "true"
      - name: CALICO_IPV4POOL_CIDR
        value: 10.42.0.0/16
      - name: CALICO_IPV4POOL_VXLAN
        value: CrossSubnet
      - name: CALICO_IPV4POOL_BLOCK_SIZE
        value: "26"
      - name: CALICO_IPV4POOL_NODE_SELECTOR
        value: all()
      - name: CALICO_IPV4POOL_DISABLE_BGP_EXPORT
        value: "false"
      - name: CALICO_NETWORKING_BACKEND
        value: bird
      - name: IP
        value: autodetect
      - name: IP_AUTODETECTION_METHOD
        value: first-found
      - name: IP6
        value: none
      - name: FELIX_IPV6SUPPORT
        value: "false"
      - name: KUBERNETES_SERVICE_HOST
        value: 10.43.0.1
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: docker.io/calico/node:master
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/calico-node
            - -shutdown
      livenessProbe:
        failureThreshold: 3
        httpGet:
          host: localhost
          path: /liveness
          port: 9099
          scheme: HTTP
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 10
      name: calico-node
      readinessProbe:
        exec:
          command:
          - /bin/calico-node
          - -bird-ready
          - -felix-ready
        failureThreshold: 3
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/pki/tls/certs
        name: tigera-ca-bundle
        readOnly: true
      - mountPath: /etc/pki/tls/cert.pem
        name: tigera-ca-bundle
        readOnly: true
        subPath: ca-bundle.crt
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/nodeagent
        name: policysync
      - mountPath: /node-certs
        name: node-certs
        readOnly: true
      - mountPath: /var/run/calico
        name: var-run-calico
      - mountPath: /var/lib/calico
        name: var-lib-calico
      - mountPath: /var/log/calico/cni
        name: cni-log-dir
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-s5p9b
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - image: docker.io/calico/pod2daemon-flexvol:master
      imagePullPolicy: IfNotPresent
      name: flexvol-driver
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/driver
        name: flexvol-driver-host
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-s5p9b
        readOnly: true
    - command:
      - /opt/cni/bin/install
      env:
      - name: CNI_CONF_NAME
        value: 10-calico.conflist
      - name: SLEEP
        value: "false"
      - name: CNI_NET_DIR
        value: /etc/cni/net.d
      - name: CNI_NETWORK_CONFIG
        valueFrom:
          configMapKeyRef:
            key: config
            name: cni-config
      - name: KUBERNETES_SERVICE_HOST
        value: 10.43.0.1
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: docker.io/calico/cni:master
      imagePullPolicy: IfNotPresent
      name: install-cni
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cni-bin-dir
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-s5p9b
        readOnly: true
    nodeName: master-2
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: calico-node
    serviceAccountName: calico-node
    terminationGracePeriodSeconds: 5
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /var/run/nodeagent
        type: DirectoryOrCreate
      name: policysync
    - configMap:
        defaultMode: 420
        name: tigera-ca-bundle
      name: tigera-ca-bundle
    - name: node-certs
      secret:
        defaultMode: 420
        secretName: node-certs
    - hostPath:
        path: /var/run/calico
        type: ""
      name: var-run-calico
    - hostPath:
        path: /var/lib/calico
        type: ""
      name: var-lib-calico
    - hostPath:
        path: /opt/cni/bin
        type: ""
      name: cni-bin-dir
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni-net-dir
    - hostPath:
        path: /var/log/calico/cni
        type: ""
      name: cni-log-dir
    - hostPath:
        path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds
        type: DirectoryOrCreate
      name: flexvol-driver-host
    - name: kube-api-access-s5p9b
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:14:13Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:16:37Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:16:37Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://d96e32e87d5d15e1a12bba0cc1e31b426064204a705993e12ab495715b3ce2ad
      image: calico/node:master
      imageID: docker-pullable://calico/node@sha256:48f83d5dfa9111eb4cab0f99abe7b3e40dae57610d16bd3f539447493a7f8618
      lastState: {}
      name: calico-node
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:15:58Z"
    hostIP: 10.2.1.181
    initContainerStatuses:
    - containerID: docker://dc4641f51c78df8c423ab6b298f1796104c80e322f84a1f687a6d07d0a680607
      image: calico/pod2daemon-flexvol:master
      imageID: docker-pullable://calico/pod2daemon-flexvol@sha256:81e82599290cdd04e348497d1b69516891094a1b91f26d4390755b3bc34eb297
      lastState: {}
      name: flexvol-driver
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://dc4641f51c78df8c423ab6b298f1796104c80e322f84a1f687a6d07d0a680607
          exitCode: 0
          finishedAt: "2024-03-13T16:12:39Z"
          reason: Completed
          startedAt: "2024-03-13T16:12:39Z"
    - containerID: docker://f052b5aaebb36aab511df4e99d31a38f4af5bc54a08f66f77d44f4af1e773fff
      image: calico/cni:master
      imageID: docker-pullable://calico/cni@sha256:44da8846472e8c262a666e19ebbb6208f2607e11e40fd4b96aa704c92ab794b9
      lastState: {}
      name: install-cni
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://f052b5aaebb36aab511df4e99d31a38f4af5bc54a08f66f77d44f4af1e773fff
          exitCode: 0
          finishedAt: "2024-03-13T16:14:13Z"
          reason: Completed
          startedAt: "2024-03-13T16:14:12Z"
    phase: Running
    podIP: 10.2.1.181
    podIPs:
    - ip: 10.2.1.181
    qosClass: BestEffort
    startTime: "2024-03-13T16:12:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      hash.operator.tigera.io/cni-config: 67d9927589bcf84d027e873bcb62d5ee6677ab82
      hash.operator.tigera.io/system: fdde45054a8ae4f629960ce37570929502e59449
      tigera-operator.hash.operator.tigera.io/tigera-ca-private: 4f5267a1bedc8badce3a9288fba115ecfdb158ed
    creationTimestamp: "2024-03-13T16:12:07Z"
    generateName: calico-node-
    labels:
      app.kubernetes.io/name: calico-node
      controller-revision-hash: 6ccf5bfd78
      k8s-app: calico-node
      pod-template-generation: "1"
    name: calico-node-7mxh5
    namespace: calico-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: calico-node
      uid: 82403527-ae8d-4ba4-a2a0-e2ebe48bf965
    resourceVersion: "13082264"
    uid: 464b956f-7ead-46ca-9b72-c82e5406bb3d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - worker-1
    containers:
    - env:
      - name: DATASTORE_TYPE
        value: kubernetes
      - name: WAIT_FOR_DATASTORE
        value: "true"
      - name: CLUSTER_TYPE
        value: k8s,operator,bgp
      - name: CALICO_DISABLE_FILE_LOGGING
        value: "false"
      - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
        value: ACCEPT
      - name: FELIX_HEALTHENABLED
        value: "true"
      - name: FELIX_HEALTHPORT
        value: "9099"
      - name: NODENAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: FELIX_TYPHAK8SNAMESPACE
        value: calico-system
      - name: FELIX_TYPHAK8SSERVICENAME
        value: calico-typha
      - name: FELIX_TYPHACAFILE
        value: /etc/pki/tls/certs/tigera-ca-bundle.crt
      - name: FELIX_TYPHACERTFILE
        value: /node-certs/tls.crt
      - name: FELIX_TYPHAKEYFILE
        value: /node-certs/tls.key
      - name: FIPS_MODE_ENABLED
        value: "false"
      - name: FELIX_TYPHACN
        value: typha-server
      - name: CALICO_MANAGE_CNI
        value: "true"
      - name: CALICO_IPV4POOL_CIDR
        value: 10.42.0.0/16
      - name: CALICO_IPV4POOL_VXLAN
        value: CrossSubnet
      - name: CALICO_IPV4POOL_BLOCK_SIZE
        value: "26"
      - name: CALICO_IPV4POOL_NODE_SELECTOR
        value: all()
      - name: CALICO_IPV4POOL_DISABLE_BGP_EXPORT
        value: "false"
      - name: CALICO_NETWORKING_BACKEND
        value: bird
      - name: IP
        value: autodetect
      - name: IP_AUTODETECTION_METHOD
        value: first-found
      - name: IP6
        value: none
      - name: FELIX_IPV6SUPPORT
        value: "false"
      - name: KUBERNETES_SERVICE_HOST
        value: 10.43.0.1
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: docker.io/calico/node:master
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/calico-node
            - -shutdown
      livenessProbe:
        failureThreshold: 3
        httpGet:
          host: localhost
          path: /liveness
          port: 9099
          scheme: HTTP
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 10
      name: calico-node
      readinessProbe:
        exec:
          command:
          - /bin/calico-node
          - -bird-ready
          - -felix-ready
        failureThreshold: 3
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/pki/tls/certs
        name: tigera-ca-bundle
        readOnly: true
      - mountPath: /etc/pki/tls/cert.pem
        name: tigera-ca-bundle
        readOnly: true
        subPath: ca-bundle.crt
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/nodeagent
        name: policysync
      - mountPath: /node-certs
        name: node-certs
        readOnly: true
      - mountPath: /var/run/calico
        name: var-run-calico
      - mountPath: /var/lib/calico
        name: var-lib-calico
      - mountPath: /var/log/calico/cni
        name: cni-log-dir
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gq99s
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - image: docker.io/calico/pod2daemon-flexvol:master
      imagePullPolicy: IfNotPresent
      name: flexvol-driver
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/driver
        name: flexvol-driver-host
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gq99s
        readOnly: true
    - command:
      - /opt/cni/bin/install
      env:
      - name: CNI_CONF_NAME
        value: 10-calico.conflist
      - name: SLEEP
        value: "false"
      - name: CNI_NET_DIR
        value: /etc/cni/net.d
      - name: CNI_NETWORK_CONFIG
        valueFrom:
          configMapKeyRef:
            key: config
            name: cni-config
      - name: KUBERNETES_SERVICE_HOST
        value: 10.43.0.1
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: docker.io/calico/cni:master
      imagePullPolicy: IfNotPresent
      name: install-cni
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cni-bin-dir
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gq99s
        readOnly: true
    nodeName: worker-1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: calico-node
    serviceAccountName: calico-node
    terminationGracePeriodSeconds: 5
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /var/run/nodeagent
        type: DirectoryOrCreate
      name: policysync
    - configMap:
        defaultMode: 420
        name: tigera-ca-bundle
      name: tigera-ca-bundle
    - name: node-certs
      secret:
        defaultMode: 420
        secretName: node-certs
    - hostPath:
        path: /var/run/calico
        type: ""
      name: var-run-calico
    - hostPath:
        path: /var/lib/calico
        type: ""
      name: var-lib-calico
    - hostPath:
        path: /opt/cni/bin
        type: ""
      name: cni-bin-dir
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni-net-dir
    - hostPath:
        path: /var/log/calico/cni
        type: ""
      name: cni-log-dir
    - hostPath:
        path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds
        type: DirectoryOrCreate
      name: flexvol-driver-host
    - name: kube-api-access-gq99s
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:44:33Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:45:30Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:45:30Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://44ac8535051a084b3e27b4d4941e6fef7150771667191acf0e09948d018a332e
      image: calico/node:master
      imageID: docker-pullable://calico/node@sha256:48f83d5dfa9111eb4cab0f99abe7b3e40dae57610d16bd3f539447493a7f8618
      lastState: {}
      name: calico-node
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:45:02Z"
    hostIP: 10.2.1.183
    initContainerStatuses:
    - containerID: docker://ff9112e1f81d8db3f1da2cfbb7a1a64c7db485d87a2c2e18870953043c0e3616
      image: calico/pod2daemon-flexvol:master
      imageID: docker-pullable://calico/pod2daemon-flexvol@sha256:81e82599290cdd04e348497d1b69516891094a1b91f26d4390755b3bc34eb297
      lastState: {}
      name: flexvol-driver
      ready: true
      restartCount: 1
      state:
        terminated:
          containerID: docker://ff9112e1f81d8db3f1da2cfbb7a1a64c7db485d87a2c2e18870953043c0e3616
          exitCode: 0
          finishedAt: "2024-03-13T16:44:11Z"
          reason: Completed
          startedAt: "2024-03-13T16:44:11Z"
    - containerID: docker://f5df3ab73375c970a6855e1d6e936628d43a4ba1353e50958f156f10c84ac3df
      image: calico/cni:master
      imageID: docker-pullable://calico/cni@sha256:44da8846472e8c262a666e19ebbb6208f2607e11e40fd4b96aa704c92ab794b9
      lastState: {}
      name: install-cni
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://f5df3ab73375c970a6855e1d6e936628d43a4ba1353e50958f156f10c84ac3df
          exitCode: 0
          finishedAt: "2024-03-13T16:44:33Z"
          reason: Completed
          startedAt: "2024-03-13T16:44:32Z"
    phase: Running
    podIP: 10.2.1.183
    podIPs:
    - ip: 10.2.1.183
    qosClass: BestEffort
    startTime: "2024-03-13T16:12:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      hash.operator.tigera.io/cni-config: 67d9927589bcf84d027e873bcb62d5ee6677ab82
      hash.operator.tigera.io/system: fdde45054a8ae4f629960ce37570929502e59449
      tigera-operator.hash.operator.tigera.io/tigera-ca-private: 4f5267a1bedc8badce3a9288fba115ecfdb158ed
    creationTimestamp: "2024-03-13T16:12:07Z"
    generateName: calico-node-
    labels:
      app.kubernetes.io/name: calico-node
      controller-revision-hash: 6ccf5bfd78
      k8s-app: calico-node
      pod-template-generation: "1"
    name: calico-node-ghksr
    namespace: calico-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: calico-node
      uid: 82403527-ae8d-4ba4-a2a0-e2ebe48bf965
    resourceVersion: "13074276"
    uid: b23133ab-300c-4875-b29d-da5fb180ad5b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - worker-2
    containers:
    - env:
      - name: DATASTORE_TYPE
        value: kubernetes
      - name: WAIT_FOR_DATASTORE
        value: "true"
      - name: CLUSTER_TYPE
        value: k8s,operator,bgp
      - name: CALICO_DISABLE_FILE_LOGGING
        value: "false"
      - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
        value: ACCEPT
      - name: FELIX_HEALTHENABLED
        value: "true"
      - name: FELIX_HEALTHPORT
        value: "9099"
      - name: NODENAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: FELIX_TYPHAK8SNAMESPACE
        value: calico-system
      - name: FELIX_TYPHAK8SSERVICENAME
        value: calico-typha
      - name: FELIX_TYPHACAFILE
        value: /etc/pki/tls/certs/tigera-ca-bundle.crt
      - name: FELIX_TYPHACERTFILE
        value: /node-certs/tls.crt
      - name: FELIX_TYPHAKEYFILE
        value: /node-certs/tls.key
      - name: FIPS_MODE_ENABLED
        value: "false"
      - name: FELIX_TYPHACN
        value: typha-server
      - name: CALICO_MANAGE_CNI
        value: "true"
      - name: CALICO_IPV4POOL_CIDR
        value: 10.42.0.0/16
      - name: CALICO_IPV4POOL_VXLAN
        value: CrossSubnet
      - name: CALICO_IPV4POOL_BLOCK_SIZE
        value: "26"
      - name: CALICO_IPV4POOL_NODE_SELECTOR
        value: all()
      - name: CALICO_IPV4POOL_DISABLE_BGP_EXPORT
        value: "false"
      - name: CALICO_NETWORKING_BACKEND
        value: bird
      - name: IP
        value: autodetect
      - name: IP_AUTODETECTION_METHOD
        value: first-found
      - name: IP6
        value: none
      - name: FELIX_IPV6SUPPORT
        value: "false"
      - name: KUBERNETES_SERVICE_HOST
        value: 10.43.0.1
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: docker.io/calico/node:master
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/calico-node
            - -shutdown
      livenessProbe:
        failureThreshold: 3
        httpGet:
          host: localhost
          path: /liveness
          port: 9099
          scheme: HTTP
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 10
      name: calico-node
      readinessProbe:
        exec:
          command:
          - /bin/calico-node
          - -bird-ready
          - -felix-ready
        failureThreshold: 3
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/pki/tls/certs
        name: tigera-ca-bundle
        readOnly: true
      - mountPath: /etc/pki/tls/cert.pem
        name: tigera-ca-bundle
        readOnly: true
        subPath: ca-bundle.crt
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/nodeagent
        name: policysync
      - mountPath: /node-certs
        name: node-certs
        readOnly: true
      - mountPath: /var/run/calico
        name: var-run-calico
      - mountPath: /var/lib/calico
        name: var-lib-calico
      - mountPath: /var/log/calico/cni
        name: cni-log-dir
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-465rm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - image: docker.io/calico/pod2daemon-flexvol:master
      imagePullPolicy: IfNotPresent
      name: flexvol-driver
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/driver
        name: flexvol-driver-host
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-465rm
        readOnly: true
    - command:
      - /opt/cni/bin/install
      env:
      - name: CNI_CONF_NAME
        value: 10-calico.conflist
      - name: SLEEP
        value: "false"
      - name: CNI_NET_DIR
        value: /etc/cni/net.d
      - name: CNI_NETWORK_CONFIG
        valueFrom:
          configMapKeyRef:
            key: config
            name: cni-config
      - name: KUBERNETES_SERVICE_HOST
        value: 10.43.0.1
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: docker.io/calico/cni:master
      imagePullPolicy: IfNotPresent
      name: install-cni
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cni-bin-dir
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-465rm
        readOnly: true
    nodeName: worker-2
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: calico-node
    serviceAccountName: calico-node
    terminationGracePeriodSeconds: 5
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /var/run/nodeagent
        type: DirectoryOrCreate
      name: policysync
    - configMap:
        defaultMode: 420
        name: tigera-ca-bundle
      name: tigera-ca-bundle
    - name: node-certs
      secret:
        defaultMode: 420
        secretName: node-certs
    - hostPath:
        path: /var/run/calico
        type: ""
      name: var-run-calico
    - hostPath:
        path: /var/lib/calico
        type: ""
      name: var-lib-calico
    - hostPath:
        path: /opt/cni/bin
        type: ""
      name: cni-bin-dir
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni-net-dir
    - hostPath:
        path: /var/log/calico/cni
        type: ""
      name: cni-log-dir
    - hostPath:
        path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds
        type: DirectoryOrCreate
      name: flexvol-driver-host
    - name: kube-api-access-465rm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:13:31Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:16:07Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:16:07Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://24ab3ff597a4edd839fffe1aacf32fe5c34174cdd60e2a42674d04dbeb8700b7
      image: calico/node:master
      imageID: docker-pullable://calico/node@sha256:48f83d5dfa9111eb4cab0f99abe7b3e40dae57610d16bd3f539447493a7f8618
      lastState: {}
      name: calico-node
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:15:20Z"
    hostIP: 10.2.1.184
    initContainerStatuses:
    - containerID: docker://d4f2c299dc731050348c195d7d611407d401a855cb414ff459d5e32af6e5e89d
      image: calico/pod2daemon-flexvol:master
      imageID: docker-pullable://calico/pod2daemon-flexvol@sha256:81e82599290cdd04e348497d1b69516891094a1b91f26d4390755b3bc34eb297
      lastState: {}
      name: flexvol-driver
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://d4f2c299dc731050348c195d7d611407d401a855cb414ff459d5e32af6e5e89d
          exitCode: 0
          finishedAt: "2024-03-13T16:12:16Z"
          reason: Completed
          startedAt: "2024-03-13T16:12:16Z"
    - containerID: docker://0ff7569d9b3c330bbc1e57e8efff1257ae1cd60ad3c574866f2b674a6cd9cb44
      image: calico/cni:master
      imageID: docker-pullable://calico/cni@sha256:44da8846472e8c262a666e19ebbb6208f2607e11e40fd4b96aa704c92ab794b9
      lastState: {}
      name: install-cni
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://0ff7569d9b3c330bbc1e57e8efff1257ae1cd60ad3c574866f2b674a6cd9cb44
          exitCode: 0
          finishedAt: "2024-03-13T16:13:31Z"
          reason: Completed
          startedAt: "2024-03-13T16:13:30Z"
    phase: Running
    podIP: 10.2.1.184
    podIPs:
    - ip: 10.2.1.184
    qosClass: BestEffort
    startTime: "2024-03-13T16:12:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      hash.operator.tigera.io/cni-config: 67d9927589bcf84d027e873bcb62d5ee6677ab82
      hash.operator.tigera.io/system: fdde45054a8ae4f629960ce37570929502e59449
      tigera-operator.hash.operator.tigera.io/tigera-ca-private: 4f5267a1bedc8badce3a9288fba115ecfdb158ed
    creationTimestamp: "2024-03-13T16:12:07Z"
    generateName: calico-node-
    labels:
      app.kubernetes.io/name: calico-node
      controller-revision-hash: 6ccf5bfd78
      k8s-app: calico-node
      pod-template-generation: "1"
    name: calico-node-j2wl5
    namespace: calico-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: calico-node
      uid: 82403527-ae8d-4ba4-a2a0-e2ebe48bf965
    resourceVersion: "13074471"
    uid: 881330b7-38c7-46d8-93bf-190c5065c77c
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - worker-3
    containers:
    - env:
      - name: DATASTORE_TYPE
        value: kubernetes
      - name: WAIT_FOR_DATASTORE
        value: "true"
      - name: CLUSTER_TYPE
        value: k8s,operator,bgp
      - name: CALICO_DISABLE_FILE_LOGGING
        value: "false"
      - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
        value: ACCEPT
      - name: FELIX_HEALTHENABLED
        value: "true"
      - name: FELIX_HEALTHPORT
        value: "9099"
      - name: NODENAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: FELIX_TYPHAK8SNAMESPACE
        value: calico-system
      - name: FELIX_TYPHAK8SSERVICENAME
        value: calico-typha
      - name: FELIX_TYPHACAFILE
        value: /etc/pki/tls/certs/tigera-ca-bundle.crt
      - name: FELIX_TYPHACERTFILE
        value: /node-certs/tls.crt
      - name: FELIX_TYPHAKEYFILE
        value: /node-certs/tls.key
      - name: FIPS_MODE_ENABLED
        value: "false"
      - name: FELIX_TYPHACN
        value: typha-server
      - name: CALICO_MANAGE_CNI
        value: "true"
      - name: CALICO_IPV4POOL_CIDR
        value: 10.42.0.0/16
      - name: CALICO_IPV4POOL_VXLAN
        value: CrossSubnet
      - name: CALICO_IPV4POOL_BLOCK_SIZE
        value: "26"
      - name: CALICO_IPV4POOL_NODE_SELECTOR
        value: all()
      - name: CALICO_IPV4POOL_DISABLE_BGP_EXPORT
        value: "false"
      - name: CALICO_NETWORKING_BACKEND
        value: bird
      - name: IP
        value: autodetect
      - name: IP_AUTODETECTION_METHOD
        value: first-found
      - name: IP6
        value: none
      - name: FELIX_IPV6SUPPORT
        value: "false"
      - name: KUBERNETES_SERVICE_HOST
        value: 10.43.0.1
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: docker.io/calico/node:master
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/calico-node
            - -shutdown
      livenessProbe:
        failureThreshold: 3
        httpGet:
          host: localhost
          path: /liveness
          port: 9099
          scheme: HTTP
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 10
      name: calico-node
      readinessProbe:
        exec:
          command:
          - /bin/calico-node
          - -bird-ready
          - -felix-ready
        failureThreshold: 3
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/pki/tls/certs
        name: tigera-ca-bundle
        readOnly: true
      - mountPath: /etc/pki/tls/cert.pem
        name: tigera-ca-bundle
        readOnly: true
        subPath: ca-bundle.crt
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/nodeagent
        name: policysync
      - mountPath: /node-certs
        name: node-certs
        readOnly: true
      - mountPath: /var/run/calico
        name: var-run-calico
      - mountPath: /var/lib/calico
        name: var-lib-calico
      - mountPath: /var/log/calico/cni
        name: cni-log-dir
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kxwbg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - image: docker.io/calico/pod2daemon-flexvol:master
      imagePullPolicy: IfNotPresent
      name: flexvol-driver
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/driver
        name: flexvol-driver-host
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kxwbg
        readOnly: true
    - command:
      - /opt/cni/bin/install
      env:
      - name: CNI_CONF_NAME
        value: 10-calico.conflist
      - name: SLEEP
        value: "false"
      - name: CNI_NET_DIR
        value: /etc/cni/net.d
      - name: CNI_NETWORK_CONFIG
        valueFrom:
          configMapKeyRef:
            key: config
            name: cni-config
      - name: KUBERNETES_SERVICE_HOST
        value: 10.43.0.1
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: docker.io/calico/cni:master
      imagePullPolicy: IfNotPresent
      name: install-cni
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cni-bin-dir
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kxwbg
        readOnly: true
    nodeName: worker-3
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: calico-node
    serviceAccountName: calico-node
    terminationGracePeriodSeconds: 5
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /var/run/nodeagent
        type: DirectoryOrCreate
      name: policysync
    - configMap:
        defaultMode: 420
        name: tigera-ca-bundle
      name: tigera-ca-bundle
    - name: node-certs
      secret:
        defaultMode: 420
        secretName: node-certs
    - hostPath:
        path: /var/run/calico
        type: ""
      name: var-run-calico
    - hostPath:
        path: /var/lib/calico
        type: ""
      name: var-lib-calico
    - hostPath:
        path: /opt/cni/bin
        type: ""
      name: cni-bin-dir
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni-net-dir
    - hostPath:
        path: /var/log/calico/cni
        type: ""
      name: cni-log-dir
    - hostPath:
        path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds
        type: DirectoryOrCreate
      name: flexvol-driver-host
    - name: kube-api-access-kxwbg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:13:51Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:16:37Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:16:37Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://293e7f6b483d2a72f9f45fd177230adc647e0182eaf91fb83b85bd63d11c72fa
      image: calico/node:master
      imageID: docker-pullable://calico/node@sha256:48f83d5dfa9111eb4cab0f99abe7b3e40dae57610d16bd3f539447493a7f8618
      lastState: {}
      name: calico-node
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:15:55Z"
    hostIP: 10.2.1.185
    initContainerStatuses:
    - containerID: docker://bfdb57408696f9d2d2028d01d659b11e66280faf891e347e02d0a658a50193ec
      image: calico/pod2daemon-flexvol:master
      imageID: docker-pullable://calico/pod2daemon-flexvol@sha256:81e82599290cdd04e348497d1b69516891094a1b91f26d4390755b3bc34eb297
      lastState: {}
      name: flexvol-driver
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://bfdb57408696f9d2d2028d01d659b11e66280faf891e347e02d0a658a50193ec
          exitCode: 0
          finishedAt: "2024-03-13T16:12:16Z"
          reason: Completed
          startedAt: "2024-03-13T16:12:16Z"
    - containerID: docker://1619d03e72b0203d6ffcb78cdf6c2639495d5d792c08a6f1681d52598fff7384
      image: calico/cni:master
      imageID: docker-pullable://calico/cni@sha256:44da8846472e8c262a666e19ebbb6208f2607e11e40fd4b96aa704c92ab794b9
      lastState: {}
      name: install-cni
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://1619d03e72b0203d6ffcb78cdf6c2639495d5d792c08a6f1681d52598fff7384
          exitCode: 0
          finishedAt: "2024-03-13T16:13:50Z"
          reason: Completed
          startedAt: "2024-03-13T16:13:50Z"
    phase: Running
    podIP: 10.2.1.185
    podIPs:
    - ip: 10.2.1.185
    qosClass: BestEffort
    startTime: "2024-03-13T16:12:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      hash.operator.tigera.io/cni-config: 67d9927589bcf84d027e873bcb62d5ee6677ab82
      hash.operator.tigera.io/system: fdde45054a8ae4f629960ce37570929502e59449
      tigera-operator.hash.operator.tigera.io/tigera-ca-private: 4f5267a1bedc8badce3a9288fba115ecfdb158ed
    creationTimestamp: "2024-03-13T16:12:07Z"
    generateName: calico-node-
    labels:
      app.kubernetes.io/name: calico-node
      controller-revision-hash: 6ccf5bfd78
      k8s-app: calico-node
      pod-template-generation: "1"
    name: calico-node-v5lcv
    namespace: calico-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: calico-node
      uid: 82403527-ae8d-4ba4-a2a0-e2ebe48bf965
    resourceVersion: "13074606"
    uid: 6c02cc0d-0bb1-4354-8024-c9e95fe82df1
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - master-1
    containers:
    - env:
      - name: DATASTORE_TYPE
        value: kubernetes
      - name: WAIT_FOR_DATASTORE
        value: "true"
      - name: CLUSTER_TYPE
        value: k8s,operator,bgp
      - name: CALICO_DISABLE_FILE_LOGGING
        value: "false"
      - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
        value: ACCEPT
      - name: FELIX_HEALTHENABLED
        value: "true"
      - name: FELIX_HEALTHPORT
        value: "9099"
      - name: NODENAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: FELIX_TYPHAK8SNAMESPACE
        value: calico-system
      - name: FELIX_TYPHAK8SSERVICENAME
        value: calico-typha
      - name: FELIX_TYPHACAFILE
        value: /etc/pki/tls/certs/tigera-ca-bundle.crt
      - name: FELIX_TYPHACERTFILE
        value: /node-certs/tls.crt
      - name: FELIX_TYPHAKEYFILE
        value: /node-certs/tls.key
      - name: FIPS_MODE_ENABLED
        value: "false"
      - name: FELIX_TYPHACN
        value: typha-server
      - name: CALICO_MANAGE_CNI
        value: "true"
      - name: CALICO_IPV4POOL_CIDR
        value: 10.42.0.0/16
      - name: CALICO_IPV4POOL_VXLAN
        value: CrossSubnet
      - name: CALICO_IPV4POOL_BLOCK_SIZE
        value: "26"
      - name: CALICO_IPV4POOL_NODE_SELECTOR
        value: all()
      - name: CALICO_IPV4POOL_DISABLE_BGP_EXPORT
        value: "false"
      - name: CALICO_NETWORKING_BACKEND
        value: bird
      - name: IP
        value: autodetect
      - name: IP_AUTODETECTION_METHOD
        value: first-found
      - name: IP6
        value: none
      - name: FELIX_IPV6SUPPORT
        value: "false"
      - name: KUBERNETES_SERVICE_HOST
        value: 10.43.0.1
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: docker.io/calico/node:master
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/calico-node
            - -shutdown
      livenessProbe:
        failureThreshold: 3
        httpGet:
          host: localhost
          path: /liveness
          port: 9099
          scheme: HTTP
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 10
      name: calico-node
      readinessProbe:
        exec:
          command:
          - /bin/calico-node
          - -bird-ready
          - -felix-ready
        failureThreshold: 3
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/pki/tls/certs
        name: tigera-ca-bundle
        readOnly: true
      - mountPath: /etc/pki/tls/cert.pem
        name: tigera-ca-bundle
        readOnly: true
        subPath: ca-bundle.crt
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/nodeagent
        name: policysync
      - mountPath: /node-certs
        name: node-certs
        readOnly: true
      - mountPath: /var/run/calico
        name: var-run-calico
      - mountPath: /var/lib/calico
        name: var-lib-calico
      - mountPath: /var/log/calico/cni
        name: cni-log-dir
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x62s6
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - image: docker.io/calico/pod2daemon-flexvol:master
      imagePullPolicy: IfNotPresent
      name: flexvol-driver
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/driver
        name: flexvol-driver-host
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x62s6
        readOnly: true
    - command:
      - /opt/cni/bin/install
      env:
      - name: CNI_CONF_NAME
        value: 10-calico.conflist
      - name: SLEEP
        value: "false"
      - name: CNI_NET_DIR
        value: /etc/cni/net.d
      - name: CNI_NETWORK_CONFIG
        valueFrom:
          configMapKeyRef:
            key: config
            name: cni-config
      - name: KUBERNETES_SERVICE_HOST
        value: 10.43.0.1
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: docker.io/calico/cni:master
      imagePullPolicy: IfNotPresent
      name: install-cni
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cni-bin-dir
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x62s6
        readOnly: true
    nodeName: master-1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: calico-node
    serviceAccountName: calico-node
    terminationGracePeriodSeconds: 5
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /var/run/nodeagent
        type: DirectoryOrCreate
      name: policysync
    - configMap:
        defaultMode: 420
        name: tigera-ca-bundle
      name: tigera-ca-bundle
    - name: node-certs
      secret:
        defaultMode: 420
        secretName: node-certs
    - hostPath:
        path: /var/run/calico
        type: ""
      name: var-run-calico
    - hostPath:
        path: /var/lib/calico
        type: ""
      name: var-lib-calico
    - hostPath:
        path: /opt/cni/bin
        type: ""
      name: cni-bin-dir
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni-net-dir
    - hostPath:
        path: /var/log/calico/cni
        type: ""
      name: cni-log-dir
    - hostPath:
        path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds
        type: DirectoryOrCreate
      name: flexvol-driver-host
    - name: kube-api-access-x62s6
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:14:27Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:17:07Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:17:07Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://f3b8c1cd662d3fc9655cf9fad81d9ded33aee7f9cff6646c2542b1bf821dafe8
      image: calico/node:master
      imageID: docker-pullable://calico/node@sha256:48f83d5dfa9111eb4cab0f99abe7b3e40dae57610d16bd3f539447493a7f8618
      lastState: {}
      name: calico-node
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:16:15Z"
    hostIP: 10.2.1.180
    initContainerStatuses:
    - containerID: docker://b44429f57300430583d747e0cca192064d4c0ef25085979b384b3fe6e2fbcfa8
      image: calico/pod2daemon-flexvol:master
      imageID: docker-pullable://calico/pod2daemon-flexvol@sha256:81e82599290cdd04e348497d1b69516891094a1b91f26d4390755b3bc34eb297
      lastState: {}
      name: flexvol-driver
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://b44429f57300430583d747e0cca192064d4c0ef25085979b384b3fe6e2fbcfa8
          exitCode: 0
          finishedAt: "2024-03-13T16:12:18Z"
          reason: Completed
          startedAt: "2024-03-13T16:12:17Z"
    - containerID: docker://89ca75a90a070272b64e995912655000347a8f294cab45db4fc81b7526bc40c8
      image: calico/cni:master
      imageID: docker-pullable://calico/cni@sha256:44da8846472e8c262a666e19ebbb6208f2607e11e40fd4b96aa704c92ab794b9
      lastState: {}
      name: install-cni
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://89ca75a90a070272b64e995912655000347a8f294cab45db4fc81b7526bc40c8
          exitCode: 0
          finishedAt: "2024-03-13T16:14:27Z"
          reason: Completed
          startedAt: "2024-03-13T16:14:26Z"
    phase: Running
    podIP: 10.2.1.180
    podIPs:
    - ip: 10.2.1.180
    qosClass: BestEffort
    startTime: "2024-03-13T16:12:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      hash.operator.tigera.io/cni-config: 67d9927589bcf84d027e873bcb62d5ee6677ab82
      hash.operator.tigera.io/system: fdde45054a8ae4f629960ce37570929502e59449
      tigera-operator.hash.operator.tigera.io/tigera-ca-private: 4f5267a1bedc8badce3a9288fba115ecfdb158ed
    creationTimestamp: "2024-03-13T16:12:07Z"
    generateName: calico-node-
    labels:
      app.kubernetes.io/name: calico-node
      controller-revision-hash: 6ccf5bfd78
      k8s-app: calico-node
      pod-template-generation: "1"
    name: calico-node-wd98r
    namespace: calico-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: calico-node
      uid: 82403527-ae8d-4ba4-a2a0-e2ebe48bf965
    resourceVersion: "13074608"
    uid: cdf0016f-5cb0-4e35-8ce3-1f84d96c4654
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - master-3
    containers:
    - env:
      - name: DATASTORE_TYPE
        value: kubernetes
      - name: WAIT_FOR_DATASTORE
        value: "true"
      - name: CLUSTER_TYPE
        value: k8s,operator,bgp
      - name: CALICO_DISABLE_FILE_LOGGING
        value: "false"
      - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
        value: ACCEPT
      - name: FELIX_HEALTHENABLED
        value: "true"
      - name: FELIX_HEALTHPORT
        value: "9099"
      - name: NODENAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: FELIX_TYPHAK8SNAMESPACE
        value: calico-system
      - name: FELIX_TYPHAK8SSERVICENAME
        value: calico-typha
      - name: FELIX_TYPHACAFILE
        value: /etc/pki/tls/certs/tigera-ca-bundle.crt
      - name: FELIX_TYPHACERTFILE
        value: /node-certs/tls.crt
      - name: FELIX_TYPHAKEYFILE
        value: /node-certs/tls.key
      - name: FIPS_MODE_ENABLED
        value: "false"
      - name: FELIX_TYPHACN
        value: typha-server
      - name: CALICO_MANAGE_CNI
        value: "true"
      - name: CALICO_IPV4POOL_CIDR
        value: 10.42.0.0/16
      - name: CALICO_IPV4POOL_VXLAN
        value: CrossSubnet
      - name: CALICO_IPV4POOL_BLOCK_SIZE
        value: "26"
      - name: CALICO_IPV4POOL_NODE_SELECTOR
        value: all()
      - name: CALICO_IPV4POOL_DISABLE_BGP_EXPORT
        value: "false"
      - name: CALICO_NETWORKING_BACKEND
        value: bird
      - name: IP
        value: autodetect
      - name: IP_AUTODETECTION_METHOD
        value: first-found
      - name: IP6
        value: none
      - name: FELIX_IPV6SUPPORT
        value: "false"
      - name: KUBERNETES_SERVICE_HOST
        value: 10.43.0.1
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: docker.io/calico/node:master
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/calico-node
            - -shutdown
      livenessProbe:
        failureThreshold: 3
        httpGet:
          host: localhost
          path: /liveness
          port: 9099
          scheme: HTTP
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 10
      name: calico-node
      readinessProbe:
        exec:
          command:
          - /bin/calico-node
          - -bird-ready
          - -felix-ready
        failureThreshold: 3
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/pki/tls/certs
        name: tigera-ca-bundle
        readOnly: true
      - mountPath: /etc/pki/tls/cert.pem
        name: tigera-ca-bundle
        readOnly: true
        subPath: ca-bundle.crt
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/nodeagent
        name: policysync
      - mountPath: /node-certs
        name: node-certs
        readOnly: true
      - mountPath: /var/run/calico
        name: var-run-calico
      - mountPath: /var/lib/calico
        name: var-lib-calico
      - mountPath: /var/log/calico/cni
        name: cni-log-dir
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-h2hcs
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - image: docker.io/calico/pod2daemon-flexvol:master
      imagePullPolicy: IfNotPresent
      name: flexvol-driver
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/driver
        name: flexvol-driver-host
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-h2hcs
        readOnly: true
    - command:
      - /opt/cni/bin/install
      env:
      - name: CNI_CONF_NAME
        value: 10-calico.conflist
      - name: SLEEP
        value: "false"
      - name: CNI_NET_DIR
        value: /etc/cni/net.d
      - name: CNI_NETWORK_CONFIG
        valueFrom:
          configMapKeyRef:
            key: config
            name: cni-config
      - name: KUBERNETES_SERVICE_HOST
        value: 10.43.0.1
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: docker.io/calico/cni:master
      imagePullPolicy: IfNotPresent
      name: install-cni
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cni-bin-dir
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-h2hcs
        readOnly: true
    nodeName: master-3
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: calico-node
    serviceAccountName: calico-node
    terminationGracePeriodSeconds: 5
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /var/run/nodeagent
        type: DirectoryOrCreate
      name: policysync
    - configMap:
        defaultMode: 420
        name: tigera-ca-bundle
      name: tigera-ca-bundle
    - name: node-certs
      secret:
        defaultMode: 420
        secretName: node-certs
    - hostPath:
        path: /var/run/calico
        type: ""
      name: var-run-calico
    - hostPath:
        path: /var/lib/calico
        type: ""
      name: var-lib-calico
    - hostPath:
        path: /opt/cni/bin
        type: ""
      name: cni-bin-dir
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni-net-dir
    - hostPath:
        path: /var/log/calico/cni
        type: ""
      name: cni-log-dir
    - hostPath:
        path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds
        type: DirectoryOrCreate
      name: flexvol-driver-host
    - name: kube-api-access-h2hcs
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:14:25Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:17:07Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:17:07Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://48dcd65a2bcdd1046c763f917835b2b7a0ea9e35f1bd8359e28b35cbcbc8b493
      image: calico/node:master
      imageID: docker-pullable://calico/node@sha256:48f83d5dfa9111eb4cab0f99abe7b3e40dae57610d16bd3f539447493a7f8618
      lastState: {}
      name: calico-node
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:16:20Z"
    hostIP: 10.2.1.182
    initContainerStatuses:
    - containerID: docker://c9165cca89e568ea9c7dc6b3507d523bbcb647955aaa7c8078808a662781ed16
      image: calico/pod2daemon-flexvol:master
      imageID: docker-pullable://calico/pod2daemon-flexvol@sha256:81e82599290cdd04e348497d1b69516891094a1b91f26d4390755b3bc34eb297
      lastState: {}
      name: flexvol-driver
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://c9165cca89e568ea9c7dc6b3507d523bbcb647955aaa7c8078808a662781ed16
          exitCode: 0
          finishedAt: "2024-03-13T16:12:19Z"
          reason: Completed
          startedAt: "2024-03-13T16:12:19Z"
    - containerID: docker://8dc9d1335e0f4bf6a0021c54f32ee6683e973c16e56c5cf8eb26e85115f15314
      image: calico/cni:master
      imageID: docker-pullable://calico/cni@sha256:44da8846472e8c262a666e19ebbb6208f2607e11e40fd4b96aa704c92ab794b9
      lastState: {}
      name: install-cni
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://8dc9d1335e0f4bf6a0021c54f32ee6683e973c16e56c5cf8eb26e85115f15314
          exitCode: 0
          finishedAt: "2024-03-13T16:14:24Z"
          reason: Completed
          startedAt: "2024-03-13T16:14:24Z"
    phase: Running
    podIP: 10.2.1.182
    podIPs:
    - ip: 10.2.1.182
    qosClass: BestEffort
    startTime: "2024-03-13T16:12:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      hash.operator.tigera.io/system: fdde45054a8ae4f629960ce37570929502e59449
      tigera-operator.hash.operator.tigera.io/tigera-ca-private: 4f5267a1bedc8badce3a9288fba115ecfdb158ed
      tigera-operator.hash.operator.tigera.io/typha-certs: c9d75be38dacbdb51cf4b46c6f5d1bc93a95e3db
    creationTimestamp: "2024-03-13T16:12:10Z"
    generateName: calico-typha-5d4bd748b9-
    labels:
      app.kubernetes.io/name: calico-typha
      k8s-app: calico-typha
      pod-template-hash: 5d4bd748b9
    name: calico-typha-5d4bd748b9-g6mfm
    namespace: calico-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: calico-typha-5d4bd748b9
      uid: 85b27842-6d00-4aab-867a-24f6db7d7d98
    resourceVersion: "13073178"
    uid: 745c88fd-2424-4eeb-aeb7-34348dda3872
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - calico-typha
            topologyKey: topology.kubernetes.io/zone
          weight: 1
    containers:
    - env:
      - name: TYPHA_LOGSEVERITYSCREEN
        value: info
      - name: TYPHA_LOGFILEPATH
        value: none
      - name: TYPHA_LOGSEVERITYSYS
        value: none
      - name: TYPHA_CONNECTIONREBALANCINGMODE
        value: kubernetes
      - name: TYPHA_DATASTORETYPE
        value: kubernetes
      - name: TYPHA_HEALTHENABLED
        value: "true"
      - name: TYPHA_HEALTHPORT
        value: "9098"
      - name: TYPHA_K8SNAMESPACE
        value: calico-system
      - name: TYPHA_CAFILE
        value: /etc/pki/tls/certs/tigera-ca-bundle.crt
      - name: TYPHA_SERVERCERTFILE
        value: /typha-certs/tls.crt
      - name: TYPHA_SERVERKEYFILE
        value: /typha-certs/tls.key
      - name: TYPHA_FIPSMODEENABLED
        value: "false"
      - name: TYPHA_SHUTDOWNTIMEOUTSECS
        value: "300"
      - name: TYPHA_CLIENTCN
        value: typha-client
      - name: KUBERNETES_SERVICE_HOST
        value: 10.43.0.1
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: docker.io/calico/typha:master
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          host: localhost
          path: /liveness
          port: 9098
          scheme: HTTP
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 10
      name: calico-typha
      ports:
      - containerPort: 5473
        hostPort: 5473
        name: calico-typha
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: localhost
          path: /readiness
          port: 9098
          scheme: HTTP
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/pki/tls/certs
        name: tigera-ca-bundle
        readOnly: true
      - mountPath: /etc/pki/tls/cert.pem
        name: tigera-ca-bundle
        readOnly: true
        subPath: ca-bundle.crt
      - mountPath: /typha-certs
        name: typha-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kbtkz
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: master-1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: calico-typha
    serviceAccountName: calico-typha
    terminationGracePeriodSeconds: 300
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: tigera-ca-bundle
      name: tigera-ca-bundle
    - name: typha-certs
      secret:
        defaultMode: 420
        secretName: typha-certs
    - name: kube-api-access-kbtkz
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:10Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:57Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:57Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:10Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://6f41ef8624e3fbe989374b2e60f15312cce701c73514c44c766ca4bfa2c29c30
      image: calico/typha:master
      imageID: docker-pullable://calico/typha@sha256:9cb70c141405d06c3397a24af1f1a97cbb9c2617cabd55203a7a14775d6cafa3
      lastState: {}
      name: calico-typha
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:12:57Z"
    hostIP: 10.2.1.180
    phase: Running
    podIP: 10.2.1.180
    podIPs:
    - ip: 10.2.1.180
    qosClass: BestEffort
    startTime: "2024-03-13T16:12:10Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      hash.operator.tigera.io/system: fdde45054a8ae4f629960ce37570929502e59449
      tigera-operator.hash.operator.tigera.io/tigera-ca-private: 4f5267a1bedc8badce3a9288fba115ecfdb158ed
      tigera-operator.hash.operator.tigera.io/typha-certs: c9d75be38dacbdb51cf4b46c6f5d1bc93a95e3db
    creationTimestamp: "2024-03-13T16:12:07Z"
    generateName: calico-typha-5d4bd748b9-
    labels:
      app.kubernetes.io/name: calico-typha
      k8s-app: calico-typha
      pod-template-hash: 5d4bd748b9
    name: calico-typha-5d4bd748b9-ptxm5
    namespace: calico-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: calico-typha-5d4bd748b9
      uid: 85b27842-6d00-4aab-867a-24f6db7d7d98
    resourceVersion: "13073066"
    uid: f82dd8f6-419d-4853-817b-dbdb9168a804
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - calico-typha
            topologyKey: topology.kubernetes.io/zone
          weight: 1
    containers:
    - env:
      - name: TYPHA_LOGSEVERITYSCREEN
        value: info
      - name: TYPHA_LOGFILEPATH
        value: none
      - name: TYPHA_LOGSEVERITYSYS
        value: none
      - name: TYPHA_CONNECTIONREBALANCINGMODE
        value: kubernetes
      - name: TYPHA_DATASTORETYPE
        value: kubernetes
      - name: TYPHA_HEALTHENABLED
        value: "true"
      - name: TYPHA_HEALTHPORT
        value: "9098"
      - name: TYPHA_K8SNAMESPACE
        value: calico-system
      - name: TYPHA_CAFILE
        value: /etc/pki/tls/certs/tigera-ca-bundle.crt
      - name: TYPHA_SERVERCERTFILE
        value: /typha-certs/tls.crt
      - name: TYPHA_SERVERKEYFILE
        value: /typha-certs/tls.key
      - name: TYPHA_FIPSMODEENABLED
        value: "false"
      - name: TYPHA_SHUTDOWNTIMEOUTSECS
        value: "300"
      - name: TYPHA_CLIENTCN
        value: typha-client
      - name: KUBERNETES_SERVICE_HOST
        value: 10.43.0.1
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: docker.io/calico/typha:master
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          host: localhost
          path: /liveness
          port: 9098
          scheme: HTTP
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 10
      name: calico-typha
      ports:
      - containerPort: 5473
        hostPort: 5473
        name: calico-typha
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: localhost
          path: /readiness
          port: 9098
          scheme: HTTP
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/pki/tls/certs
        name: tigera-ca-bundle
        readOnly: true
      - mountPath: /etc/pki/tls/cert.pem
        name: tigera-ca-bundle
        readOnly: true
        subPath: ca-bundle.crt
      - mountPath: /typha-certs
        name: typha-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-c6tzq
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: master-2
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: calico-typha
    serviceAccountName: calico-typha
    terminationGracePeriodSeconds: 300
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: tigera-ca-bundle
      name: tigera-ca-bundle
    - name: typha-certs
      secret:
        defaultMode: 420
        secretName: typha-certs
    - name: kube-api-access-c6tzq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:07Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:34Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:34Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://35d5d2d9af3bc6027a294a1439519650a5af76a062ed458e51d2a017a8592911
      image: calico/typha:master
      imageID: docker-pullable://calico/typha@sha256:9cb70c141405d06c3397a24af1f1a97cbb9c2617cabd55203a7a14775d6cafa3
      lastState: {}
      name: calico-typha
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:12:33Z"
    hostIP: 10.2.1.181
    phase: Running
    podIP: 10.2.1.181
    podIPs:
    - ip: 10.2.1.181
    qosClass: BestEffort
    startTime: "2024-03-13T16:12:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      hash.operator.tigera.io/system: fdde45054a8ae4f629960ce37570929502e59449
      tigera-operator.hash.operator.tigera.io/tigera-ca-private: 4f5267a1bedc8badce3a9288fba115ecfdb158ed
      tigera-operator.hash.operator.tigera.io/typha-certs: c9d75be38dacbdb51cf4b46c6f5d1bc93a95e3db
    creationTimestamp: "2024-03-13T16:12:10Z"
    generateName: calico-typha-5d4bd748b9-
    labels:
      app.kubernetes.io/name: calico-typha
      k8s-app: calico-typha
      pod-template-hash: 5d4bd748b9
    name: calico-typha-5d4bd748b9-rlqvs
    namespace: calico-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: calico-typha-5d4bd748b9
      uid: 85b27842-6d00-4aab-867a-24f6db7d7d98
    resourceVersion: "13073185"
    uid: 5a9cc704-7365-4fd1-86b7-ad5f83975e7c
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - calico-typha
            topologyKey: topology.kubernetes.io/zone
          weight: 1
    containers:
    - env:
      - name: TYPHA_LOGSEVERITYSCREEN
        value: info
      - name: TYPHA_LOGFILEPATH
        value: none
      - name: TYPHA_LOGSEVERITYSYS
        value: none
      - name: TYPHA_CONNECTIONREBALANCINGMODE
        value: kubernetes
      - name: TYPHA_DATASTORETYPE
        value: kubernetes
      - name: TYPHA_HEALTHENABLED
        value: "true"
      - name: TYPHA_HEALTHPORT
        value: "9098"
      - name: TYPHA_K8SNAMESPACE
        value: calico-system
      - name: TYPHA_CAFILE
        value: /etc/pki/tls/certs/tigera-ca-bundle.crt
      - name: TYPHA_SERVERCERTFILE
        value: /typha-certs/tls.crt
      - name: TYPHA_SERVERKEYFILE
        value: /typha-certs/tls.key
      - name: TYPHA_FIPSMODEENABLED
        value: "false"
      - name: TYPHA_SHUTDOWNTIMEOUTSECS
        value: "300"
      - name: TYPHA_CLIENTCN
        value: typha-client
      - name: KUBERNETES_SERVICE_HOST
        value: 10.43.0.1
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: docker.io/calico/typha:master
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          host: localhost
          path: /liveness
          port: 9098
          scheme: HTTP
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 10
      name: calico-typha
      ports:
      - containerPort: 5473
        hostPort: 5473
        name: calico-typha
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: localhost
          path: /readiness
          port: 9098
          scheme: HTTP
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/pki/tls/certs
        name: tigera-ca-bundle
        readOnly: true
      - mountPath: /etc/pki/tls/cert.pem
        name: tigera-ca-bundle
        readOnly: true
        subPath: ca-bundle.crt
      - mountPath: /typha-certs
        name: typha-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cxgmv
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: master-3
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: calico-typha
    serviceAccountName: calico-typha
    terminationGracePeriodSeconds: 300
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: tigera-ca-bundle
      name: tigera-ca-bundle
    - name: typha-certs
      secret:
        defaultMode: 420
        secretName: typha-certs
    - name: kube-api-access-cxgmv
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:10Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:57Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:57Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:10Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://9d32771d5276cc1985e60abdd50fb20398cac66a2ff73095096d2be15dcd3344
      image: calico/typha:master
      imageID: docker-pullable://calico/typha@sha256:9cb70c141405d06c3397a24af1f1a97cbb9c2617cabd55203a7a14775d6cafa3
      lastState: {}
      name: calico-typha
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:12:57Z"
    hostIP: 10.2.1.182
    phase: Running
    podIP: 10.2.1.182
    podIPs:
    - ip: 10.2.1.182
    qosClass: BestEffort
    startTime: "2024-03-13T16:12:10Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 7721250f57f61deb9d6414bb9c524ada82bdf234709221a9b0bd5e26800c82a7
      cni.projectcalico.org/podIP: 10.42.38.64/32
      cni.projectcalico.org/podIPs: 10.42.38.64/32
    creationTimestamp: "2024-03-13T16:12:07Z"
    generateName: csi-node-driver-
    labels:
      app.kubernetes.io/name: csi-node-driver
      controller-revision-hash: 64874c855f
      k8s-app: csi-node-driver
      name: csi-node-driver
      pod-template-generation: "1"
    name: csi-node-driver-628jw
    namespace: calico-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-node-driver
      uid: fdae6af8-076d-4c08-9d47-2295f1381898
    resourceVersion: "13074350"
    uid: f1a4d1e3-be23-47a5-87f0-e6a608053caa
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - worker-4
    containers:
    - args:
      - --nodeid=$(KUBE_NODE_NAME)
      - --loglevel=$(LOG_LEVEL)
      env:
      - name: LOG_LEVEL
        value: warn
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/calico/csi:master
      imagePullPolicy: IfNotPresent
      name: calico-csi
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run
        name: varrun
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/lib/kubelet
        mountPropagation: Bidirectional
        name: kubelet-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5l9fz
        readOnly: true
    - args:
      - --v=5
      - --csi-address=$(ADDRESS)
      - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)
      env:
      - name: ADDRESS
        value: /csi/csi.sock
      - name: DRIVER_REG_SOCK_PATH
        value: /var/lib/kubelet/plugins/csi.tigera.io/csi.sock
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/calico/node-driver-registrar:master
      imagePullPolicy: IfNotPresent
      name: csi-node-driver-registrar
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5l9fz
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker-4
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/run
        type: ""
      name: varrun
    - hostPath:
        path: /var/lib/kubelet
        type: Directory
      name: kubelet-dir
    - hostPath:
        path: /var/lib/kubelet/plugins/csi.tigera.io
        type: DirectoryOrCreate
      name: socket-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry
        type: Directory
      name: registration-dir
    - name: kube-api-access-5l9fz
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:07Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:16:16Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:16:16Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://5493495b2fadb5c6fd6c84e330a424d1de722dff941acd2869e1c375c1570362
      image: calico/csi:master
      imageID: docker-pullable://calico/csi@sha256:28fc73661d7c3214f6cd345ab613585c430a2258328dd59d9f0a0f7080075ee6
      lastState: {}
      name: calico-csi
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:16:00Z"
    - containerID: docker://f93021196ac34cd907e6db1328cf9a0026b7d131f77495f7fc3f580dc3e70b48
      image: calico/node-driver-registrar:master
      imageID: docker-pullable://calico/node-driver-registrar@sha256:228f31848eaee8a7b2bc92cea1997f955c1b425620a84c0a24c24d1b8ed0307f
      lastState: {}
      name: csi-node-driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:16:15Z"
    hostIP: 10.2.1.186
    phase: Running
    podIP: 10.42.38.64
    podIPs:
    - ip: 10.42.38.64
    qosClass: BestEffort
    startTime: "2024-03-13T16:12:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 89618c323d35a61ac73dd85692ff6550d8e4c0e6807b58ba34e19b23885de5a2
      cni.projectcalico.org/podIP: 10.42.39.0/32
      cni.projectcalico.org/podIPs: 10.42.39.0/32
    creationTimestamp: "2024-03-13T16:12:07Z"
    generateName: csi-node-driver-
    labels:
      app.kubernetes.io/name: csi-node-driver
      controller-revision-hash: 64874c855f
      k8s-app: csi-node-driver
      name: csi-node-driver
      pod-template-generation: "1"
    name: csi-node-driver-748r4
    namespace: calico-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-node-driver
      uid: fdae6af8-076d-4c08-9d47-2295f1381898
    resourceVersion: "13074410"
    uid: c47eff8c-91d7-4e04-80fa-74e3be5858d3
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - master-1
    containers:
    - args:
      - --nodeid=$(KUBE_NODE_NAME)
      - --loglevel=$(LOG_LEVEL)
      env:
      - name: LOG_LEVEL
        value: warn
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/calico/csi:master
      imagePullPolicy: IfNotPresent
      name: calico-csi
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run
        name: varrun
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/lib/kubelet
        mountPropagation: Bidirectional
        name: kubelet-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qd77l
        readOnly: true
    - args:
      - --v=5
      - --csi-address=$(ADDRESS)
      - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)
      env:
      - name: ADDRESS
        value: /csi/csi.sock
      - name: DRIVER_REG_SOCK_PATH
        value: /var/lib/kubelet/plugins/csi.tigera.io/csi.sock
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/calico/node-driver-registrar:master
      imagePullPolicy: IfNotPresent
      name: csi-node-driver-registrar
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qd77l
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: master-1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/run
        type: ""
      name: varrun
    - hostPath:
        path: /var/lib/kubelet
        type: Directory
      name: kubelet-dir
    - hostPath:
        path: /var/lib/kubelet/plugins/csi.tigera.io
        type: DirectoryOrCreate
      name: socket-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry
        type: Directory
      name: registration-dir
    - name: kube-api-access-qd77l
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:07Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:16:26Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:16:26Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://c1c6809d2e8a69686ac0bf68041a865311ff516ce8b70d9ac7651931afeb0630
      image: calico/csi:master
      imageID: docker-pullable://calico/csi@sha256:28fc73661d7c3214f6cd345ab613585c430a2258328dd59d9f0a0f7080075ee6
      lastState: {}
      name: calico-csi
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:16:19Z"
    - containerID: docker://975780243f547e293910a73079347679f6627e8751a5a68329f3f302fc7bef7f
      image: calico/node-driver-registrar:master
      imageID: docker-pullable://calico/node-driver-registrar@sha256:228f31848eaee8a7b2bc92cea1997f955c1b425620a84c0a24c24d1b8ed0307f
      lastState: {}
      name: csi-node-driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:16:25Z"
    hostIP: 10.2.1.180
    phase: Running
    podIP: 10.42.39.0
    podIPs:
    - ip: 10.42.39.0
    qosClass: BestEffort
    startTime: "2024-03-13T16:12:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 1145d9094c503de4b2671e05340cb55911ce64b9c660769454e2e04f90803ef4
      cni.projectcalico.org/podIP: 10.42.236.64/32
      cni.projectcalico.org/podIPs: 10.42.236.64/32
    creationTimestamp: "2024-03-13T16:12:07Z"
    generateName: csi-node-driver-
    labels:
      app.kubernetes.io/name: csi-node-driver
      controller-revision-hash: 64874c855f
      k8s-app: csi-node-driver
      name: csi-node-driver
      pod-template-generation: "1"
    name: csi-node-driver-9h4zr
    namespace: calico-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-node-driver
      uid: fdae6af8-076d-4c08-9d47-2295f1381898
    resourceVersion: "13074431"
    uid: 027ae220-e4ca-4b34-8878-9272c5bd3ace
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - master-3
    containers:
    - args:
      - --nodeid=$(KUBE_NODE_NAME)
      - --loglevel=$(LOG_LEVEL)
      env:
      - name: LOG_LEVEL
        value: warn
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/calico/csi:master
      imagePullPolicy: IfNotPresent
      name: calico-csi
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run
        name: varrun
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/lib/kubelet
        mountPropagation: Bidirectional
        name: kubelet-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mhnjx
        readOnly: true
    - args:
      - --v=5
      - --csi-address=$(ADDRESS)
      - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)
      env:
      - name: ADDRESS
        value: /csi/csi.sock
      - name: DRIVER_REG_SOCK_PATH
        value: /var/lib/kubelet/plugins/csi.tigera.io/csi.sock
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/calico/node-driver-registrar:master
      imagePullPolicy: IfNotPresent
      name: csi-node-driver-registrar
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mhnjx
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: master-3
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/run
        type: ""
      name: varrun
    - hostPath:
        path: /var/lib/kubelet
        type: Directory
      name: kubelet-dir
    - hostPath:
        path: /var/lib/kubelet/plugins/csi.tigera.io
        type: DirectoryOrCreate
      name: socket-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry
        type: Directory
      name: registration-dir
    - name: kube-api-access-mhnjx
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:07Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:16:30Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:16:30Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://1ac4e52ff409fb0445a7ef39ff55ec9a0adacff0546db32398a5732ff7086441
      image: calico/csi:master
      imageID: docker-pullable://calico/csi@sha256:28fc73661d7c3214f6cd345ab613585c430a2258328dd59d9f0a0f7080075ee6
      lastState: {}
      name: calico-csi
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:16:24Z"
    - containerID: docker://f7bcb906459f1610c2f0c60e70fb9cb2ba48590d1bb40f227b61fd6a266e0474
      image: calico/node-driver-registrar:master
      imageID: docker-pullable://calico/node-driver-registrar@sha256:228f31848eaee8a7b2bc92cea1997f955c1b425620a84c0a24c24d1b8ed0307f
      lastState: {}
      name: csi-node-driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:16:30Z"
    hostIP: 10.2.1.182
    phase: Running
    podIP: 10.42.236.64
    podIPs:
    - ip: 10.42.236.64
    qosClass: BestEffort
    startTime: "2024-03-13T16:12:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 5f82ac25c7da35e337245ebf04b199a8c7ccea77b0ebcd33c12b8cee569f7a09
      cni.projectcalico.org/podIP: 10.42.133.192/32
      cni.projectcalico.org/podIPs: 10.42.133.192/32
    creationTimestamp: "2024-03-13T16:12:07Z"
    generateName: csi-node-driver-
    labels:
      app.kubernetes.io/name: csi-node-driver
      controller-revision-hash: 64874c855f
      k8s-app: csi-node-driver
      name: csi-node-driver
      pod-template-generation: "1"
    name: csi-node-driver-cphbs
    namespace: calico-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-node-driver
      uid: fdae6af8-076d-4c08-9d47-2295f1381898
    resourceVersion: "13074395"
    uid: e28f781e-b439-4d65-a431-e23abe36d02b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - worker-2
    containers:
    - args:
      - --nodeid=$(KUBE_NODE_NAME)
      - --loglevel=$(LOG_LEVEL)
      env:
      - name: LOG_LEVEL
        value: warn
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/calico/csi:master
      imagePullPolicy: IfNotPresent
      name: calico-csi
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run
        name: varrun
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/lib/kubelet
        mountPropagation: Bidirectional
        name: kubelet-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5gzkm
        readOnly: true
    - args:
      - --v=5
      - --csi-address=$(ADDRESS)
      - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)
      env:
      - name: ADDRESS
        value: /csi/csi.sock
      - name: DRIVER_REG_SOCK_PATH
        value: /var/lib/kubelet/plugins/csi.tigera.io/csi.sock
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/calico/node-driver-registrar:master
      imagePullPolicy: IfNotPresent
      name: csi-node-driver-registrar
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5gzkm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker-2
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/run
        type: ""
      name: varrun
    - hostPath:
        path: /var/lib/kubelet
        type: Directory
      name: kubelet-dir
    - hostPath:
        path: /var/lib/kubelet/plugins/csi.tigera.io
        type: DirectoryOrCreate
      name: socket-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry
        type: Directory
      name: registration-dir
    - name: kube-api-access-5gzkm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:07Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:16:23Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:16:23Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://06bce720a45d1bb035114ffb1176f385665e79c66ce2fc5c612d2d9362c86e77
      image: calico/csi:master
      imageID: docker-pullable://calico/csi@sha256:28fc73661d7c3214f6cd345ab613585c430a2258328dd59d9f0a0f7080075ee6
      lastState: {}
      name: calico-csi
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:15:28Z"
    - containerID: docker://1782b076bd8f4baee7495b92a52df3046e787b3576fab94325c9f6112dd23ec7
      image: calico/node-driver-registrar:master
      imageID: docker-pullable://calico/node-driver-registrar@sha256:228f31848eaee8a7b2bc92cea1997f955c1b425620a84c0a24c24d1b8ed0307f
      lastState: {}
      name: csi-node-driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:16:22Z"
    hostIP: 10.2.1.184
    phase: Running
    podIP: 10.42.133.192
    podIPs:
    - ip: 10.42.133.192
    qosClass: BestEffort
    startTime: "2024-03-13T16:12:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: aef740c12c0c82e5c7c230019964f8f016dafc7c4e02536601bed76560fa2cec
      cni.projectcalico.org/podIP: 10.42.97.192/32
      cni.projectcalico.org/podIPs: 10.42.97.192/32
    creationTimestamp: "2024-03-13T16:12:07Z"
    generateName: csi-node-driver-
    labels:
      app.kubernetes.io/name: csi-node-driver
      controller-revision-hash: 64874c855f
      k8s-app: csi-node-driver
      name: csi-node-driver
      pod-template-generation: "1"
    name: csi-node-driver-gh8ks
    namespace: calico-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-node-driver
      uid: fdae6af8-076d-4c08-9d47-2295f1381898
    resourceVersion: "13074338"
    uid: 6dc8fd1b-4e7d-4a44-808e-0a9d27a21ff6
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - worker-3
    containers:
    - args:
      - --nodeid=$(KUBE_NODE_NAME)
      - --loglevel=$(LOG_LEVEL)
      env:
      - name: LOG_LEVEL
        value: warn
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/calico/csi:master
      imagePullPolicy: IfNotPresent
      name: calico-csi
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run
        name: varrun
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/lib/kubelet
        mountPropagation: Bidirectional
        name: kubelet-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5cknt
        readOnly: true
    - args:
      - --v=5
      - --csi-address=$(ADDRESS)
      - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)
      env:
      - name: ADDRESS
        value: /csi/csi.sock
      - name: DRIVER_REG_SOCK_PATH
        value: /var/lib/kubelet/plugins/csi.tigera.io/csi.sock
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/calico/node-driver-registrar:master
      imagePullPolicy: IfNotPresent
      name: csi-node-driver-registrar
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5cknt
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker-3
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/run
        type: ""
      name: varrun
    - hostPath:
        path: /var/lib/kubelet
        type: Directory
      name: kubelet-dir
    - hostPath:
        path: /var/lib/kubelet/plugins/csi.tigera.io
        type: DirectoryOrCreate
      name: socket-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry
        type: Directory
      name: registration-dir
    - name: kube-api-access-5cknt
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:07Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:16:16Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:16:16Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://26bcf9daa2eab6433596605875e8534c68f6b7fae8d8d462d873a7173f82fba7
      image: calico/csi:master
      imageID: docker-pullable://calico/csi@sha256:28fc73661d7c3214f6cd345ab613585c430a2258328dd59d9f0a0f7080075ee6
      lastState: {}
      name: calico-csi
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:16:04Z"
    - containerID: docker://1a4ee85a46d510252db4215cd6a301896ec441e6e3770c7d51201564c1b3771b
      image: calico/node-driver-registrar:master
      imageID: docker-pullable://calico/node-driver-registrar@sha256:228f31848eaee8a7b2bc92cea1997f955c1b425620a84c0a24c24d1b8ed0307f
      lastState: {}
      name: csi-node-driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:16:15Z"
    hostIP: 10.2.1.185
    phase: Running
    podIP: 10.42.97.192
    podIPs:
    - ip: 10.42.97.192
    qosClass: BestEffort
    startTime: "2024-03-13T16:12:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 7ae95c56202bf7bcfcd8ce30d69e1691b03795bc02df4aac474f5297c4e8d42a
      cni.projectcalico.org/podIP: 10.42.226.64/32
      cni.projectcalico.org/podIPs: 10.42.226.64/32
    creationTimestamp: "2024-03-13T16:12:07Z"
    generateName: csi-node-driver-
    labels:
      app.kubernetes.io/name: csi-node-driver
      controller-revision-hash: 64874c855f
      k8s-app: csi-node-driver
      name: csi-node-driver
      pod-template-generation: "1"
    name: csi-node-driver-kgnp7
    namespace: calico-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-node-driver
      uid: fdae6af8-076d-4c08-9d47-2295f1381898
    resourceVersion: "13081945"
    uid: cfbbb9a5-53bd-4cae-9d5d-393b34f057cf
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - worker-1
    containers:
    - args:
      - --nodeid=$(KUBE_NODE_NAME)
      - --loglevel=$(LOG_LEVEL)
      env:
      - name: LOG_LEVEL
        value: warn
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/calico/csi:master
      imagePullPolicy: IfNotPresent
      name: calico-csi
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run
        name: varrun
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/lib/kubelet
        mountPropagation: Bidirectional
        name: kubelet-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hcl6m
        readOnly: true
    - args:
      - --v=5
      - --csi-address=$(ADDRESS)
      - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)
      env:
      - name: ADDRESS
        value: /csi/csi.sock
      - name: DRIVER_REG_SOCK_PATH
        value: /var/lib/kubelet/plugins/csi.tigera.io/csi.sock
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/calico/node-driver-registrar:master
      imagePullPolicy: IfNotPresent
      name: csi-node-driver-registrar
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hcl6m
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker-1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/run
        type: ""
      name: varrun
    - hostPath:
        path: /var/lib/kubelet
        type: Directory
      name: kubelet-dir
    - hostPath:
        path: /var/lib/kubelet/plugins/csi.tigera.io
        type: DirectoryOrCreate
      name: socket-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry
        type: Directory
      name: registration-dir
    - name: kube-api-access-hcl6m
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:07Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:45:12Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:45:12Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://09b9d15d8f17aafc2431a8a48c25f5ade1944d8cdc005342aed204ff51463dac
      image: calico/csi:master
      imageID: docker-pullable://calico/csi@sha256:28fc73661d7c3214f6cd345ab613585c430a2258328dd59d9f0a0f7080075ee6
      lastState: {}
      name: calico-csi
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:45:06Z"
    - containerID: docker://90eeb3e42b3d390097afaf2a2305ca827ab8c1091d8a237d79594d6863b5964c
      image: calico/node-driver-registrar:master
      imageID: docker-pullable://calico/node-driver-registrar@sha256:228f31848eaee8a7b2bc92cea1997f955c1b425620a84c0a24c24d1b8ed0307f
      lastState: {}
      name: csi-node-driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:45:11Z"
    hostIP: 10.2.1.183
    phase: Running
    podIP: 10.42.226.64
    podIPs:
    - ip: 10.42.226.64
    qosClass: BestEffort
    startTime: "2024-03-13T16:12:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 7e1cc353c9c7d17fd591424b2bd435309c7609953d7edd0f9f56c795ddc34252
      cni.projectcalico.org/podIP: 10.42.205.192/32
      cni.projectcalico.org/podIPs: 10.42.205.192/32
    creationTimestamp: "2024-03-13T16:12:07Z"
    generateName: csi-node-driver-
    labels:
      app.kubernetes.io/name: csi-node-driver
      controller-revision-hash: 64874c855f
      k8s-app: csi-node-driver
      name: csi-node-driver
      pod-template-generation: "1"
    name: csi-node-driver-th9ff
    namespace: calico-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-node-driver
      uid: fdae6af8-076d-4c08-9d47-2295f1381898
    resourceVersion: "13074323"
    uid: 6b3e229a-20e1-472e-a2f9-5375224f8e5c
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - master-2
    containers:
    - args:
      - --nodeid=$(KUBE_NODE_NAME)
      - --loglevel=$(LOG_LEVEL)
      env:
      - name: LOG_LEVEL
        value: warn
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/calico/csi:master
      imagePullPolicy: IfNotPresent
      name: calico-csi
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run
        name: varrun
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/lib/kubelet
        mountPropagation: Bidirectional
        name: kubelet-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qj92l
        readOnly: true
    - args:
      - --v=5
      - --csi-address=$(ADDRESS)
      - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)
      env:
      - name: ADDRESS
        value: /csi/csi.sock
      - name: DRIVER_REG_SOCK_PATH
        value: /var/lib/kubelet/plugins/csi.tigera.io/csi.sock
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/calico/node-driver-registrar:master
      imagePullPolicy: IfNotPresent
      name: csi-node-driver-registrar
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        privileged: true
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qj92l
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: master-2
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/run
        type: ""
      name: varrun
    - hostPath:
        path: /var/lib/kubelet
        type: Directory
      name: kubelet-dir
    - hostPath:
        path: /var/lib/kubelet/plugins/csi.tigera.io
        type: DirectoryOrCreate
      name: socket-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry
        type: Directory
      name: registration-dir
    - name: kube-api-access-qj92l
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:07Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:16:14Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:16:14Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:12:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://e97e0f0496d7704daf9d206405c45ad1d7f31cdf9fc1d85523cfe056a779b1f1
      image: calico/csi:master
      imageID: docker-pullable://calico/csi@sha256:28fc73661d7c3214f6cd345ab613585c430a2258328dd59d9f0a0f7080075ee6
      lastState: {}
      name: calico-csi
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:16:06Z"
    - containerID: docker://1d631bf40ebcd48c89c53f3f30f3a50bac97336bacc8f6428eefa5b5a126b479
      image: calico/node-driver-registrar:master
      imageID: docker-pullable://calico/node-driver-registrar@sha256:228f31848eaee8a7b2bc92cea1997f955c1b425620a84c0a24c24d1b8ed0307f
      lastState: {}
      name: csi-node-driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:16:14Z"
    hostIP: 10.2.1.181
    phase: Running
    podIP: 10.42.205.192
    podIPs:
    - ip: 10.42.205.192
    qosClass: BestEffort
    startTime: "2024-03-13T16:12:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: e9d8f566983576eda70164bd2c5fb6d2a343c7687363661bffcd6d087b84795a
      cni.projectcalico.org/podIP: 10.42.97.198/32
      cni.projectcalico.org/podIPs: 10.42.97.198/32
    creationTimestamp: "2024-03-13T16:25:21Z"
    labels:
      run: mytestpod
    name: mytestpod
    namespace: default
    resourceVersion: "16412396"
    uid: 393e1815-9036-4479-9fea-a31a79dcf534
  spec:
    containers:
    - args:
      - sleep
      - "3000"
      image: ubuntu
      imagePullPolicy: Always
      name: mytestpod
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pksxs
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker-3
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-pksxs
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:25:21Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-25T11:58:10Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-25T11:58:10Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:25:21Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://4b76cd678cd9c46ae1229a05e5559de6cee60ed2e4231d4b6f5259eadddc407c
      image: ubuntu:latest
      imageID: docker-pullable://ubuntu@sha256:77906da86b60585ce12215807090eb327e7386c8fafb5402369e421f44eff17e
      lastState:
        terminated:
          containerID: docker://5ee60911a3d9a23ac7d54387908625b68ba06fb393a7386f8b630cc7901712d1
          exitCode: 0
          finishedAt: "2024-03-25T11:58:07Z"
          reason: Completed
          startedAt: "2024-03-25T11:08:07Z"
      name: mytestpod
      ready: true
      restartCount: 340
      started: true
      state:
        running:
          startedAt: "2024-03-25T11:58:09Z"
    hostIP: 10.2.1.185
    phase: Running
    podIP: 10.42.97.198
    podIPs:
    - ip: 10.42.97.198
    qosClass: BestEffort
    startTime: "2024-03-13T16:25:21Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 3cbc2d1bf95e316bf310cd0bbb573860810b372141ded6945f4bf7ca85493d61
      cni.projectcalico.org/podIP: 10.42.38.68/32
      cni.projectcalico.org/podIPs: 10.42.38.68/32
    creationTimestamp: "2024-03-13T16:29:32Z"
    labels:
      run: net-debug
    name: net-debug
    namespace: default
    resourceVersion: "13078041"
    uid: fe6b6f8e-42cd-4339-a7c2-b5605c52d0e9
  spec:
    containers:
    - image: nicolaka/netshoot
      imagePullPolicy: Always
      name: net-debug
      resources: {}
      stdin: true
      stdinOnce: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4lwvh
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker-4
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-4lwvh
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:29:32Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:30:44Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:30:44Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:29:32Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://12f38338a73b3e2849420bc4f2893a7dff28600566fa3dbe90d7f7431fde7271
      image: nicolaka/netshoot:latest
      imageID: docker-pullable://nicolaka/netshoot@sha256:b569665f0c32391b93f4de344f07bf6353ddff9d8c801ac3318d996db848a64c
      lastState:
        terminated:
          containerID: docker://90d47c901465c0c82d76d8f8af5536e080b8d7c1fbbed16a7ba3eefd9614f703
          exitCode: 1
          finishedAt: "2024-03-13T16:30:41Z"
          reason: Error
          startedAt: "2024-03-13T16:29:34Z"
      name: net-debug
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:30:43Z"
    hostIP: 10.2.1.186
    phase: Running
    podIP: 10.42.38.68
    podIPs:
    - ip: 10.42.38.68
    qosClass: BestEffort
    startTime: "2024-03-13T16:29:32Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: ba1e344728ab177f0b33302e9ce5f3b0200faf7cd19a2d25a13c5cb1517cb757
      cni.projectcalico.org/podIP: 10.42.38.74/32
      cni.projectcalico.org/podIPs: 10.42.38.74/32
    creationTimestamp: "2024-03-13T16:49:42Z"
    generateName: ingress-nginx-controller-699bbd7596-
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.9.6
      helm.sh/chart: ingress-nginx-4.9.1
      pod-template-hash: 699bbd7596
    name: ingress-nginx-controller-699bbd7596-zhjrj
    namespace: ingress-nginx
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: ingress-nginx-controller-699bbd7596
      uid: 76e4dee8-22d9-4298-99f4-b5a14395636f
    resourceVersion: "13085769"
    uid: e253df7c-95ec-495e-a303-c1c857426a5a
  spec:
    containers:
    - args:
      - /nginx-ingress-controller
      - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
      - --election-id=ingress-nginx-leader
      - --controller-class=k8s.io/ingress-nginx
      - --ingress-class=nginx
      - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
      - --validating-webhook=:8443
      - --validating-webhook-certificate=/usr/local/certificates/cert
      - --validating-webhook-key=/usr/local/certificates/key
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: LD_PRELOAD
        value: /usr/local/lib/libmimalloc.so
      image: registry.k8s.io/ingress-nginx/controller:v1.9.6@sha256:1405cc613bd95b2c6edd8b2a152510ae91c7e62aea4698500d23b2145960ab9c
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /wait-shutdown
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: controller
      ports:
      - containerPort: 80
        name: http
        protocol: TCP
      - containerPort: 443
        name: https
        protocol: TCP
      - containerPort: 8443
        name: webhook
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 100m
          memory: 90Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsNonRoot: true
        runAsUser: 101
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /usr/local/certificates/
        name: webhook-cert
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kssfr
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker-4
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: ingress-nginx
    serviceAccountName: ingress-nginx
    terminationGracePeriodSeconds: 300
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: webhook-cert
      secret:
        defaultMode: 420
        secretName: ingress-nginx-admission
    - name: kube-api-access-kssfr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:46Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:39Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:39Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:46Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://ca7b432297dc4a0a730dcbbf32734df79c129fcee7472ef3695c5b836c6eb78b
      image: registry.k8s.io/ingress-nginx/controller@sha256:1405cc613bd95b2c6edd8b2a152510ae91c7e62aea4698500d23b2145960ab9c
      imageID: docker-pullable://registry.k8s.io/ingress-nginx/controller@sha256:1405cc613bd95b2c6edd8b2a152510ae91c7e62aea4698500d23b2145960ab9c
      lastState: {}
      name: controller
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:50:27Z"
    hostIP: 10.2.1.186
    phase: Running
    podIP: 10.42.38.74
    podIPs:
    - ip: 10.42.38.74
    qosClass: Burstable
    startTime: "2024-03-13T16:49:46Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: d0913efc608f6a811a03f1a8fb18dfa59ce2738c880695a5037e376114bbb78d
      cni.projectcalico.org/podIP: 10.42.38.75/32
      cni.projectcalico.org/podIPs: 10.42.38.75/32
    creationTimestamp: "2024-03-13T16:49:42Z"
    generateName: coredns-autoscaler-5567d8c485-
    labels:
      k8s-app: coredns-autoscaler
      pod-template-hash: 5567d8c485
    name: coredns-autoscaler-5567d8c485-gl4f8
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-autoscaler-5567d8c485
      uid: 4acbbd70-8ca3-4d1a-a033-7a4fa61a2e94
    resourceVersion: "13085733"
    uid: f248fc6b-2f27-49ed-b0a6-dacb9991d251
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: node-role.kubernetes.io/worker
              operator: Exists
    containers:
    - command:
      - /cluster-proportional-autoscaler
      - --namespace=kube-system
      - --configmap=coredns-autoscaler
      - --target=Deployment/coredns
      - --default-params={"linear":{"coresPerReplica":128,"nodesPerReplica":4,"min":1,"preventSinglePointFailure":true}}
      - --nodelabels=node-role.kubernetes.io/worker=true,beta.kubernetes.io/os=linux
      - --logtostderr=true
      - --v=2
      image: rancher/mirrored-cluster-proportional-autoscaler:1.8.6
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthz
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: autoscaler
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8080
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 20m
          memory: 10Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4kdw5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker-4
    nodeSelector:
      beta.kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns-autoscaler
    serviceAccountName: coredns-autoscaler
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    volumes:
    - name: kube-api-access-4kdw5
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:47Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:35Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:35Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:47Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://3926227c0b34be8fd54ca7d803c50bdae4a9bdab805c892f554ec13be161fb73
      image: rancher/mirrored-cluster-proportional-autoscaler:1.8.6
      imageID: docker-pullable://rancher/mirrored-cluster-proportional-autoscaler@sha256:d9333aded9b1a0526a0c756f9c72037abd742a08ed099575588175a8d9e29cee
      lastState: {}
      name: autoscaler
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:50:34Z"
    hostIP: 10.2.1.186
    phase: Running
    podIP: 10.42.38.75
    podIPs:
    - ip: 10.42.38.75
    qosClass: Burstable
    startTime: "2024-03-13T16:49:47Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 5fabbd92544f18275b350d1626047cb6584291ed661ed12644255ba2af9bfb9f
      cni.projectcalico.org/podIP: 10.42.97.202/32
      cni.projectcalico.org/podIPs: 10.42.97.202/32
      kubectl.kubernetes.io/restartedAt: "2024-03-13T10:26:22+01:00"
      seccomp.security.alpha.kubernetes.io/pod: docker/default
    creationTimestamp: "2024-03-13T16:32:16Z"
    generateName: coredns-b9597578-
    labels:
      k8s-app: kube-dns
      pod-template-hash: b9597578
    name: coredns-b9597578-d6gcx
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-b9597578
      uid: d36b48bb-6047-405f-9897-bae16c4dc40b
    resourceVersion: "13078553"
    uid: 7944c702-3236-4131-a8d2-96c50b761c87
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: node-role.kubernetes.io/worker
              operator: Exists
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - kube-dns
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: rancher/mirrored-coredns-coredns:1.9.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bftqs
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: worker-3
    nodeSelector:
      beta.kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: coredns
      name: config-volume
    - name: kube-api-access-bftqs
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:32:16Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:32:18Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:32:18Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:32:16Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://486a4e30ad26b8174192a0833f4050050e01bacb746a15b73cdcea34b66fddc3
      image: rancher/mirrored-coredns-coredns:1.9.4
      imageID: docker-pullable://rancher/mirrored-coredns-coredns@sha256:823626055cba80e2ad6ff26e18df206c7f26964c7cd81a8ef57b4dc16c0eec61
      lastState: {}
      name: coredns
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:32:17Z"
    hostIP: 10.2.1.185
    phase: Running
    podIP: 10.42.97.202
    podIPs:
    - ip: 10.42.97.202
    qosClass: Burstable
    startTime: "2024-03-13T16:32:16Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 37f72f766b3e71f29c5b0d5eabdf0dc4087aa1d487be7989de1965e30d80f973
      cni.projectcalico.org/podIP: 10.42.133.200/32
      cni.projectcalico.org/podIPs: 10.42.133.200/32
      kubectl.kubernetes.io/restartedAt: "2024-03-13T10:26:22+01:00"
      seccomp.security.alpha.kubernetes.io/pod: docker/default
    creationTimestamp: "2024-03-13T16:31:16Z"
    generateName: coredns-b9597578-
    labels:
      k8s-app: kube-dns
      pod-template-hash: b9597578
    name: coredns-b9597578-tqbw8
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-b9597578
      uid: d36b48bb-6047-405f-9897-bae16c4dc40b
    resourceVersion: "13078328"
    uid: 3cd03bc7-8e6e-4084-8254-5d1816d5b72d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: node-role.kubernetes.io/worker
              operator: Exists
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - kube-dns
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: rancher/mirrored-coredns-coredns:1.9.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-svbzl
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: worker-2
    nodeSelector:
      beta.kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: coredns
      name: config-volume
    - name: kube-api-access-svbzl
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:31:16Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:31:26Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:31:26Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:31:16Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://d9deb79d83292cf3dc86375d8eed1f349eb91f464ca8adddc6d4203b1a7c5bb5
      image: rancher/mirrored-coredns-coredns:1.9.4
      imageID: docker-pullable://rancher/mirrored-coredns-coredns@sha256:823626055cba80e2ad6ff26e18df206c7f26964c7cd81a8ef57b4dc16c0eec61
      lastState: {}
      name: coredns
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:31:26Z"
    hostIP: 10.2.1.184
    phase: Running
    podIP: 10.42.133.200
    podIPs:
    - ip: 10.42.133.200
    qosClass: Burstable
    startTime: "2024-03-13T16:31:16Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 06bcf2edcd8d817a8844c5169fd7ac9d73061550353190b44f3b96b23a594816
      cni.projectcalico.org/podIP: 10.42.97.201/32
      cni.projectcalico.org/podIPs: 10.42.97.201/32
    creationTimestamp: "2024-03-13T16:31:16Z"
    generateName: metrics-server-7886b5f87c-
    labels:
      k8s-app: metrics-server
      pod-template-hash: 7886b5f87c
    name: metrics-server-7886b5f87c-75n4w
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: metrics-server-7886b5f87c
      uid: f17d2efc-8224-4688-afac-06810a75d711
    resourceVersion: "13078451"
    uid: 04b83756-442b-4d8a-986d-8ad09c5136e8
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: beta.kubernetes.io/os
              operator: NotIn
              values:
              - windows
            - key: node-role.kubernetes.io/worker
              operator: Exists
    containers:
    - args:
      - --cert-dir=/tmp
      - --secure-port=4443
      - --kubelet-insecure-tls
      - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      - --metric-resolution=15s
      image: rancher/mirrored-metrics-server:v0.6.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: https
          scheme: HTTPS
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: metrics-server
      ports:
      - containerPort: 4443
        name: https
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: https
          scheme: HTTPS
        initialDelaySeconds: 20
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
      securityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 1000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wp9jt
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker-3
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: metrics-server
    serviceAccountName: metrics-server
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    volumes:
    - emptyDir: {}
      name: tmp-dir
    - name: kube-api-access-wp9jt
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:31:16Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:31:56Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:31:56Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:31:16Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://49878baff3c165c22eb23d6590b94bf85d5256110b1c503d8fba336eb96b382f
      image: rancher/mirrored-metrics-server:v0.6.3
      imageID: docker-pullable://rancher/mirrored-metrics-server@sha256:c2dfd72bafd6406ed306d9fbd07f55c496b004293d13d3de88a4567eacc36558
      lastState: {}
      name: metrics-server
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:31:36Z"
    hostIP: 10.2.1.185
    phase: Running
    podIP: 10.42.97.201
    podIPs:
    - ip: 10.42.97.201
    qosClass: Burstable
    startTime: "2024-03-13T16:31:16Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 0d5a0ca7f610b59fcad74d660c7b206901869054fac81aff51da5b5c6935e44f
      cni.projectcalico.org/podIP: 10.42.97.212/32
      cni.projectcalico.org/podIPs: 10.42.97.212/32
      kubectl.kubernetes.io/default-container: alertmanager
    creationTimestamp: "2024-03-13T16:44:12Z"
    generateName: alertmanager-kube-prometheus-stack-alertmanager-
    labels:
      alertmanager: kube-prometheus-stack-alertmanager
      app.kubernetes.io/instance: kube-prometheus-stack-alertmanager
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/version: 0.26.0
      controller-revision-hash: alertmanager-kube-prometheus-stack-alertmanager-6c5bff765
      statefulset.kubernetes.io/pod-name: alertmanager-kube-prometheus-stack-alertmanager-0
    name: alertmanager-kube-prometheus-stack-alertmanager-0
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: alertmanager-kube-prometheus-stack-alertmanager
      uid: 516570fd-990e-48d4-9110-8801a42180e0
    resourceVersion: "13086219"
    uid: bf7ce886-ce25-4b1b-881c-e029063a6092
  spec:
    containers:
    - args:
      - --config.file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --storage.path=/alertmanager
      - --data.retention=120h
      - --cluster.listen-address=
      - --web.listen-address=:9093
      - --web.external-url=http://kube-prometheus-stack-alertmanager.monitoring:9093
      - --web.route-prefix=/alertmanager
      - --cluster.label=monitoring/kube-prometheus-stack-alertmanager
      - --cluster.peer=alertmanager-kube-prometheus-stack-alertmanager-0.alertmanager-operated:9094
      - --cluster.reconnect-timeout=5m
      - --web.config.file=/etc/alertmanager/web_config/web-config.yaml
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/prometheus/alertmanager:v0.26.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /alertmanager/-/healthy
          port: http-web
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 3
      name: alertmanager
      ports:
      - containerPort: 9093
        name: http-web
        protocol: TCP
      - containerPort: 9094
        name: mesh-tcp
        protocol: TCP
      - containerPort: 9094
        name: mesh-udp
        protocol: UDP
      readinessProbe:
        failureThreshold: 10
        httpGet:
          path: /alertmanager/-/ready
          port: http-web
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        requests:
          memory: 200Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
      - mountPath: /etc/alertmanager/config_out
        name: config-out
        readOnly: true
      - mountPath: /etc/alertmanager/certs
        name: tls-assets
        readOnly: true
      - mountPath: /alertmanager
        name: alertmanager-kube-prometheus-stack-alertmanager-db
        subPath: alertmanager-db
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4hcfv
        readOnly: true
    - args:
      - --listen-address=:8080
      - --reload-url=http://127.0.0.1:9093/alertmanager/-/reload
      - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
      - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --watched-dir=/etc/alertmanager/config
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "-1"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
      imagePullPolicy: IfNotPresent
      name: config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
      - mountPath: /etc/alertmanager/config_out
        name: config-out
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4hcfv
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: alertmanager-kube-prometheus-stack-alertmanager-0
    initContainers:
    - args:
      - --watch-interval=0
      - --listen-address=:8080
      - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
      - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --watched-dir=/etc/alertmanager/config
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "-1"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
      imagePullPolicy: IfNotPresent
      name: init-config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
      - mountPath: /etc/alertmanager/config_out
        name: config-out
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4hcfv
        readOnly: true
    nodeName: worker-3
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: kube-prometheus-stack-alertmanager
    serviceAccountName: kube-prometheus-stack-alertmanager
    subdomain: alertmanager-operated
    terminationGracePeriodSeconds: 120
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: alertmanager-kube-prometheus-stack-alertmanager-db
      persistentVolumeClaim:
        claimName: alertmanager-kube-prometheus-stack-alertmanager-db-alertmanager-kube-prometheus-stack-alertmanager-0
    - name: config-volume
      secret:
        defaultMode: 420
        secretName: alertmanager-kube-prometheus-stack-alertmanager-generated
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: alertmanager-kube-prometheus-stack-alertmanager-tls-assets-0
    - emptyDir:
        medium: Memory
      name: config-out
    - name: web-config
      secret:
        defaultMode: 420
        secretName: alertmanager-kube-prometheus-stack-alertmanager-web-config
    - name: kube-api-access-4hcfv
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:51:08Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:51:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:51:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:44:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://fe40b973416c4c153b79189387b245c0efe3760ce503a405ce65f49a1d4692c4
      image: quay.io/prometheus/alertmanager:v0.26.0
      imageID: docker-pullable://quay.io/prometheus/alertmanager@sha256:361db356b33041437517f1cd298462055580585f26555c317df1a3caf2868552
      lastState: {}
      name: alertmanager
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:51:19Z"
    - containerID: docker://9994e4cbce3c15908eeca63e084cba238d1ec96fe7b38aa82d2315ed364ce397
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
      imageID: docker-pullable://quay.io/prometheus-operator/prometheus-config-reloader@sha256:e20576b76ffd85d2a28d62809092f47b339737320e80646ec6d0e7ac0f4c8e43
      lastState: {}
      name: config-reloader
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:51:20Z"
    hostIP: 10.2.1.185
    initContainerStatuses:
    - containerID: docker://4196b741da46c6b6ed8ff89b6ec8676ebb26b92a49ee6689da89829c0cabac23
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
      imageID: docker-pullable://quay.io/prometheus-operator/prometheus-config-reloader@sha256:e20576b76ffd85d2a28d62809092f47b339737320e80646ec6d0e7ac0f4c8e43
      lastState: {}
      name: init-config-reloader
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://4196b741da46c6b6ed8ff89b6ec8676ebb26b92a49ee6689da89829c0cabac23
          exitCode: 0
          finishedAt: "2024-03-13T16:51:08Z"
          reason: Completed
          startedAt: "2024-03-13T16:51:08Z"
    phase: Running
    podIP: 10.42.97.212
    podIPs:
    - ip: 10.42.97.212
    qosClass: Burstable
    startTime: "2024-03-13T16:44:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: c8e876853580e8b0d807a852e9813e06a752130be7ce716cc267fb1881ca7e39
      checksum/sc-dashboard-provider-config: 593c0a8778b83f11fe80ccb21dfb20bc46705e2be3178df1dc4c89d164c8cd9c
      checksum/secret: 52393145826148a5c4d0489c34fdfe1036fe8275afda36f83c7eee420dd9eb7d
      cni.projectcalico.org/containerID: baac763c1bf33b66686e92c50908ae679275eccb3791af43c45188192016c803
      cni.projectcalico.org/podIP: 10.42.133.216/32
      cni.projectcalico.org/podIPs: 10.42.133.216/32
      kubectl.kubernetes.io/default-container: grafana
    creationTimestamp: "2024-03-13T16:52:04Z"
    generateName: kube-prometheus-stack-grafana-7c77d9b866-
    labels:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/name: grafana
      pod-template-hash: 7c77d9b866
    name: kube-prometheus-stack-grafana-7c77d9b866-v68qc
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kube-prometheus-stack-grafana-7c77d9b866
      uid: e018a8ec-e321-4c87-b2cc-a12541c3ee1a
    resourceVersion: "13086983"
    uid: 98775e57-c37c-41f9-9a28-343c932e47fd
  spec:
    automountServiceAccountToken: true
    containers:
    - env:
      - name: METHOD
        value: WATCH
      - name: LABEL
        value: grafana_dashboard
      - name: LABEL_VALUE
        value: "1"
      - name: FOLDER
        value: /tmp/dashboards
      - name: RESOURCE
        value: both
      - name: NAMESPACE
        value: ALL
      - name: REQ_USERNAME
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: kube-prometheus-stack-grafana
      - name: REQ_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: kube-prometheus-stack-grafana
      - name: REQ_URL
        value: http://localhost:3000/api/admin/provisioning/dashboards/reload
      - name: REQ_METHOD
        value: POST
      image: quay.io/kiwigrid/k8s-sidecar:1.25.2
      imagePullPolicy: IfNotPresent
      name: grafana-sc-dashboard
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp/dashboards
        name: sc-dashboard-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7svqr
        readOnly: true
    - env:
      - name: METHOD
        value: WATCH
      - name: LABEL
        value: grafana_datasource
      - name: LABEL_VALUE
        value: "1"
      - name: FOLDER
        value: /etc/grafana/provisioning/datasources
      - name: RESOURCE
        value: both
      - name: REQ_USERNAME
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: kube-prometheus-stack-grafana
      - name: REQ_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: kube-prometheus-stack-grafana
      - name: REQ_URL
        value: http://localhost:3000/api/admin/provisioning/datasources/reload
      - name: REQ_METHOD
        value: POST
      image: quay.io/kiwigrid/k8s-sidecar:1.25.2
      imagePullPolicy: IfNotPresent
      name: grafana-sc-datasources
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7svqr
        readOnly: true
    - env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: GF_SECURITY_ADMIN_USER
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: kube-prometheus-stack-grafana
      - name: GF_SECURITY_ADMIN_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: kube-prometheus-stack-grafana
      - name: GF_PATHS_DATA
        value: /var/lib/grafana/
      - name: GF_PATHS_LOGS
        value: /var/log/grafana
      - name: GF_PATHS_PLUGINS
        value: /var/lib/grafana/plugins
      - name: GF_PATHS_PROVISIONING
        value: /etc/grafana/provisioning
      image: docker.io/grafana/grafana:10.2.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /api/health
          port: 3000
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      name: grafana
      ports:
      - containerPort: 3000
        name: grafana
        protocol: TCP
      - containerPort: 9094
        name: gossip-tcp
        protocol: TCP
      - containerPort: 9094
        name: gossip-udp
        protocol: UDP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /api/health
          port: 3000
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/grafana/grafana.ini
        name: config
        subPath: grafana.ini
      - mountPath: /var/lib/grafana
        name: storage
      - mountPath: /tmp/dashboards
        name: sc-dashboard-volume
      - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
        name: sc-dashboard-provider
        subPath: provider.yaml
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7svqr
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - chown
      - -R
      - 472:472
      - /var/lib/grafana
      image: docker.io/library/busybox:1.31.1
      imagePullPolicy: IfNotPresent
      name: init-chown-data
      resources: {}
      securityContext:
        capabilities:
          add:
          - CHOWN
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/grafana
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7svqr
        readOnly: true
    nodeName: worker-2
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 472
      runAsGroup: 472
      runAsNonRoot: true
      runAsUser: 472
    serviceAccount: kube-prometheus-stack-grafana
    serviceAccountName: kube-prometheus-stack-grafana
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-prometheus-stack-grafana
      name: config
    - name: storage
      persistentVolumeClaim:
        claimName: kube-prometheus-stack-grafana
    - emptyDir: {}
      name: sc-dashboard-volume
    - configMap:
        defaultMode: 420
        name: kube-prometheus-stack-grafana-config-dashboards
      name: sc-dashboard-provider
    - emptyDir: {}
      name: sc-datasources-volume
    - name: kube-api-access-7svqr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:52:38Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:52:56Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:52:56Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:52:04Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://81e9a5731e62f76eb6fa8c9b29afd9597bbaec7a09d25937e239d400a3dd1c96
      image: grafana/grafana:10.2.2
      imageID: docker-pullable://grafana/grafana@sha256:e3e9c2b5776fe3657f4954dfa91579224f98a0316f51d431989b15425e95530f
      lastState: {}
      name: grafana
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:52:40Z"
    - containerID: docker://77c2e6ac86df90b6c75fcbe19c4c4a80526d396752a8258d74c3d6ca4d82ea62
      image: quay.io/kiwigrid/k8s-sidecar:1.25.2
      imageID: docker-pullable://quay.io/kiwigrid/k8s-sidecar@sha256:cb4c638ffb1fa1eb49678e0f0423564b39254533f63f4ca6a6c24260472e0c4f
      lastState: {}
      name: grafana-sc-dashboard
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:52:38Z"
    - containerID: docker://d2e138495591ae287fff5d83afc1b2d5f2b05cf47ef75b1533d4205c88c5967b
      image: quay.io/kiwigrid/k8s-sidecar:1.25.2
      imageID: docker-pullable://quay.io/kiwigrid/k8s-sidecar@sha256:cb4c638ffb1fa1eb49678e0f0423564b39254533f63f4ca6a6c24260472e0c4f
      lastState: {}
      name: grafana-sc-datasources
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:52:39Z"
    hostIP: 10.2.1.184
    initContainerStatuses:
    - containerID: docker://f8d2864414483b9423e8cbbf77e3708ceb08134f648f13873f411d807fe9751c
      image: busybox:1.31.1
      imageID: docker-pullable://busybox@sha256:95cf004f559831017cdf4628aaf1bb30133677be8702a8c5f2994629f637a209
      lastState: {}
      name: init-chown-data
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://f8d2864414483b9423e8cbbf77e3708ceb08134f648f13873f411d807fe9751c
          exitCode: 0
          finishedAt: "2024-03-13T16:52:37Z"
          reason: Completed
          startedAt: "2024-03-13T16:52:37Z"
    phase: Running
    podIP: 10.42.133.216
    podIPs:
    - ip: 10.42.133.216
    qosClass: BestEffort
    startTime: "2024-03-13T16:52:04Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 123581fefab8c318c890c4ed30728e6bce0de49b9654bcc87bc9f2e8dddff355
      cni.projectcalico.org/podIP: 10.42.97.195/32
      cni.projectcalico.org/podIPs: 10.42.97.195/32
    creationTimestamp: "2024-03-13T16:18:30Z"
    generateName: kube-prometheus-stack-kube-state-metrics-d68548445-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.10.1
      helm.sh/chart: kube-state-metrics-5.15.2
      pod-template-hash: d68548445
      release: kube-prometheus-stack
    name: kube-prometheus-stack-kube-state-metrics-d68548445-d5vrs
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kube-prometheus-stack-kube-state-metrics-d68548445
      uid: 9cd8bc32-21a8-4368-be2e-5275564fa971
    resourceVersion: "13075250"
    uid: 3cc4f744-6dde-4004-af16-a03662cbc5cf
  spec:
    containers:
    - args:
      - --port=8080
      - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.10.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kube-state-metrics
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wz55b
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker-3
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: kube-prometheus-stack-kube-state-metrics
    serviceAccountName: kube-prometheus-stack-kube-state-metrics
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-wz55b
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:18:30Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:18:40Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:18:40Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:18:30Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://71b616f84ac513a657b8b1b1ca36515ec3607c82aa8ddf61977fcbfc4bb1ba2f
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.10.1
      imageID: docker-pullable://registry.k8s.io/kube-state-metrics/kube-state-metrics@sha256:af8220f534938de121a694cb7314313a6195c9d494fc30bfa6885b08a276bb82
      lastState: {}
      name: kube-state-metrics
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:18:31Z"
    hostIP: 10.2.1.185
    phase: Running
    podIP: 10.42.97.195
    podIPs:
    - ip: 10.42.97.195
    qosClass: BestEffort
    startTime: "2024-03-13T16:18:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 1523518d7a5236511410a10709df8f475138c5ad4d31786a7ce2f94da0f7742a
      cni.projectcalico.org/podIP: 10.42.97.194/32
      cni.projectcalico.org/podIPs: 10.42.97.194/32
    creationTimestamp: "2024-03-13T16:18:29Z"
    generateName: kube-prometheus-stack-operator-79b4dd5d7c-
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 55.5.1
      chart: kube-prometheus-stack-55.5.1
      heritage: Helm
      pod-template-hash: 79b4dd5d7c
      release: kube-prometheus-stack
    name: kube-prometheus-stack-operator-79b4dd5d7c-qzhkc
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kube-prometheus-stack-operator-79b4dd5d7c
      uid: 25988fa9-c9b4-4fe1-b5a7-157582b85d80
    resourceVersion: "13075174"
    uid: cef834ce-afe6-432f-804c-1b20ae39012a
  spec:
    containers:
    - args:
      - --kubelet-service=kube-system/kube-prometheus-stack-kubelet
      - --localhost=127.0.0.1
      - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
      - --config-reloader-cpu-request=0
      - --config-reloader-cpu-limit=0
      - --config-reloader-memory-request=0
      - --config-reloader-memory-limit=0
      - --thanos-default-base-image=quay.io/thanos/thanos:v0.32.5
      - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
      - --web.enable-tls=true
      - --web.cert-file=/cert/cert
      - --web.key-file=/cert/key
      - --web.listen-address=:10250
      - --web.tls-min-version=VersionTLS13
      env:
      - name: GOGC
        value: "30"
      image: quay.io/prometheus-operator/prometheus-operator:v0.70.0
      imagePullPolicy: IfNotPresent
      name: kube-prometheus-stack
      ports:
      - containerPort: 10250
        name: https
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /cert
        name: tls-secret
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xdnpl
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker-3
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: kube-prometheus-stack-operator
    serviceAccountName: kube-prometheus-stack-operator
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: tls-secret
      secret:
        defaultMode: 420
        secretName: kube-prometheus-stack-admission
    - name: kube-api-access-xdnpl
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:18:29Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:18:31Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:18:31Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:18:29Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://550cf5ddd035ba7c6913786289450d1d8285006839ba1d2612acb9bda4d3b922
      image: quay.io/prometheus-operator/prometheus-operator:v0.70.0
      imageID: docker-pullable://quay.io/prometheus-operator/prometheus-operator@sha256:5ce078d4cd5d0b39bbb2d323a7902eb05680276e25a041115db9128f61b451c8
      lastState: {}
      name: kube-prometheus-stack
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:18:30Z"
    hostIP: 10.2.1.185
    phase: Running
    podIP: 10.42.97.194
    podIPs:
    - ip: 10.42.97.194
    qosClass: BestEffort
    startTime: "2024-03-13T16:18:29Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2024-02-02T10:31:12Z"
    generateName: kube-prometheus-stack-prometheus-node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.7.0
      controller-revision-hash: d4bc4ccc4
      helm.sh/chart: prometheus-node-exporter-4.24.0
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: kube-prometheus-stack
    name: kube-prometheus-stack-prometheus-node-exporter-5lbjp
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-prometheus-stack-prometheus-node-exporter
      uid: 437693ec-8560-429d-9ac6-3e2d4f8b1c2b
    resourceVersion: "13001797"
    uid: c01af2ec-354d-4a71-92ef-45a1c5081d48
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - worker-4
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9100
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.7.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: worker-4
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: kube-prometheus-stack-prometheus-node-exporter
    serviceAccountName: kube-prometheus-stack-prometheus-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-02-02T10:31:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T10:38:40Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T10:38:40Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-02-02T10:31:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://8b8dffabc585b3117e63119dacbe338a26f2de2fc546a2bbbc4ee7ba5977ffec
      image: quay.io/prometheus/node-exporter:v1.7.0
      imageID: docker-pullable://quay.io/prometheus/node-exporter@sha256:4cb2b9019f1757be8482419002cb7afe028fdba35d47958829e4cfeaf6246d80
      lastState:
        terminated:
          containerID: docker://bb8e0cb63fa79fea61ccd3e49d3987667aaa785c04e549112df8d743b5f8848c
          exitCode: 255
          finishedAt: "2024-03-13T10:38:11Z"
          reason: Error
          startedAt: "2024-02-28T07:51:24Z"
      name: node-exporter
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2024-03-13T10:38:36Z"
    hostIP: 10.2.1.186
    phase: Running
    podIP: 10.2.1.186
    podIPs:
    - ip: 10.2.1.186
    qosClass: BestEffort
    startTime: "2024-02-02T10:31:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2024-02-02T10:31:12Z"
    generateName: kube-prometheus-stack-prometheus-node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.7.0
      controller-revision-hash: d4bc4ccc4
      helm.sh/chart: prometheus-node-exporter-4.24.0
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: kube-prometheus-stack
    name: kube-prometheus-stack-prometheus-node-exporter-966g8
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-prometheus-stack-prometheus-node-exporter
      uid: 437693ec-8560-429d-9ac6-3e2d4f8b1c2b
    resourceVersion: "12997153"
    uid: 375c174c-06bd-4d7d-b873-8d903446a8a1
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - worker-3
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9100
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.7.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: worker-3
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: kube-prometheus-stack-prometheus-node-exporter
    serviceAccountName: kube-prometheus-stack-prometheus-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-02-02T10:31:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T10:21:40Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T10:21:40Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-02-02T10:31:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://9e8f1abd04656ac9a4ba8c7ddaef277348e1a9215a01ba344120add00b17ec77
      image: quay.io/prometheus/node-exporter:v1.7.0
      imageID: docker-pullable://quay.io/prometheus/node-exporter@sha256:4cb2b9019f1757be8482419002cb7afe028fdba35d47958829e4cfeaf6246d80
      lastState:
        terminated:
          containerID: docker://869a14db4f5db35d8c120e342d72157d288c58d6b6e5933205b88a00b1e04998
          exitCode: 143
          finishedAt: "2024-03-13T10:11:09Z"
          reason: Error
          startedAt: "2024-02-28T07:24:56Z"
      name: node-exporter
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2024-03-13T10:21:34Z"
    hostIP: 10.2.1.185
    phase: Running
    podIP: 10.2.1.185
    podIPs:
    - ip: 10.2.1.185
    qosClass: BestEffort
    startTime: "2024-02-02T10:31:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2024-02-02T10:31:12Z"
    generateName: kube-prometheus-stack-prometheus-node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.7.0
      controller-revision-hash: d4bc4ccc4
      helm.sh/chart: prometheus-node-exporter-4.24.0
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: kube-prometheus-stack
    name: kube-prometheus-stack-prometheus-node-exporter-rwqb6
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-prometheus-stack-prometheus-node-exporter
      uid: 437693ec-8560-429d-9ac6-3e2d4f8b1c2b
    resourceVersion: "13056919"
    uid: 14d51699-9479-4cb0-aae6-7093237d33a8
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - worker-2
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9100
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.7.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: worker-2
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: kube-prometheus-stack-prometheus-node-exporter
    serviceAccountName: kube-prometheus-stack-prometheus-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-02-02T10:31:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T15:07:13Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T15:07:13Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-02-02T10:31:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://03bad4c48b2e6dbb728da80077732adeca4df72265566806b90b8227f7a654c7
      image: quay.io/prometheus/node-exporter:v1.7.0
      imageID: docker-pullable://quay.io/prometheus/node-exporter@sha256:4cb2b9019f1757be8482419002cb7afe028fdba35d47958829e4cfeaf6246d80
      lastState:
        terminated:
          containerID: docker://63b30ba271045397e5e04a2b6ee834640f3de73016b1f7194f7697ac3518e976
          exitCode: 143
          finishedAt: "2024-03-13T15:05:57Z"
          reason: Error
          startedAt: "2024-03-13T10:22:16Z"
      name: node-exporter
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2024-03-13T15:07:10Z"
    hostIP: 10.2.1.184
    phase: Running
    podIP: 10.2.1.184
    podIPs:
    - ip: 10.2.1.184
    qosClass: BestEffort
    startTime: "2024-02-02T10:31:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2024-02-02T10:31:12Z"
    generateName: kube-prometheus-stack-prometheus-node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.7.0
      controller-revision-hash: d4bc4ccc4
      helm.sh/chart: prometheus-node-exporter-4.24.0
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: kube-prometheus-stack
    name: kube-prometheus-stack-prometheus-node-exporter-v2fqc
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-prometheus-stack-prometheus-node-exporter
      uid: 437693ec-8560-429d-9ac6-3e2d4f8b1c2b
    resourceVersion: "13081607"
    uid: 555e1ea5-d8fd-424a-9057-a5d647c67626
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - worker-1
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9100
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.7.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: worker-1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: kube-prometheus-stack-prometheus-node-exporter
    serviceAccountName: kube-prometheus-stack-prometheus-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-02-02T10:31:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:43:57Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:43:57Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-02-02T10:31:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://06a13ab9fbe386c3b14f70faf6e971ec42770fbc68e9855ab4ecac2bbe314dc8
      image: quay.io/prometheus/node-exporter:v1.7.0
      imageID: docker-pullable://quay.io/prometheus/node-exporter@sha256:4cb2b9019f1757be8482419002cb7afe028fdba35d47958829e4cfeaf6246d80
      lastState:
        terminated:
          containerID: docker://01475cba909319ef9b46365ec15e70ba883ce3108cf1e901d8d9ea01170fbd53
          exitCode: 143
          finishedAt: "2024-03-13T16:21:23Z"
          reason: Error
          startedAt: "2024-03-13T10:09:57Z"
      name: node-exporter
      ready: true
      restartCount: 3
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:43:56Z"
    hostIP: 10.2.1.183
    phase: Running
    podIP: 10.2.1.183
    podIPs:
    - ip: 10.2.1.183
    qosClass: BestEffort
    startTime: "2024-02-02T10:31:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 3ef1b264287adf70f710d275e5b0d4e9683badf2f8958f2d7e59b805a337f888
      cni.projectcalico.org/podIP: 10.42.133.215/32
      cni.projectcalico.org/podIPs: 10.42.133.215/32
      kubectl.kubernetes.io/default-container: prometheus
    creationTimestamp: "2024-03-13T16:52:05Z"
    generateName: prometheus-kube-prometheus-stack-prometheus-
    labels:
      app.kubernetes.io/instance: kube-prometheus-stack-prometheus
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/version: 2.48.1
      controller-revision-hash: prometheus-kube-prometheus-stack-prometheus-76dc8d49f4
      operator.prometheus.io/name: kube-prometheus-stack-prometheus
      operator.prometheus.io/shard: "0"
      prometheus: kube-prometheus-stack-prometheus
      statefulset.kubernetes.io/pod-name: prometheus-kube-prometheus-stack-prometheus-0
    name: prometheus-kube-prometheus-stack-prometheus-0
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: prometheus-kube-prometheus-stack-prometheus
      uid: 34962cd4-23e7-4568-a91d-2dc616931919
    resourceVersion: "13087064"
    uid: f7290033-3a6c-473b-802f-4c31badb806f
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --web.console.templates=/etc/prometheus/consoles
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --config.file=/etc/prometheus/config_out/prometheus.env.yaml
      - --web.enable-lifecycle
      - --web.external-url=/prometheus
      - --web.route-prefix=/prometheus
      - --storage.tsdb.retention.time=14d
      - --storage.tsdb.path=/prometheus
      - --storage.tsdb.wal-compression
      - --web.config.file=/etc/prometheus/web_config/web-config.yaml
      image: quay.io/prometheus/prometheus:v2.48.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 6
        httpGet:
          path: /prometheus/-/healthy
          port: http-web
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      name: prometheus
      ports:
      - containerPort: 9090
        name: http-web
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /prometheus/-/ready
          port: http-web
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      startupProbe:
        failureThreshold: 60
        httpGet:
          path: /prometheus/-/ready
          port: http-web
          scheme: HTTP
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 3
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config_out
        name: config-out
        readOnly: true
      - mountPath: /etc/prometheus/certs
        name: tls-assets
        readOnly: true
      - mountPath: /prometheus
        name: prometheus-kube-prometheus-stack-prometheus-db
        subPath: prometheus-db
      - mountPath: /etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0
        name: prometheus-kube-prometheus-stack-prometheus-rulefiles-0
      - mountPath: /etc/prometheus/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zj9l4
        readOnly: true
    - args:
      - --listen-address=:8080
      - --reload-url=http://127.0.0.1:9090/prometheus/-/reload
      - --config-file=/etc/prometheus/config/prometheus.yaml.gz
      - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      - --watched-dir=/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
      imagePullPolicy: IfNotPresent
      name: config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0
        name: prometheus-kube-prometheus-stack-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zj9l4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: prometheus-kube-prometheus-stack-prometheus-0
    initContainers:
    - args:
      - --watch-interval=0
      - --listen-address=:8080
      - --config-file=/etc/prometheus/config/prometheus.yaml.gz
      - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      - --watched-dir=/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
      imagePullPolicy: IfNotPresent
      name: init-config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0
        name: prometheus-kube-prometheus-stack-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zj9l4
        readOnly: true
    nodeName: worker-2
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: kube-prometheus-stack-prometheus
    serviceAccountName: kube-prometheus-stack-prometheus
    shareProcessNamespace: false
    subdomain: prometheus-operated
    terminationGracePeriodSeconds: 600
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: prometheus-kube-prometheus-stack-prometheus-db
      persistentVolumeClaim:
        claimName: prometheus-kube-prometheus-stack-prometheus-db-prometheus-kube-prometheus-stack-prometheus-0
    - name: config
      secret:
        defaultMode: 420
        secretName: prometheus-kube-prometheus-stack-prometheus
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: prometheus-kube-prometheus-stack-prometheus-tls-assets-0
    - emptyDir:
        medium: Memory
      name: config-out
    - configMap:
        defaultMode: 420
        name: prometheus-kube-prometheus-stack-prometheus-rulefiles-0
      name: prometheus-kube-prometheus-stack-prometheus-rulefiles-0
    - name: web-config
      secret:
        defaultMode: 420
        secretName: prometheus-kube-prometheus-stack-prometheus-web-config
    - name: kube-api-access-zj9l4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:52:40Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:53:18Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:53:18Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:52:05Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://27a8028ae567fb2684c6d14abcddacec611f3e110aa4dda453f412147e369b25
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
      imageID: docker-pullable://quay.io/prometheus-operator/prometheus-config-reloader@sha256:e20576b76ffd85d2a28d62809092f47b339737320e80646ec6d0e7ac0f4c8e43
      lastState: {}
      name: config-reloader
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:53:03Z"
    - containerID: docker://d9a1c3deab9b8f1fd72d8207876657d2f77381e02c2d93719e888bdc77ed1a09
      image: quay.io/prometheus/prometheus:v2.48.1
      imageID: docker-pullable://quay.io/prometheus/prometheus@sha256:a67e5e402ff5410b86ec48b39eab1a3c4df2a7e78a71bf025ec5e32e09090ad4
      lastState: {}
      name: prometheus
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:53:03Z"
    hostIP: 10.2.1.184
    initContainerStatuses:
    - containerID: docker://0dbb2d29e53b2083808fa6aa1ac546c30e7c45feda2160b3f1249af7db07aa12
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
      imageID: docker-pullable://quay.io/prometheus-operator/prometheus-config-reloader@sha256:e20576b76ffd85d2a28d62809092f47b339737320e80646ec6d0e7ac0f4c8e43
      lastState: {}
      name: init-config-reloader
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://0dbb2d29e53b2083808fa6aa1ac546c30e7c45feda2160b3f1249af7db07aa12
          exitCode: 0
          finishedAt: "2024-03-13T16:52:39Z"
          reason: Completed
          startedAt: "2024-03-13T16:52:39Z"
    phase: Running
    podIP: 10.42.133.215
    podIPs:
    - ip: 10.42.133.215
    qosClass: BestEffort
    startTime: "2024-03-13T16:52:05Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-13T16:49:42Z"
    generateName: csi-cephfsplugin-
    labels:
      app: csi-cephfsplugin
      contains: csi-cephfsplugin-metrics
      controller-revision-hash: 65778df578
      pod-template-generation: "1"
    name: csi-cephfsplugin-7kgmg
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-cephfsplugin
      uid: 5366696a-b570-4ac6-8e32-91d05178bbf4
    resourceVersion: "13085088"
    uid: 162663c4-1473-4111-811e-c11ae4fda3c1
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - worker-2
    containers:
    - args:
      - --v=0
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/csi.sock
      env:
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.1
      imagePullPolicy: IfNotPresent
      name: driver-registrar
      resources:
        limits:
          cpu: 100m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      securityContext:
        capabilities:
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qdp49
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --type=cephfs
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --nodeserver=true
      - --drivername=rook-ceph.cephfs.csi.ceph.com
      - --pidlimit=-1
      - --forcecephkernelclient=true
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: quay.io/cephcsi/cephcsi:v3.10.1
      imagePullPolicy: IfNotPresent
      name: csi-cephfsplugin
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /csi/mountinfo
        name: ceph-csi-mountinfo
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: Bidirectional
        name: csi-plugins-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /dev
        name: host-dev
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qdp49
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    nodeName: worker-2
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-cephfs-plugin-sa
    serviceAccountName: rook-csi-cephfs-plugin-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/mountinfo
        type: DirectoryOrCreate
      name: ceph-csi-mountinfo
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: Directory
      name: csi-plugins-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: Directory
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - configMap:
        defaultMode: 420
        items:
        - key: csi-cluster-config-json
          path: config.json
        name: rook-ceph-csi-config
      name: ceph-csi-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - hostPath:
        path: /run/mount
        type: ""
      name: host-run-mount
    - name: kube-api-access-qdp49
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:47Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:49Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:49Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:47Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://349035cc2d01dcbc95b36c36fb4c34cc0db142554f9d62e7730a00059eef0fa6
      image: quay.io/cephcsi/cephcsi:v3.10.1
      imageID: docker-pullable://quay.io/cephcsi/cephcsi@sha256:5dd50ad6f3f9a1e8c8186fde0048ea241f056ca755acbeab42f5ebf723313e9c
      lastState: {}
      name: csi-cephfsplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:49Z"
    - containerID: docker://6af5aaf8f13a8960aaf774598202ff220a39076b78bc0891545e07f2e416765c
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.1
      imageID: docker-pullable://registry.k8s.io/sig-storage/csi-node-driver-registrar@sha256:2cddcc716c1930775228d56b0d2d339358647629701047edfdad5fcdfaf4ebcb
      lastState: {}
      name: driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:48Z"
    hostIP: 10.2.1.184
    phase: Running
    podIP: 10.2.1.184
    podIPs:
    - ip: 10.2.1.184
    qosClass: Burstable
    startTime: "2024-03-13T16:49:47Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-13T16:49:43Z"
    generateName: csi-cephfsplugin-
    labels:
      app: csi-cephfsplugin
      contains: csi-cephfsplugin-metrics
      controller-revision-hash: 65778df578
      pod-template-generation: "1"
    name: csi-cephfsplugin-btbtw
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-cephfsplugin
      uid: 5366696a-b570-4ac6-8e32-91d05178bbf4
    resourceVersion: "13085005"
    uid: 22bc78c5-20ae-45fd-b707-03568e5f578b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - worker-3
    containers:
    - args:
      - --v=0
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/csi.sock
      env:
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.1
      imagePullPolicy: IfNotPresent
      name: driver-registrar
      resources:
        limits:
          cpu: 100m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      securityContext:
        capabilities:
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-njfql
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --type=cephfs
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --nodeserver=true
      - --drivername=rook-ceph.cephfs.csi.ceph.com
      - --pidlimit=-1
      - --forcecephkernelclient=true
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: quay.io/cephcsi/cephcsi:v3.10.1
      imagePullPolicy: IfNotPresent
      name: csi-cephfsplugin
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /csi/mountinfo
        name: ceph-csi-mountinfo
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: Bidirectional
        name: csi-plugins-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /dev
        name: host-dev
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-njfql
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    nodeName: worker-3
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-cephfs-plugin-sa
    serviceAccountName: rook-csi-cephfs-plugin-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/mountinfo
        type: DirectoryOrCreate
      name: ceph-csi-mountinfo
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: Directory
      name: csi-plugins-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: Directory
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - configMap:
        defaultMode: 420
        items:
        - key: csi-cluster-config-json
          path: config.json
        name: rook-ceph-csi-config
      name: ceph-csi-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - hostPath:
        path: /run/mount
        type: ""
      name: host-run-mount
    - name: kube-api-access-njfql
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:48Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:50Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:50Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:48Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://00f4aadd4b633ac78854aa2c7786a9d11cba1c7db5fd11a2bfa8cd15e42d41a9
      image: quay.io/cephcsi/cephcsi:v3.10.1
      imageID: docker-pullable://quay.io/cephcsi/cephcsi@sha256:5dd50ad6f3f9a1e8c8186fde0048ea241f056ca755acbeab42f5ebf723313e9c
      lastState: {}
      name: csi-cephfsplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:50Z"
    - containerID: docker://bc5ce897308686bd6d611ab71f74209f62c1f2fb642fd8112e8a84ea932395c5
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.1
      imageID: docker-pullable://registry.k8s.io/sig-storage/csi-node-driver-registrar@sha256:2cddcc716c1930775228d56b0d2d339358647629701047edfdad5fcdfaf4ebcb
      lastState: {}
      name: driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:49Z"
    hostIP: 10.2.1.185
    phase: Running
    podIP: 10.2.1.185
    podIPs:
    - ip: 10.2.1.185
    qosClass: Burstable
    startTime: "2024-03-13T16:49:48Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: ad04f150237a8f597ffea51766f4c00fb52cdf70f4ace980b52fce0f30a17ab6
      cni.projectcalico.org/podIP: 10.42.133.204/32
      cni.projectcalico.org/podIPs: 10.42.133.204/32
    creationTimestamp: "2024-03-13T16:49:39Z"
    generateName: csi-cephfsplugin-provisioner-774788f85-
    labels:
      app: csi-cephfsplugin-provisioner
      contains: csi-cephfsplugin-metrics
      pod-template-hash: 774788f85
    name: csi-cephfsplugin-provisioner-774788f85-7vqrl
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: csi-cephfsplugin-provisioner-774788f85
      uid: 7d259d28-6418-483a-961d-3dde90aad44c
    resourceVersion: "13085158"
    uid: 9de47074-bbbb-4b7d-b1e9-16962d0cd960
  spec:
    affinity:
      nodeAffinity: {}
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - csi-cephfsplugin-provisioner
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --v=0
      - --csi-address=$(ADDRESS)
      - --leader-election=true
      - --timeout=2m30s
      - --leader-election-namespace=rook-ceph
      - --leader-election-lease-duration=137s
      - --leader-election-renew-deadline=107s
      - --leader-election-retry-period=26s
      env:
      - name: ADDRESS
        value: /csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-attacher:v4.4.2
      imagePullPolicy: IfNotPresent
      name: csi-attacher
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-86fnb
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      - --leader-election-lease-duration=137s
      - --leader-election-renew-deadline=107s
      - --leader-election-retry-period=26s
      - --handle-volume-inuse-error=false
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-resizer:v1.9.2
      imagePullPolicy: IfNotPresent
      name: csi-resizer
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-86fnb
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --retry-interval-start=500ms
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      - --leader-election-lease-duration=137s
      - --leader-election-renew-deadline=107s
      - --leader-election-retry-period=26s
      - --extra-create-metadata=true
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-provisioner:v3.6.2
      imagePullPolicy: IfNotPresent
      name: csi-provisioner
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-86fnb
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --type=cephfs
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --controllerserver=true
      - --drivername=rook-ceph.cephfs.csi.ceph.com
      - --pidlimit=-1
      - --forcecephkernelclient=true
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi-provisioner.sock
      image: quay.io/cephcsi/cephcsi:v3.10.1
      imagePullPolicy: IfNotPresent
      name: csi-cephfsplugin
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 512Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /dev
        name: host-dev
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-86fnb
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker-2
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-cephfs-provisioner-sa
    serviceAccountName: rook-csi-cephfs-provisioner-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
      name: socket-dir
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - configMap:
        defaultMode: 420
        items:
        - key: csi-cluster-config-json
          path: config.json
        name: rook-ceph-csi-config
      name: ceph-csi-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - name: kube-api-access-86fnb
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:52Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:52Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://d957645bca4b26c629cd08cc2693852d1901eb3315d4f62d3dd990bdfb0e0925
      image: registry.k8s.io/sig-storage/csi-attacher:v4.4.2
      imageID: docker-pullable://registry.k8s.io/sig-storage/csi-attacher@sha256:11b955fe4da278aa0e8ca9d6fd70758f2aec4b0c1e23168c665ca345260f1882
      lastState: {}
      name: csi-attacher
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:52Z"
    - containerID: docker://42002c70ce925f5a805cdab5b317f11b2621647ff80c96c67856262bdf79f813
      image: quay.io/cephcsi/cephcsi:v3.10.1
      imageID: docker-pullable://quay.io/cephcsi/cephcsi@sha256:5dd50ad6f3f9a1e8c8186fde0048ea241f056ca755acbeab42f5ebf723313e9c
      lastState: {}
      name: csi-cephfsplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:52Z"
    - containerID: docker://6f1d36b1e5a3740212093427de962cea849440e528b6d6ef4f5a6b6a34bfc882
      image: registry.k8s.io/sig-storage/csi-provisioner:v3.6.2
      imageID: docker-pullable://registry.k8s.io/sig-storage/csi-provisioner@sha256:49b94f975603d85a1820b72b1188e5b351d122011b3e5351f98c49d72719aa78
      lastState: {}
      name: csi-provisioner
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:52Z"
    - containerID: docker://68b2faa2c4a8a31c5ae3dd274024b8a2aaa48c3f32d924093065645b7ae03722
      image: registry.k8s.io/sig-storage/csi-resizer:v1.9.2
      imageID: docker-pullable://registry.k8s.io/sig-storage/csi-resizer@sha256:e998f22243869416f9860fc6a1fb07d4202eac8846defc1b85ebd015c1207605
      lastState: {}
      name: csi-resizer
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:52Z"
    hostIP: 10.2.1.184
    phase: Running
    podIP: 10.42.133.204
    podIPs:
    - ip: 10.42.133.204
    qosClass: Burstable
    startTime: "2024-03-13T16:49:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: fcb10d1af60ef289f6c6b2e42cdbd58d0d033b37bf7294db34aab82768b57799
      cni.projectcalico.org/podIP: 10.42.226.76/32
      cni.projectcalico.org/podIPs: 10.42.226.76/32
    creationTimestamp: "2024-03-13T16:49:39Z"
    generateName: csi-cephfsplugin-provisioner-774788f85-
    labels:
      app: csi-cephfsplugin-provisioner
      contains: csi-cephfsplugin-metrics
      pod-template-hash: 774788f85
    name: csi-cephfsplugin-provisioner-774788f85-vsrnf
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: csi-cephfsplugin-provisioner-774788f85
      uid: 7d259d28-6418-483a-961d-3dde90aad44c
    resourceVersion: "13084569"
    uid: 83cf52cb-902d-4a92-8e2a-5eda2c6c50c8
  spec:
    affinity:
      nodeAffinity: {}
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - csi-cephfsplugin-provisioner
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --v=0
      - --csi-address=$(ADDRESS)
      - --leader-election=true
      - --timeout=2m30s
      - --leader-election-namespace=rook-ceph
      - --leader-election-lease-duration=137s
      - --leader-election-renew-deadline=107s
      - --leader-election-retry-period=26s
      env:
      - name: ADDRESS
        value: /csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-attacher:v4.4.2
      imagePullPolicy: IfNotPresent
      name: csi-attacher
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dbd7c
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      - --leader-election-lease-duration=137s
      - --leader-election-renew-deadline=107s
      - --leader-election-retry-period=26s
      - --handle-volume-inuse-error=false
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-resizer:v1.9.2
      imagePullPolicy: IfNotPresent
      name: csi-resizer
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dbd7c
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --retry-interval-start=500ms
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      - --leader-election-lease-duration=137s
      - --leader-election-renew-deadline=107s
      - --leader-election-retry-period=26s
      - --extra-create-metadata=true
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-provisioner:v3.6.2
      imagePullPolicy: IfNotPresent
      name: csi-provisioner
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dbd7c
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --type=cephfs
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --controllerserver=true
      - --drivername=rook-ceph.cephfs.csi.ceph.com
      - --pidlimit=-1
      - --forcecephkernelclient=true
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi-provisioner.sock
      image: quay.io/cephcsi/cephcsi:v3.10.1
      imagePullPolicy: IfNotPresent
      name: csi-cephfsplugin
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 512Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /dev
        name: host-dev
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dbd7c
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker-1
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-cephfs-provisioner-sa
    serviceAccountName: rook-csi-cephfs-provisioner-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
      name: socket-dir
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - configMap:
        defaultMode: 420
        items:
        - key: csi-cluster-config-json
          path: config.json
        name: rook-ceph-csi-config
      name: ceph-csi-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - name: kube-api-access-dbd7c
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:39Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:42Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:42Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:39Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://dc597d5ec8236193789ecb1e2d26579c5b636e62538fd2b11d16cff775ccbef3
      image: registry.k8s.io/sig-storage/csi-attacher:v4.4.2
      imageID: docker-pullable://registry.k8s.io/sig-storage/csi-attacher@sha256:11b955fe4da278aa0e8ca9d6fd70758f2aec4b0c1e23168c665ca345260f1882
      lastState: {}
      name: csi-attacher
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:41Z"
    - containerID: docker://2f6bdde44d24101dec83f166128adf2959449d7434538a2e38894a0d1e4ffe84
      image: quay.io/cephcsi/cephcsi:v3.10.1
      imageID: docker-pullable://quay.io/cephcsi/cephcsi@sha256:5dd50ad6f3f9a1e8c8186fde0048ea241f056ca755acbeab42f5ebf723313e9c
      lastState: {}
      name: csi-cephfsplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:42Z"
    - containerID: docker://13716b980b5658305c401368d14c4a33e8abbeca0e0df48056440f589824d7a7
      image: registry.k8s.io/sig-storage/csi-provisioner:v3.6.2
      imageID: docker-pullable://registry.k8s.io/sig-storage/csi-provisioner@sha256:49b94f975603d85a1820b72b1188e5b351d122011b3e5351f98c49d72719aa78
      lastState: {}
      name: csi-provisioner
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:42Z"
    - containerID: docker://6b594992a0c14f4ed9f1c5fc2c36c28a072bdaa197ba6d8273dee6839fbaef0a
      image: registry.k8s.io/sig-storage/csi-resizer:v1.9.2
      imageID: docker-pullable://registry.k8s.io/sig-storage/csi-resizer@sha256:e998f22243869416f9860fc6a1fb07d4202eac8846defc1b85ebd015c1207605
      lastState: {}
      name: csi-resizer
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:42Z"
    hostIP: 10.2.1.183
    phase: Running
    podIP: 10.42.226.76
    podIPs:
    - ip: 10.42.226.76
    qosClass: Burstable
    startTime: "2024-03-13T16:49:39Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-13T16:49:44Z"
    generateName: csi-cephfsplugin-
    labels:
      app: csi-cephfsplugin
      contains: csi-cephfsplugin-metrics
      controller-revision-hash: 65778df578
      pod-template-generation: "1"
    name: csi-cephfsplugin-v9xqz
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-cephfsplugin
      uid: 5366696a-b570-4ac6-8e32-91d05178bbf4
    resourceVersion: "13085265"
    uid: d6891079-427c-4c2f-8528-96c97ee82be7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - worker-1
    containers:
    - args:
      - --v=0
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/csi.sock
      env:
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.1
      imagePullPolicy: IfNotPresent
      name: driver-registrar
      resources:
        limits:
          cpu: 100m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      securityContext:
        capabilities:
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-55jnr
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --type=cephfs
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --nodeserver=true
      - --drivername=rook-ceph.cephfs.csi.ceph.com
      - --pidlimit=-1
      - --forcecephkernelclient=true
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: quay.io/cephcsi/cephcsi:v3.10.1
      imagePullPolicy: IfNotPresent
      name: csi-cephfsplugin
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /csi/mountinfo
        name: ceph-csi-mountinfo
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: Bidirectional
        name: csi-plugins-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /dev
        name: host-dev
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-55jnr
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    nodeName: worker-1
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-cephfs-plugin-sa
    serviceAccountName: rook-csi-cephfs-plugin-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/mountinfo
        type: DirectoryOrCreate
      name: ceph-csi-mountinfo
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: Directory
      name: csi-plugins-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: Directory
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - configMap:
        defaultMode: 420
        items:
        - key: csi-cluster-config-json
          path: config.json
        name: rook-ceph-csi-config
      name: ceph-csi-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - hostPath:
        path: /run/mount
        type: ""
      name: host-run-mount
    - name: kube-api-access-55jnr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:55Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:55Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://7371602da5689b64ef2ad23215eac14f7a5e17340aa31d0f6d1f392be0db98bc
      image: quay.io/cephcsi/cephcsi:v3.10.1
      imageID: docker-pullable://quay.io/cephcsi/cephcsi@sha256:5dd50ad6f3f9a1e8c8186fde0048ea241f056ca755acbeab42f5ebf723313e9c
      lastState: {}
      name: csi-cephfsplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:54Z"
    - containerID: docker://0b59eb5db46c2a5221a43e919f89e5693df3ede49712b7f6f956b0c55680d8d3
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.1
      imageID: docker-pullable://registry.k8s.io/sig-storage/csi-node-driver-registrar@sha256:2cddcc716c1930775228d56b0d2d339358647629701047edfdad5fcdfaf4ebcb
      lastState: {}
      name: driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:54Z"
    hostIP: 10.2.1.183
    phase: Running
    podIP: 10.2.1.183
    podIPs:
    - ip: 10.2.1.183
    qosClass: Burstable
    startTime: "2024-03-13T16:49:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-13T16:49:52Z"
    generateName: csi-cephfsplugin-
    labels:
      app: csi-cephfsplugin
      contains: csi-cephfsplugin-metrics
      controller-revision-hash: 65778df578
      pod-template-generation: "1"
    name: csi-cephfsplugin-w9dlk
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-cephfsplugin
      uid: 5366696a-b570-4ac6-8e32-91d05178bbf4
    resourceVersion: "13085258"
    uid: 250cf210-07eb-472d-ba5d-86295f23e1cd
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - worker-4
    containers:
    - args:
      - --v=0
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/csi.sock
      env:
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.1
      imagePullPolicy: IfNotPresent
      name: driver-registrar
      resources:
        limits:
          cpu: 100m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      securityContext:
        capabilities:
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-m2npd
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --type=cephfs
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --nodeserver=true
      - --drivername=rook-ceph.cephfs.csi.ceph.com
      - --pidlimit=-1
      - --forcecephkernelclient=true
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: quay.io/cephcsi/cephcsi:v3.10.1
      imagePullPolicy: IfNotPresent
      name: csi-cephfsplugin
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /csi/mountinfo
        name: ceph-csi-mountinfo
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: Bidirectional
        name: csi-plugins-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /dev
        name: host-dev
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-m2npd
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    nodeName: worker-4
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-cephfs-plugin-sa
    serviceAccountName: rook-csi-cephfs-plugin-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/mountinfo
        type: DirectoryOrCreate
      name: ceph-csi-mountinfo
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: Directory
      name: csi-plugins-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: Directory
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - configMap:
        defaultMode: 420
        items:
        - key: csi-cluster-config-json
          path: config.json
        name: rook-ceph-csi-config
      name: ceph-csi-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - hostPath:
        path: /run/mount
        type: ""
      name: host-run-mount
    - name: kube-api-access-m2npd
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:55Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:55Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://109227c391653b55cfffde03967a8883fcd44642936221ee17b9f86f3edb3bbf
      image: quay.io/cephcsi/cephcsi:v3.10.1
      imageID: docker-pullable://quay.io/cephcsi/cephcsi@sha256:5dd50ad6f3f9a1e8c8186fde0048ea241f056ca755acbeab42f5ebf723313e9c
      lastState: {}
      name: csi-cephfsplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:53Z"
    - containerID: docker://bf9b2d4bc55766c5aba1bffb401808780a40a09a52dee743f2ce2c4deb9084d4
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.1
      imageID: docker-pullable://registry.k8s.io/sig-storage/csi-node-driver-registrar@sha256:2cddcc716c1930775228d56b0d2d339358647629701047edfdad5fcdfaf4ebcb
      lastState: {}
      name: driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:53Z"
    hostIP: 10.2.1.186
    phase: Running
    podIP: 10.2.1.186
    podIPs:
    - ip: 10.2.1.186
    qosClass: Burstable
    startTime: "2024-03-13T16:49:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-13T16:49:51Z"
    generateName: csi-rbdplugin-
    labels:
      app: csi-rbdplugin
      contains: csi-rbdplugin-metrics
      controller-revision-hash: 84956bdc47
      pod-template-generation: "1"
    name: csi-rbdplugin-kf57p
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-rbdplugin
      uid: 6ce54ca4-f627-468b-82db-9479e25076fb
    resourceVersion: "13085153"
    uid: 1009007b-6e1f-4a9f-a983-707ae9ea1bd9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - worker-3
    containers:
    - args:
      - --v=0
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/rook-ceph.rbd.csi.ceph.com/csi.sock
      env:
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.1
      imagePullPolicy: IfNotPresent
      name: driver-registrar
      resources:
        limits:
          cpu: 100m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      securityContext:
        capabilities:
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xnlpm
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --type=rbd
      - --nodeserver=true
      - --drivername=rook-ceph.rbd.csi.ceph.com
      - --pidlimit=-1
      - --stagingpath=/var/lib/kubelet/plugins/kubernetes.io/csi/
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: quay.io/cephcsi/cephcsi:v3.10.1
      imagePullPolicy: IfNotPresent
      name: csi-rbdplugin
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: Bidirectional
        name: plugin-mount-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-configs
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /run/secrets/tokens
        name: oidc-token
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xnlpm
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: worker-3
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-rbd-plugin-sa
    serviceAccountName: rook-csi-rbd-plugin-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.rbd.csi.ceph.com
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: Directory
      name: plugin-mount-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: Directory
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: ceph-csi-configs
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
        - configMap:
            items:
            - key: csi-mapping-config-json
              path: cluster-mapping.json
            name: rook-ceph-csi-mapping-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - hostPath:
        path: /run/mount
        type: ""
      name: host-run-mount
    - name: oidc-token
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            audience: ceph-csi-kms
            expirationSeconds: 3600
            path: oidc-token
    - name: kube-api-access-xnlpm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:51Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:53Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:53Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:51Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://0ea3261b9e98e36fa7bbb5180f5b544ac1f4e8aa58cbfdf2d598dbb504240cc0
      image: quay.io/cephcsi/cephcsi:v3.10.1
      imageID: docker-pullable://quay.io/cephcsi/cephcsi@sha256:5dd50ad6f3f9a1e8c8186fde0048ea241f056ca755acbeab42f5ebf723313e9c
      lastState: {}
      name: csi-rbdplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:53Z"
    - containerID: docker://9846d57779ddf84fc35a1ddcaed934e2021e47b0617f9dc60d8253fad3bf4dea
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.1
      imageID: docker-pullable://registry.k8s.io/sig-storage/csi-node-driver-registrar@sha256:2cddcc716c1930775228d56b0d2d339358647629701047edfdad5fcdfaf4ebcb
      lastState: {}
      name: driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:52Z"
    hostIP: 10.2.1.185
    phase: Running
    podIP: 10.2.1.185
    podIPs:
    - ip: 10.2.1.185
    qosClass: Burstable
    startTime: "2024-03-13T16:49:51Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 4f6fcb10a06c97bcd060ec2a5a3961ed3becd2a69c56f6107514ee435ad913a8
      cni.projectcalico.org/podIP: 10.42.133.205/32
      cni.projectcalico.org/podIPs: 10.42.133.205/32
    creationTimestamp: "2024-03-13T16:49:43Z"
    generateName: csi-rbdplugin-provisioner-56f7bf6d4d-
    labels:
      app: csi-rbdplugin-provisioner
      contains: csi-rbdplugin-metrics
      pod-template-hash: 56f7bf6d4d
    name: csi-rbdplugin-provisioner-56f7bf6d4d-l9wtg
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: csi-rbdplugin-provisioner-56f7bf6d4d
      uid: 9a0f4d8d-835c-4750-8876-53b3797c0357
    resourceVersion: "13085216"
    uid: f337b7fa-b2d5-4764-887a-8eb12de25a6d
  spec:
    affinity:
      nodeAffinity: {}
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - csi-rbdplugin-provisioner
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --retry-interval-start=500ms
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      - --leader-election-lease-duration=137s
      - --leader-election-renew-deadline=107s
      - --leader-election-retry-period=26s
      - --default-fstype=ext4
      - --extra-create-metadata=true
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-provisioner:v3.6.2
      imagePullPolicy: IfNotPresent
      name: csi-provisioner
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-z7gf4
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      - --leader-election-lease-duration=137s
      - --leader-election-renew-deadline=107s
      - --leader-election-retry-period=26s
      - --handle-volume-inuse-error=false
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-resizer:v1.9.2
      imagePullPolicy: IfNotPresent
      name: csi-resizer
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-z7gf4
        readOnly: true
    - args:
      - --v=0
      - --timeout=2m30s
      - --csi-address=$(ADDRESS)
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      - --leader-election-lease-duration=137s
      - --leader-election-renew-deadline=107s
      - --leader-election-retry-period=26s
      - --default-fstype=ext4
      env:
      - name: ADDRESS
        value: /csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-attacher:v4.4.2
      imagePullPolicy: IfNotPresent
      name: csi-attacher
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-z7gf4
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      - --leader-election-lease-duration=137s
      - --leader-election-renew-deadline=107s
      - --leader-election-retry-period=26s
      - --extra-create-metadata=true
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-snapshotter:v6.3.2
      imagePullPolicy: IfNotPresent
      name: csi-snapshotter
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-z7gf4
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --type=rbd
      - --controllerserver=true
      - --drivername=rook-ceph.rbd.csi.ceph.com
      - --pidlimit=-1
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi-provisioner.sock
      image: quay.io/cephcsi/cephcsi:v3.10.1
      imagePullPolicy: IfNotPresent
      name: csi-rbdplugin
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 512Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-configs
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/secrets/tokens
        name: oidc-token
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-z7gf4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker-2
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-rbd-provisioner-sa
    serviceAccountName: rook-csi-rbd-provisioner-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - emptyDir:
        medium: Memory
      name: socket-dir
    - name: ceph-csi-configs
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
        - configMap:
            items:
            - key: csi-mapping-config-json
              path: cluster-mapping.json
            name: rook-ceph-csi-mapping-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - name: oidc-token
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            audience: ceph-csi-kms
            expirationSeconds: 3600
            path: oidc-token
    - name: kube-api-access-z7gf4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:56Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:56Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://71e4ebd07d82fb88693746a1e8b7dcd7d67bf713d2fce74e018b33e27920dc69
      image: registry.k8s.io/sig-storage/csi-attacher:v4.4.2
      imageID: docker-pullable://registry.k8s.io/sig-storage/csi-attacher@sha256:11b955fe4da278aa0e8ca9d6fd70758f2aec4b0c1e23168c665ca345260f1882
      lastState: {}
      name: csi-attacher
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:54Z"
    - containerID: docker://4ee6b7880a09b8657e5c0d245ca13e91e7bfa217b64c8ef1f708408cfc2c36d9
      image: registry.k8s.io/sig-storage/csi-provisioner:v3.6.2
      imageID: docker-pullable://registry.k8s.io/sig-storage/csi-provisioner@sha256:49b94f975603d85a1820b72b1188e5b351d122011b3e5351f98c49d72719aa78
      lastState: {}
      name: csi-provisioner
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:54Z"
    - containerID: docker://85c03776aa49ccd45d1254338350221d493aafbe5254a41afbdadabab92cb60a
      image: quay.io/cephcsi/cephcsi:v3.10.1
      imageID: docker-pullable://quay.io/cephcsi/cephcsi@sha256:5dd50ad6f3f9a1e8c8186fde0048ea241f056ca755acbeab42f5ebf723313e9c
      lastState: {}
      name: csi-rbdplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:55Z"
    - containerID: docker://0f3f8f6370c4c60b7df63b28ec610e2f2e881d07df1f83092dd74000d109a3c2
      image: registry.k8s.io/sig-storage/csi-resizer:v1.9.2
      imageID: docker-pullable://registry.k8s.io/sig-storage/csi-resizer@sha256:e998f22243869416f9860fc6a1fb07d4202eac8846defc1b85ebd015c1207605
      lastState: {}
      name: csi-resizer
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:54Z"
    - containerID: docker://5726cbd58e51c8a2415c3f234e7962d4d4d5aa20f94950e855117c9fea5cf691
      image: registry.k8s.io/sig-storage/csi-snapshotter:v6.3.2
      imageID: docker-pullable://registry.k8s.io/sig-storage/csi-snapshotter@sha256:4c5a1b57e685b2631909b958487f65af7746361346fcd82a8635bea3ef14509d
      lastState: {}
      name: csi-snapshotter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:55Z"
    hostIP: 10.2.1.184
    phase: Running
    podIP: 10.42.133.205
    podIPs:
    - ip: 10.42.133.205
    qosClass: Burstable
    startTime: "2024-03-13T16:49:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: c68b5f09d61fb248406da33389723fdda8ac1c69bf68fb7e96636cd2dfaf30a0
      cni.projectcalico.org/podIP: 10.42.97.209/32
      cni.projectcalico.org/podIPs: 10.42.97.209/32
    creationTimestamp: "2024-03-13T16:49:42Z"
    generateName: csi-rbdplugin-provisioner-56f7bf6d4d-
    labels:
      app: csi-rbdplugin-provisioner
      contains: csi-rbdplugin-metrics
      pod-template-hash: 56f7bf6d4d
    name: csi-rbdplugin-provisioner-56f7bf6d4d-wk9v8
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: csi-rbdplugin-provisioner-56f7bf6d4d
      uid: 9a0f4d8d-835c-4750-8876-53b3797c0357
    resourceVersion: "13085187"
    uid: df2890b7-9077-4358-ad58-a4bfbff03633
  spec:
    affinity:
      nodeAffinity: {}
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - csi-rbdplugin-provisioner
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --retry-interval-start=500ms
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      - --leader-election-lease-duration=137s
      - --leader-election-renew-deadline=107s
      - --leader-election-retry-period=26s
      - --default-fstype=ext4
      - --extra-create-metadata=true
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-provisioner:v3.6.2
      imagePullPolicy: IfNotPresent
      name: csi-provisioner
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qf8gl
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      - --leader-election-lease-duration=137s
      - --leader-election-renew-deadline=107s
      - --leader-election-retry-period=26s
      - --handle-volume-inuse-error=false
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-resizer:v1.9.2
      imagePullPolicy: IfNotPresent
      name: csi-resizer
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qf8gl
        readOnly: true
    - args:
      - --v=0
      - --timeout=2m30s
      - --csi-address=$(ADDRESS)
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      - --leader-election-lease-duration=137s
      - --leader-election-renew-deadline=107s
      - --leader-election-retry-period=26s
      - --default-fstype=ext4
      env:
      - name: ADDRESS
        value: /csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-attacher:v4.4.2
      imagePullPolicy: IfNotPresent
      name: csi-attacher
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qf8gl
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      - --leader-election-lease-duration=137s
      - --leader-election-renew-deadline=107s
      - --leader-election-retry-period=26s
      - --extra-create-metadata=true
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-snapshotter:v6.3.2
      imagePullPolicy: IfNotPresent
      name: csi-snapshotter
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qf8gl
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --type=rbd
      - --controllerserver=true
      - --drivername=rook-ceph.rbd.csi.ceph.com
      - --pidlimit=-1
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi-provisioner.sock
      image: quay.io/cephcsi/cephcsi:v3.10.1
      imagePullPolicy: IfNotPresent
      name: csi-rbdplugin
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 512Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-configs
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/secrets/tokens
        name: oidc-token
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qf8gl
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker-3
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-rbd-provisioner-sa
    serviceAccountName: rook-csi-rbd-provisioner-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - emptyDir:
        medium: Memory
      name: socket-dir
    - name: ceph-csi-configs
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
        - configMap:
            items:
            - key: csi-mapping-config-json
              path: cluster-mapping.json
            name: rook-ceph-csi-mapping-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - name: oidc-token
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            audience: ceph-csi-kms
            expirationSeconds: 3600
            path: oidc-token
    - name: kube-api-access-qf8gl
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:56Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:56Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://c05e48d921689dba0e64734cf8a6042926e99714cb22bf38acf6ae67487e0ed6
      image: registry.k8s.io/sig-storage/csi-attacher:v4.4.2
      imageID: docker-pullable://registry.k8s.io/sig-storage/csi-attacher@sha256:11b955fe4da278aa0e8ca9d6fd70758f2aec4b0c1e23168c665ca345260f1882
      lastState: {}
      name: csi-attacher
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:55Z"
    - containerID: docker://ada9064f80ec2eba64046e1dc12b6c616a39ad7cd1567464a3fb34095183372d
      image: registry.k8s.io/sig-storage/csi-provisioner:v3.6.2
      imageID: docker-pullable://registry.k8s.io/sig-storage/csi-provisioner@sha256:49b94f975603d85a1820b72b1188e5b351d122011b3e5351f98c49d72719aa78
      lastState: {}
      name: csi-provisioner
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:55Z"
    - containerID: docker://5e47636293da6723d20629892490dcb5131ce1f0fc4295f688c9b545a19744fa
      image: quay.io/cephcsi/cephcsi:v3.10.1
      imageID: docker-pullable://quay.io/cephcsi/cephcsi@sha256:5dd50ad6f3f9a1e8c8186fde0048ea241f056ca755acbeab42f5ebf723313e9c
      lastState: {}
      name: csi-rbdplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:56Z"
    - containerID: docker://57057815c84cdd34197c73bb7a547308ae9f26cdbb6fed8445e4bac45a177533
      image: registry.k8s.io/sig-storage/csi-resizer:v1.9.2
      imageID: docker-pullable://registry.k8s.io/sig-storage/csi-resizer@sha256:e998f22243869416f9860fc6a1fb07d4202eac8846defc1b85ebd015c1207605
      lastState: {}
      name: csi-resizer
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:55Z"
    - containerID: docker://fa4122e78f1667593771019d1dc51192cc0be8107ef1ca5bc6723aa7f2fa3247
      image: registry.k8s.io/sig-storage/csi-snapshotter:v6.3.2
      imageID: docker-pullable://registry.k8s.io/sig-storage/csi-snapshotter@sha256:4c5a1b57e685b2631909b958487f65af7746361346fcd82a8635bea3ef14509d
      lastState: {}
      name: csi-snapshotter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:56Z"
    hostIP: 10.2.1.185
    phase: Running
    podIP: 10.42.97.209
    podIPs:
    - ip: 10.42.97.209
    qosClass: Burstable
    startTime: "2024-03-13T16:49:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-13T16:49:44Z"
    generateName: csi-rbdplugin-
    labels:
      app: csi-rbdplugin
      contains: csi-rbdplugin-metrics
      controller-revision-hash: 84956bdc47
      pod-template-generation: "1"
    name: csi-rbdplugin-pw2hj
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-rbdplugin
      uid: 6ce54ca4-f627-468b-82db-9479e25076fb
    resourceVersion: "13084704"
    uid: fa85799b-0de7-47a5-8550-76e64873a596
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - worker-2
    containers:
    - args:
      - --v=0
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/rook-ceph.rbd.csi.ceph.com/csi.sock
      env:
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.1
      imagePullPolicy: IfNotPresent
      name: driver-registrar
      resources:
        limits:
          cpu: 100m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      securityContext:
        capabilities:
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t9qvq
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --type=rbd
      - --nodeserver=true
      - --drivername=rook-ceph.rbd.csi.ceph.com
      - --pidlimit=-1
      - --stagingpath=/var/lib/kubelet/plugins/kubernetes.io/csi/
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: quay.io/cephcsi/cephcsi:v3.10.1
      imagePullPolicy: IfNotPresent
      name: csi-rbdplugin
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: Bidirectional
        name: plugin-mount-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-configs
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /run/secrets/tokens
        name: oidc-token
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t9qvq
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: worker-2
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-rbd-plugin-sa
    serviceAccountName: rook-csi-rbd-plugin-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.rbd.csi.ceph.com
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: Directory
      name: plugin-mount-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: Directory
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: ceph-csi-configs
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
        - configMap:
            items:
            - key: csi-mapping-config-json
              path: cluster-mapping.json
            name: rook-ceph-csi-mapping-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - hostPath:
        path: /run/mount
        type: ""
      name: host-run-mount
    - name: oidc-token
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            audience: ceph-csi-kms
            expirationSeconds: 3600
            path: oidc-token
    - name: kube-api-access-t9qvq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:45Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:47Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:47Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:45Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://fef1cc133bdd5bfa40ff86c7bb3bb3921168b244e068a8cac1d8041a35662eea
      image: quay.io/cephcsi/cephcsi:v3.10.1
      imageID: docker-pullable://quay.io/cephcsi/cephcsi@sha256:5dd50ad6f3f9a1e8c8186fde0048ea241f056ca755acbeab42f5ebf723313e9c
      lastState: {}
      name: csi-rbdplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:47Z"
    - containerID: docker://bce676a775b5b5640c6de3f1a70d08cddfb32f6c64bdaf9b4ba0e8591c6a047f
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.1
      imageID: docker-pullable://registry.k8s.io/sig-storage/csi-node-driver-registrar@sha256:2cddcc716c1930775228d56b0d2d339358647629701047edfdad5fcdfaf4ebcb
      lastState: {}
      name: driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:47Z"
    hostIP: 10.2.1.184
    phase: Running
    podIP: 10.2.1.184
    podIPs:
    - ip: 10.2.1.184
    qosClass: Burstable
    startTime: "2024-03-13T16:49:45Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-13T16:49:45Z"
    generateName: csi-rbdplugin-
    labels:
      app: csi-rbdplugin
      contains: csi-rbdplugin-metrics
      controller-revision-hash: 84956bdc47
      pod-template-generation: "1"
    name: csi-rbdplugin-wbdjn
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-rbdplugin
      uid: 6ce54ca4-f627-468b-82db-9479e25076fb
    resourceVersion: "13085324"
    uid: d4f2b9d7-aee1-4757-a54f-93897c7f1b4a
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - worker-1
    containers:
    - args:
      - --v=0
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/rook-ceph.rbd.csi.ceph.com/csi.sock
      env:
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.1
      imagePullPolicy: IfNotPresent
      name: driver-registrar
      resources:
        limits:
          cpu: 100m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      securityContext:
        capabilities:
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-48w9n
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --type=rbd
      - --nodeserver=true
      - --drivername=rook-ceph.rbd.csi.ceph.com
      - --pidlimit=-1
      - --stagingpath=/var/lib/kubelet/plugins/kubernetes.io/csi/
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: quay.io/cephcsi/cephcsi:v3.10.1
      imagePullPolicy: IfNotPresent
      name: csi-rbdplugin
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: Bidirectional
        name: plugin-mount-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-configs
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /run/secrets/tokens
        name: oidc-token
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-48w9n
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: worker-1
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-rbd-plugin-sa
    serviceAccountName: rook-csi-rbd-plugin-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.rbd.csi.ceph.com
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: Directory
      name: plugin-mount-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: Directory
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: ceph-csi-configs
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
        - configMap:
            items:
            - key: csi-mapping-config-json
              path: cluster-mapping.json
            name: rook-ceph-csi-mapping-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - hostPath:
        path: /run/mount
        type: ""
      name: host-run-mount
    - name: oidc-token
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            audience: ceph-csi-kms
            expirationSeconds: 3600
            path: oidc-token
    - name: kube-api-access-48w9n
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:53Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:57Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:57Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:53Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://1645689c38c649f39870ac4ebbaed0862ad0f51f806a6e5620e3ce06da4f354d
      image: quay.io/cephcsi/cephcsi:v3.10.1
      imageID: docker-pullable://quay.io/cephcsi/cephcsi@sha256:5dd50ad6f3f9a1e8c8186fde0048ea241f056ca755acbeab42f5ebf723313e9c
      lastState: {}
      name: csi-rbdplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:56Z"
    - containerID: docker://0b2cf5afd55b8e656ae303a3aa291f864c63a18e2215dbaccc9a3f0484f39acb
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.1
      imageID: docker-pullable://registry.k8s.io/sig-storage/csi-node-driver-registrar@sha256:2cddcc716c1930775228d56b0d2d339358647629701047edfdad5fcdfaf4ebcb
      lastState: {}
      name: driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:55Z"
    hostIP: 10.2.1.183
    phase: Running
    podIP: 10.2.1.183
    podIPs:
    - ip: 10.2.1.183
    qosClass: Burstable
    startTime: "2024-03-13T16:49:53Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-13T16:49:43Z"
    generateName: csi-rbdplugin-
    labels:
      app: csi-rbdplugin
      contains: csi-rbdplugin-metrics
      controller-revision-hash: 84956bdc47
      pod-template-generation: "1"
    name: csi-rbdplugin-xk4g6
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-rbdplugin
      uid: 6ce54ca4-f627-468b-82db-9479e25076fb
    resourceVersion: "13085114"
    uid: da7afbbd-7f68-407c-866f-84d513cfe774
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - worker-4
    containers:
    - args:
      - --v=0
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/rook-ceph.rbd.csi.ceph.com/csi.sock
      env:
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.1
      imagePullPolicy: IfNotPresent
      name: driver-registrar
      resources:
        limits:
          cpu: 100m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      securityContext:
        capabilities:
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hql9z
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --type=rbd
      - --nodeserver=true
      - --drivername=rook-ceph.rbd.csi.ceph.com
      - --pidlimit=-1
      - --stagingpath=/var/lib/kubelet/plugins/kubernetes.io/csi/
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: quay.io/cephcsi/cephcsi:v3.10.1
      imagePullPolicy: IfNotPresent
      name: csi-rbdplugin
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: Bidirectional
        name: plugin-mount-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-configs
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /run/secrets/tokens
        name: oidc-token
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hql9z
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: worker-4
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-rbd-plugin-sa
    serviceAccountName: rook-csi-rbd-plugin-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.rbd.csi.ceph.com
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: Directory
      name: plugin-mount-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: Directory
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: ceph-csi-configs
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
        - configMap:
            items:
            - key: csi-mapping-config-json
              path: cluster-mapping.json
            name: rook-ceph-csi-mapping-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - hostPath:
        path: /run/mount
        type: ""
      name: host-run-mount
    - name: oidc-token
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            audience: ceph-csi-kms
            expirationSeconds: 3600
            path: oidc-token
    - name: kube-api-access-hql9z
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:48Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:51Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:51Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:48Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://51095bade7fd623670281f66dd836d1e205ddd68b2ef5e0e0288e32affdf988e
      image: quay.io/cephcsi/cephcsi:v3.10.1
      imageID: docker-pullable://quay.io/cephcsi/cephcsi@sha256:5dd50ad6f3f9a1e8c8186fde0048ea241f056ca755acbeab42f5ebf723313e9c
      lastState: {}
      name: csi-rbdplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:51Z"
    - containerID: docker://29d886e64712f8fe91d556f8bd3510f76d4912278c0b2253c67f33a4071e18d6
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.1
      imageID: docker-pullable://registry.k8s.io/sig-storage/csi-node-driver-registrar@sha256:2cddcc716c1930775228d56b0d2d339358647629701047edfdad5fcdfaf4ebcb
      lastState: {}
      name: driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:50Z"
    hostIP: 10.2.1.186
    phase: Running
    podIP: 10.2.1.186
    podIPs:
    - ip: 10.2.1.186
    qosClass: Burstable
    startTime: "2024-03-13T16:49:48Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: e6b0c970469e81c192a15d7035942d4d744e7c8702672396df2d91952a4d1af9
      cni.projectcalico.org/podIP: 10.42.226.83/32
      cni.projectcalico.org/podIPs: 10.42.226.83/32
    creationTimestamp: "2024-03-13T16:49:57Z"
    generateName: rook-ceph-crashcollector-worker-1-78679dd88c-
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 17.2.6-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: worker-1
      node_name: worker-1
      pod-template-hash: 78679dd88c
      rook_cluster: rook-ceph
    name: rook-ceph-crashcollector-worker-1-78679dd88c-tngfd
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-crashcollector-worker-1-78679dd88c
      uid: fa2e9266-45c2-42d5-9fcb-149f4bd6061b
    resourceVersion: "13085389"
    uid: 59df731c-0a67-44f4-8b2c-f75e601387a5
  spec:
    containers:
    - command:
      - ceph-crash
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v17.2.6
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: ceph-crash
      resources:
        limits:
          cpu: 500m
          memory: 60Mi
        requests:
          cpu: 100m
          memory: 60Mi
      securityContext:
        privileged: false
        runAsGroup: 167
        runAsUser: 167
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /etc/ceph/crash-collector-keyring-store/
        name: rook-ceph-crash-collector-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lm7t5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - /var/lib/ceph/crash/posted
      command:
      - mkdir
      - -p
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: make-container-crash-dir
      resources:
        limits:
          cpu: 500m
          memory: 60Mi
        requests:
          cpu: 100m
          memory: 60Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lm7t5
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: 500m
          memory: 60Mi
        requests:
          cpu: 100m
          memory: 60Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lm7t5
        readOnly: true
    nodeName: worker-1
    nodeSelector:
      kubernetes.io/hostname: worker-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 5
    volumes:
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - name: rook-ceph-crash-collector-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-crash-collector-keyring
    - name: kube-api-access-lm7t5
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:02Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:03Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:03Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:57Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://f6e18fe6c7a958139ef3cb0952f9600a8671b2d471464c94f2436e140c8bd553
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: ceph-crash
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:50:02Z"
    hostIP: 10.2.1.183
    initContainerStatuses:
    - containerID: docker://5b5f72cff972518b9fd1128123649bb64a54de5e96b95e929ba21650477e9042
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: make-container-crash-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://5b5f72cff972518b9fd1128123649bb64a54de5e96b95e929ba21650477e9042
          exitCode: 0
          finishedAt: "2024-03-13T16:50:00Z"
          reason: Completed
          startedAt: "2024-03-13T16:50:00Z"
    - containerID: docker://0341f876b8d07aee293d08fe728e2cbb72afc7ecbebd15aacceadfed6a2c7268
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://0341f876b8d07aee293d08fe728e2cbb72afc7ecbebd15aacceadfed6a2c7268
          exitCode: 0
          finishedAt: "2024-03-13T16:50:01Z"
          reason: Completed
          startedAt: "2024-03-13T16:50:01Z"
    phase: Running
    podIP: 10.42.226.83
    podIPs:
    - ip: 10.42.226.83
    qosClass: Burstable
    startTime: "2024-03-13T16:49:57Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 5f366ee15781fb2c5d95bfea74a84f325a85ca5910ea2b07328e02f24c96dc70
      cni.projectcalico.org/podIP: 10.42.133.209/32
      cni.projectcalico.org/podIPs: 10.42.133.209/32
    creationTimestamp: "2024-03-13T16:50:11Z"
    generateName: rook-ceph-crashcollector-worker-2-686d8549dd-
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 17.2.6-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: worker-2
      node_name: worker-2
      pod-template-hash: 686d8549dd
      rook_cluster: rook-ceph
    name: rook-ceph-crashcollector-worker-2-686d8549dd-hdcfk
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-crashcollector-worker-2-686d8549dd
      uid: d71e0b82-e612-4cea-84fb-45cc7629bcce
    resourceVersion: "13085558"
    uid: 39e56ac3-f393-4c0d-a2da-63a510188aeb
  spec:
    containers:
    - command:
      - ceph-crash
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v17.2.6
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: ceph-crash
      resources:
        limits:
          cpu: 500m
          memory: 60Mi
        requests:
          cpu: 100m
          memory: 60Mi
      securityContext:
        privileged: false
        runAsGroup: 167
        runAsUser: 167
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /etc/ceph/crash-collector-keyring-store/
        name: rook-ceph-crash-collector-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pf59k
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - /var/lib/ceph/crash/posted
      command:
      - mkdir
      - -p
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: make-container-crash-dir
      resources:
        limits:
          cpu: 500m
          memory: 60Mi
        requests:
          cpu: 100m
          memory: 60Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pf59k
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: 500m
          memory: 60Mi
        requests:
          cpu: 100m
          memory: 60Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pf59k
        readOnly: true
    nodeName: worker-2
    nodeSelector:
      kubernetes.io/hostname: worker-2
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - name: rook-ceph-crash-collector-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-crash-collector-keyring
    - name: kube-api-access-pf59k
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:14Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:15Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:15Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:11Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://0d5562454b78cb852cddfa40c9691a499a1f2a7d5d0b31366b017e6faa8cabbb
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: ceph-crash
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:50:14Z"
    hostIP: 10.2.1.184
    initContainerStatuses:
    - containerID: docker://c5a996e3d2bf967d96b360b864ecccdaebd38a5390116fb8b077810de3bb7667
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: make-container-crash-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://c5a996e3d2bf967d96b360b864ecccdaebd38a5390116fb8b077810de3bb7667
          exitCode: 0
          finishedAt: "2024-03-13T16:50:12Z"
          reason: Completed
          startedAt: "2024-03-13T16:50:12Z"
    - containerID: docker://fffb2fc8cf0e490641d42c4b24e6e79ce10f0ffb8a79331afbb4eafa7ad123ea
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://fffb2fc8cf0e490641d42c4b24e6e79ce10f0ffb8a79331afbb4eafa7ad123ea
          exitCode: 0
          finishedAt: "2024-03-13T16:50:13Z"
          reason: Completed
          startedAt: "2024-03-13T16:50:13Z"
    phase: Running
    podIP: 10.42.133.209
    podIPs:
    - ip: 10.42.133.209
    qosClass: Burstable
    startTime: "2024-03-13T16:50:11Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: b64146771d5507d069ff199ef2c88414bd8c15261731a73eb0787491cc60346c
      cni.projectcalico.org/podIP: 10.42.97.210/32
      cni.projectcalico.org/podIPs: 10.42.97.210/32
    creationTimestamp: "2024-03-13T16:49:41Z"
    generateName: rook-ceph-crashcollector-worker-3-5845867477-
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 17.2.6-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: worker-3
      node_name: worker-3
      pod-template-hash: "5845867477"
      rook_cluster: rook-ceph
    name: rook-ceph-crashcollector-worker-3-5845867477-9jw7w
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-crashcollector-worker-3-5845867477
      uid: 3bf658e7-148a-49e0-96c1-9ec09cd43c20
    resourceVersion: "13085404"
    uid: c63420f6-d49c-415a-80f7-a9402e9eb6aa
  spec:
    containers:
    - command:
      - ceph-crash
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v17.2.6
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: ceph-crash
      resources:
        limits:
          cpu: 500m
          memory: 60Mi
        requests:
          cpu: 100m
          memory: 60Mi
      securityContext:
        privileged: false
        runAsGroup: 167
        runAsUser: 167
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /etc/ceph/crash-collector-keyring-store/
        name: rook-ceph-crash-collector-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-j4c9p
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - /var/lib/ceph/crash/posted
      command:
      - mkdir
      - -p
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: make-container-crash-dir
      resources:
        limits:
          cpu: 500m
          memory: 60Mi
        requests:
          cpu: 100m
          memory: 60Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-j4c9p
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: 500m
          memory: 60Mi
        requests:
          cpu: 100m
          memory: 60Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-j4c9p
        readOnly: true
    nodeName: worker-3
    nodeSelector:
      kubernetes.io/hostname: worker-3
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 5
    volumes:
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - name: rook-ceph-crash-collector-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-crash-collector-keyring
    - name: kube-api-access-j4c9p
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:03Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:04Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:04Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:59Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://fa434a401628085147f90c1d74134272930ee5e949611b4cc49bbf0ff5630689
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: ceph-crash
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:50:03Z"
    hostIP: 10.2.1.185
    initContainerStatuses:
    - containerID: docker://65bae7a7e8f810a5923fa0382c7429d9a033311ccca1cc92c1c0dde37f38f4bf
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: make-container-crash-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://65bae7a7e8f810a5923fa0382c7429d9a033311ccca1cc92c1c0dde37f38f4bf
          exitCode: 0
          finishedAt: "2024-03-13T16:50:01Z"
          reason: Completed
          startedAt: "2024-03-13T16:50:01Z"
    - containerID: docker://802023f003c8889529e164f3c9b2e1358e67a3f42c57b23645a2112b5fd7f261
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://802023f003c8889529e164f3c9b2e1358e67a3f42c57b23645a2112b5fd7f261
          exitCode: 0
          finishedAt: "2024-03-13T16:50:02Z"
          reason: Completed
          startedAt: "2024-03-13T16:50:02Z"
    phase: Running
    podIP: 10.42.97.210
    podIPs:
    - ip: 10.42.97.210
    qosClass: Burstable
    startTime: "2024-03-13T16:49:59Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 3033b009456b89557196dab1570a2972395fa15a24a562fc793c6fa142d1d596
      cni.projectcalico.org/podIP: 10.42.38.80/32
      cni.projectcalico.org/podIPs: 10.42.38.80/32
    creationTimestamp: "2024-03-13T16:49:56Z"
    generateName: rook-ceph-crashcollector-worker-4-66d7cd88d8-
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 17.2.6-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: worker-4
      node_name: worker-4
      pod-template-hash: 66d7cd88d8
      rook_cluster: rook-ceph
    name: rook-ceph-crashcollector-worker-4-66d7cd88d8-798qw
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-crashcollector-worker-4-66d7cd88d8
      uid: b6db64ba-a63b-496a-b3ef-f041a9123f8f
    resourceVersion: "13085361"
    uid: da03304a-bd81-4d6a-bd39-92876b2550bc
  spec:
    containers:
    - command:
      - ceph-crash
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v17.2.6
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: ceph-crash
      resources:
        limits:
          cpu: 500m
          memory: 60Mi
        requests:
          cpu: 100m
          memory: 60Mi
      securityContext:
        privileged: false
        runAsGroup: 167
        runAsUser: 167
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /etc/ceph/crash-collector-keyring-store/
        name: rook-ceph-crash-collector-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mfbfl
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - /var/lib/ceph/crash/posted
      command:
      - mkdir
      - -p
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: make-container-crash-dir
      resources:
        limits:
          cpu: 500m
          memory: 60Mi
        requests:
          cpu: 100m
          memory: 60Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mfbfl
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: 500m
          memory: 60Mi
        requests:
          cpu: 100m
          memory: 60Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mfbfl
        readOnly: true
    nodeName: worker-4
    nodeSelector:
      kubernetes.io/hostname: worker-4
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 5
    volumes:
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - name: rook-ceph-crash-collector-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-crash-collector-keyring
    - name: kube-api-access-mfbfl
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:01Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:02Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:02Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:56Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://53b4cf399932d81b90c766999af7cb2c4c4234f53b28e3817964d0a491d17f4a
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: ceph-crash
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:50:01Z"
    hostIP: 10.2.1.186
    initContainerStatuses:
    - containerID: docker://95cfd79ff8d49578f2f50e970ee1b6d3fa7bd73f6ea664239eea54f950dc88b6
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: make-container-crash-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://95cfd79ff8d49578f2f50e970ee1b6d3fa7bd73f6ea664239eea54f950dc88b6
          exitCode: 0
          finishedAt: "2024-03-13T16:49:58Z"
          reason: Completed
          startedAt: "2024-03-13T16:49:58Z"
    - containerID: docker://d6df843ab8775324dd7984ba4a42150da15e9e79be8dbe94ac6442e2e2914443
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://d6df843ab8775324dd7984ba4a42150da15e9e79be8dbe94ac6442e2e2914443
          exitCode: 0
          finishedAt: "2024-03-13T16:50:00Z"
          reason: Completed
          startedAt: "2024-03-13T16:50:00Z"
    phase: Running
    podIP: 10.42.38.80
    podIPs:
    - ip: 10.42.38.80
    qosClass: Burstable
    startTime: "2024-03-13T16:49:56Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 90d88213f32736dd2ce0e4b7a452f2093639074e007c8c7e4b28a8e3b3c186b6
      cni.projectcalico.org/podIP: 10.42.97.205/32
      cni.projectcalico.org/podIPs: 10.42.97.205/32
    creationTimestamp: "2024-03-13T16:49:40Z"
    generateName: rook-ceph-mds-ceph-filesystem-a-6d784b75dd-
    labels:
      app: rook-ceph-mds
      app.kubernetes.io/component: cephfilesystems.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: ceph-filesystem-a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mds
      app.kubernetes.io/part-of: ceph-filesystem
      ceph_daemon_id: ceph-filesystem-a
      ceph_daemon_type: mds
      mds: ceph-filesystem-a
      pod-template-hash: 6d784b75dd
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      rook_file_system: ceph-filesystem
    name: rook-ceph-mds-ceph-filesystem-a-6d784b75dd-l5kjc
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-mds-ceph-filesystem-a-6d784b75dd
      uid: bad5d37a-663b-4bc1-be76-e2f1093cd851
    resourceVersion: "13085130"
    uid: 173d09eb-f541-46bc-9ccf-be2786ad22bf
  spec:
    affinity: {}
    containers:
    - args:
      - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=ceph-filesystem-a
      - --setuser=ceph
      - --setgroup=ceph
      - --foreground
      - --public-addr=$(ROOK_POD_IP)
      command:
      - ceph-mds
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v17.2.6
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - timeout
          - "20"
          - sh
          - -c
          - |
            #!/usr/bin/env bash
            # do not use 'set -e -u' etc. because it is important to only fail this probe when failure is certain
            # spurious failures risk destabilizing ceph or the filesystem

            MDS_ID="ceph-filesystem-a"
            FILESYSTEM_NAME="ceph-filesystem"
            KEYRING="/etc/ceph/keyring-store/keyring"

            outp="$(ceph fs dump --mon-host="$ROOK_CEPH_MON_HOST" --mon-initial-members="$ROOK_CEPH_MON_INITIAL_MEMBERS" --keyring "$KEYRING" --format json)"
            rc=$?
            if [ $rc -ne 0 ]; then
                echo "ceph MDS dump check failed with the following output:"
                echo "$outp"
                echo "passing probe to avoid restarting MDS. cannot determine if MDS is unhealthy. restarting MDS risks destabilizing ceph/filesystem, which is likely unreachable or in error state"
                exit 0
            fi

            # get the active and standby MDS in the fs map
            standbyMds=$(echo "$outp" | jq ".standbys | map(.name) | any(.[]; . == \"$MDS_ID\")")
            activeMds=$(echo "$outp" | jq ".filesystems[] | select(.mdsmap.fs_name == \"$FILESYSTEM_NAME\") | .mdsmap.info | map(.name) | any(.[]; . == \"$MDS_ID\")")

            if [[ $standbyMds == true || $activeMds == true ]]; then
                echo "MDS ID present in MDS map, no need to re-start the container"
                exit 0
            fi

            echo "Error: MDS ID not present in MDS map"
            exit 1
        failureThreshold: 5
        initialDelaySeconds: 30
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 25
      name: mds
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 256Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mds-ceph-filesystem-a-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mds/ceph-ceph-filesystem-a
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cr9wm
        readOnly: true
      workingDir: /var/log/ceph
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-mds.ceph-filesystem-a\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cr9wm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      - /var/lib/ceph/mds/ceph-ceph-filesystem-a
      command:
      - chown
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 256Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mds-ceph-filesystem-a-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mds/ceph-ceph-filesystem-a
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cr9wm
        readOnly: true
    nodeName: worker-3
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 5
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - name: rook-ceph-mds-ceph-filesystem-a-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-mds-ceph-filesystem-a-keyring
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - emptyDir: {}
      name: ceph-daemon-data
    - name: kube-api-access-cr9wm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:42Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:43Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:43Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://f7bbffbb574388d02f95455336d0d47ae42d115d6ace25614f8a35ea6aefa7f7
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: log-collector
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:43Z"
    - containerID: docker://f97288bc2ceb24a01efd8c536f86892029a828a21bca708af970a93f8112a07e
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: mds
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:43Z"
    hostIP: 10.2.1.185
    initContainerStatuses:
    - containerID: docker://151e6898ea9ef0a6fe1fb2fecb2902535af3df8a211fd3a203daa9ca992fb77c
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://151e6898ea9ef0a6fe1fb2fecb2902535af3df8a211fd3a203daa9ca992fb77c
          exitCode: 0
          finishedAt: "2024-03-13T16:49:42Z"
          reason: Completed
          startedAt: "2024-03-13T16:49:42Z"
    phase: Running
    podIP: 10.42.97.205
    podIPs:
    - ip: 10.42.97.205
    qosClass: Burstable
    startTime: "2024-03-13T16:49:40Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 94d69f8cf48e2bc9c50638b06b9ddc19783ceba95fee2a743ffcccb0fa70c45a
      cni.projectcalico.org/podIP: 10.42.226.79/32
      cni.projectcalico.org/podIPs: 10.42.226.79/32
    creationTimestamp: "2024-03-13T16:49:40Z"
    generateName: rook-ceph-mds-ceph-filesystem-b-8459d7cc6-
    labels:
      app: rook-ceph-mds
      app.kubernetes.io/component: cephfilesystems.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: ceph-filesystem-b
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mds
      app.kubernetes.io/part-of: ceph-filesystem
      ceph_daemon_id: ceph-filesystem-b
      ceph_daemon_type: mds
      mds: ceph-filesystem-b
      pod-template-hash: 8459d7cc6
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      rook_file_system: ceph-filesystem
    name: rook-ceph-mds-ceph-filesystem-b-8459d7cc6-cjj5z
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-mds-ceph-filesystem-b-8459d7cc6
      uid: 2ff40c2b-cea0-49ca-9bcc-e0cdb82a7b8f
    resourceVersion: "13084730"
    uid: 8570f553-e589-4f87-a136-b4ce78380ce6
  spec:
    affinity: {}
    containers:
    - args:
      - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=ceph-filesystem-b
      - --setuser=ceph
      - --setgroup=ceph
      - --foreground
      - --public-addr=$(ROOK_POD_IP)
      command:
      - ceph-mds
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v17.2.6
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - timeout
          - "20"
          - sh
          - -c
          - |
            #!/usr/bin/env bash
            # do not use 'set -e -u' etc. because it is important to only fail this probe when failure is certain
            # spurious failures risk destabilizing ceph or the filesystem

            MDS_ID="ceph-filesystem-b"
            FILESYSTEM_NAME="ceph-filesystem"
            KEYRING="/etc/ceph/keyring-store/keyring"

            outp="$(ceph fs dump --mon-host="$ROOK_CEPH_MON_HOST" --mon-initial-members="$ROOK_CEPH_MON_INITIAL_MEMBERS" --keyring "$KEYRING" --format json)"
            rc=$?
            if [ $rc -ne 0 ]; then
                echo "ceph MDS dump check failed with the following output:"
                echo "$outp"
                echo "passing probe to avoid restarting MDS. cannot determine if MDS is unhealthy. restarting MDS risks destabilizing ceph/filesystem, which is likely unreachable or in error state"
                exit 0
            fi

            # get the active and standby MDS in the fs map
            standbyMds=$(echo "$outp" | jq ".standbys | map(.name) | any(.[]; . == \"$MDS_ID\")")
            activeMds=$(echo "$outp" | jq ".filesystems[] | select(.mdsmap.fs_name == \"$FILESYSTEM_NAME\") | .mdsmap.info | map(.name) | any(.[]; . == \"$MDS_ID\")")

            if [[ $standbyMds == true || $activeMds == true ]]; then
                echo "MDS ID present in MDS map, no need to re-start the container"
                exit 0
            fi

            echo "Error: MDS ID not present in MDS map"
            exit 1
        failureThreshold: 5
        initialDelaySeconds: 30
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 25
      name: mds
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 256Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mds-ceph-filesystem-b-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mds/ceph-ceph-filesystem-b
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2p722
        readOnly: true
      workingDir: /var/log/ceph
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-mds.ceph-filesystem-b\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2p722
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      - /var/lib/ceph/mds/ceph-ceph-filesystem-b
      command:
      - chown
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 256Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mds-ceph-filesystem-b-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mds/ceph-ceph-filesystem-b
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2p722
        readOnly: true
    nodeName: worker-1
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 5
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - name: rook-ceph-mds-ceph-filesystem-b-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-mds-ceph-filesystem-b-keyring
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - emptyDir: {}
      name: ceph-daemon-data
    - name: kube-api-access-2p722
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:44Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:45Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:45Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://3581f7810fe3e3ddf8b46f2c7d23e55aaa0e380865277084fa119864d225024b
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: log-collector
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:44Z"
    - containerID: docker://a7ea0d8c13743be6ec7189e4c9c821aedb78927c4903bc53377a9b100f9750e3
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: mds
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:44Z"
    hostIP: 10.2.1.183
    initContainerStatuses:
    - containerID: docker://5a828c40d68b90358bbd8bbcffce735540e37e7127734ee243a872975e985824
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://5a828c40d68b90358bbd8bbcffce735540e37e7127734ee243a872975e985824
          exitCode: 0
          finishedAt: "2024-03-13T16:49:42Z"
          reason: Completed
          startedAt: "2024-03-13T16:49:42Z"
    phase: Running
    podIP: 10.42.226.79
    podIPs:
    - ip: 10.42.226.79
    qosClass: Burstable
    startTime: "2024-03-13T16:49:40Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 4bc5f6d10f6474b1f23f3a19dca5e39eaad1a5a3bd3a0c7461d189db69e9d34f
      cni.projectcalico.org/podIP: 10.42.38.78/32
      cni.projectcalico.org/podIPs: 10.42.38.78/32
      prometheus.io/port: "9283"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-03-13T16:49:40Z"
    generateName: rook-ceph-mgr-a-88759fcf8-
    labels:
      app: rook-ceph-mgr
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mgr
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: a
      ceph_daemon_type: mgr
      instance: a
      mgr: a
      mgr_role: standby
      pod-template-hash: 88759fcf8
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mgr-a-88759fcf8-b6g86
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-mgr-a-88759fcf8
      uid: 6ab2404d-0e0a-4d0a-8f3f-f17ffe6ede21
    resourceVersion: "13085647"
    uid: a4bea652-3deb-48e9-ac9d-47bdb35c2619
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app: rook-ceph-mgr
              rook_cluster: rook-ceph
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=a
      - --setuser=ceph
      - --setgroup=ceph
      - --client-mount-uid=0
      - --client-mount-gid=0
      - --foreground
      - --public-addr=$(ROOK_POD_IP)
      command:
      - ceph-mgr
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v17.2.6
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: ROOK_OPERATOR_NAMESPACE
        value: rook-ceph
      - name: ROOK_CEPH_CLUSTER_CRD_VERSION
        value: v1
      - name: ROOK_CEPH_CLUSTER_CRD_NAME
        value: rook-ceph
      - name: CEPH_ARGS
        value: --mon-host $(ROOK_CEPH_MON_HOST) --keyring /etc/ceph/keyring-store/keyring
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with the
            following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: mgr
      ports:
      - containerPort: 6800
        name: mgr
        protocol: TCP
      - containerPort: 9283
        name: http-metrics
        protocol: TCP
      - containerPort: 8443
        name: dashboard
        protocol: TCP
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 500m
          memory: 512Mi
      securityContext:
        privileged: false
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with the
            following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
        failureThreshold: 6
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mgr-a-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mgr/ceph-a
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2hscg
        readOnly: true
      workingDir: /var/log/ceph
    - args:
      - ceph
      - mgr
      - watch-active
      env:
      - name: ROOK_CLUSTER_ID
        value: 02274ff6-97d3-4d09-b941-31edb9bdad99
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CEPH_USERNAME
        valueFrom:
          secretKeyRef:
            key: ceph-username
            name: rook-ceph-mon
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: ROOK_DASHBOARD_ENABLED
        value: "true"
      - name: ROOK_MONITORING_ENABLED
        value: "false"
      - name: ROOK_UPDATE_INTERVAL
        value: 15s
      - name: ROOK_DAEMON_NAME
        value: a
      - name: ROOK_CEPH_VERSION
        value: ceph version 17.2.6-0 quincy
      image: rook/ceph:v1.13.1
      imagePullPolicy: IfNotPresent
      name: watch-active
      resources:
        limits:
          cpu: 500m
          memory: 100Mi
        requests:
          cpu: 100m
          memory: 40Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook-ceph-mon
        name: ceph-admin-secret
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2hscg
        readOnly: true
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-mgr.a\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2hscg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      - /var/lib/ceph/mgr/ceph-a
      command:
      - chown
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 500m
          memory: 512Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mgr-a-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mgr/ceph-a
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2hscg
        readOnly: true
    nodeName: worker-4
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-mgr
    serviceAccountName: rook-ceph-mgr
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 5
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - name: rook-ceph-mgr-a-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-mgr-a-keyring
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - emptyDir: {}
      name: ceph-daemon-data
    - name: ceph-admin-secret
      secret:
        defaultMode: 420
        items:
        - key: ceph-secret
          path: secret.keyring
        secretName: rook-ceph-mon
    - name: kube-api-access-2hscg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:53Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:12Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:12Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:49Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://b6576ca903beae2daf4e4b7a16c85cafedeb16f7286b7c84bbdc6907e89e8255
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: log-collector
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:55Z"
    - containerID: docker://a541387b74ffb4a15c568a120d43df483800c0dfd7eceb310d3ddee88963fb78
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: mgr
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:54Z"
    - containerID: docker://6ba9b6173f7f5cb4a47c0a0859f62abefea7be5c67612a7a02ebd22113f8cce7
      image: rook/ceph:v1.13.1
      imageID: docker-pullable://rook/ceph@sha256:bf7833f0b3a65a71be36c7a87b83fb22b5df78dba058e4401169cdabe0b09e05
      lastState: {}
      name: watch-active
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:54Z"
    hostIP: 10.2.1.186
    initContainerStatuses:
    - containerID: docker://4bd259af903f5470e79cf302f14cb04df020b5bbf5ccca8f5c5787a8591da811
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://4bd259af903f5470e79cf302f14cb04df020b5bbf5ccca8f5c5787a8591da811
          exitCode: 0
          finishedAt: "2024-03-13T16:49:52Z"
          reason: Completed
          startedAt: "2024-03-13T16:49:52Z"
    phase: Running
    podIP: 10.42.38.78
    podIPs:
    - ip: 10.42.38.78
    qosClass: Burstable
    startTime: "2024-03-13T16:49:49Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 31532c331756dfeffe3704484986d73085a4f944713eeb315f8d6e9c39652a60
      cni.projectcalico.org/podIP: 10.42.226.82/32
      cni.projectcalico.org/podIPs: 10.42.226.82/32
      prometheus.io/port: "9283"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-03-13T16:49:40Z"
    generateName: rook-ceph-mgr-b-684bb98899-
    labels:
      app: rook-ceph-mgr
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: b
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mgr
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: b
      ceph_daemon_type: mgr
      instance: b
      mgr: b
      mgr_role: active
      pod-template-hash: 684bb98899
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mgr-b-684bb98899-5c87w
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-mgr-b-684bb98899
      uid: 30e58d80-cae0-4fe6-bd3f-c6c0cefe4ce9
    resourceVersion: "13085515"
    uid: ca94ce9b-cba5-4be3-b908-79b8a2436e1d
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app: rook-ceph-mgr
              rook_cluster: rook-ceph
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=b
      - --setuser=ceph
      - --setgroup=ceph
      - --client-mount-uid=0
      - --client-mount-gid=0
      - --foreground
      - --public-addr=$(ROOK_POD_IP)
      command:
      - ceph-mgr
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v17.2.6
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: ROOK_OPERATOR_NAMESPACE
        value: rook-ceph
      - name: ROOK_CEPH_CLUSTER_CRD_VERSION
        value: v1
      - name: ROOK_CEPH_CLUSTER_CRD_NAME
        value: rook-ceph
      - name: CEPH_ARGS
        value: --mon-host $(ROOK_CEPH_MON_HOST) --keyring /etc/ceph/keyring-store/keyring
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.b.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with the
            following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: mgr
      ports:
      - containerPort: 6800
        name: mgr
        protocol: TCP
      - containerPort: 9283
        name: http-metrics
        protocol: TCP
      - containerPort: 8443
        name: dashboard
        protocol: TCP
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 500m
          memory: 512Mi
      securityContext:
        privileged: false
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.b.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with the
            following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
        failureThreshold: 6
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mgr-b-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mgr/ceph-b
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6xr7w
        readOnly: true
      workingDir: /var/log/ceph
    - args:
      - ceph
      - mgr
      - watch-active
      env:
      - name: ROOK_CLUSTER_ID
        value: 02274ff6-97d3-4d09-b941-31edb9bdad99
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CEPH_USERNAME
        valueFrom:
          secretKeyRef:
            key: ceph-username
            name: rook-ceph-mon
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: ROOK_DASHBOARD_ENABLED
        value: "true"
      - name: ROOK_MONITORING_ENABLED
        value: "false"
      - name: ROOK_UPDATE_INTERVAL
        value: 15s
      - name: ROOK_DAEMON_NAME
        value: b
      - name: ROOK_CEPH_VERSION
        value: ceph version 17.2.6-0 quincy
      image: rook/ceph:v1.13.1
      imagePullPolicy: IfNotPresent
      name: watch-active
      resources:
        limits:
          cpu: 500m
          memory: 100Mi
        requests:
          cpu: 100m
          memory: 40Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook-ceph-mon
        name: ceph-admin-secret
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6xr7w
        readOnly: true
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-mgr.b\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6xr7w
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      - /var/lib/ceph/mgr/ceph-b
      command:
      - chown
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 500m
          memory: 512Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mgr-b-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mgr/ceph-b
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6xr7w
        readOnly: true
    nodeName: worker-1
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-mgr
    serviceAccountName: rook-ceph-mgr
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 5
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - name: rook-ceph-mgr-b-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-mgr-b-keyring
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - emptyDir: {}
      name: ceph-daemon-data
    - name: ceph-admin-secret
      secret:
        defaultMode: 420
        items:
        - key: ceph-secret
          path: secret.keyring
        secretName: rook-ceph-mon
    - name: kube-api-access-6xr7w
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:12Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:12Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://c707391ee000f0d4ca7d71e073b124a9622c671059f43db42065267ebddbc22b
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: log-collector
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:56Z"
    - containerID: docker://43b06c10ab391e855b0fabd86273004210080d42e2b8b839b8b2e84d0c076583
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: mgr
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:55Z"
    - containerID: docker://df8fe3c5eb9a06286de8851cba453b852560656ffcd9fb98bc549ae2ef1d20ad
      image: rook/ceph:v1.13.1
      imageID: docker-pullable://rook/ceph@sha256:bf7833f0b3a65a71be36c7a87b83fb22b5df78dba058e4401169cdabe0b09e05
      lastState: {}
      name: watch-active
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:56Z"
    hostIP: 10.2.1.183
    initContainerStatuses:
    - containerID: docker://49e4c3f4b3b46e99a6864af55c5ad6b8aded979ec7a363b0badda656be3a1ed0
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://49e4c3f4b3b46e99a6864af55c5ad6b8aded979ec7a363b0badda656be3a1ed0
          exitCode: 0
          finishedAt: "2024-03-13T16:49:53Z"
          reason: Completed
          startedAt: "2024-03-13T16:49:53Z"
    phase: Running
    podIP: 10.42.226.82
    podIPs:
    - ip: 10.42.226.82
    qosClass: Burstable
    startTime: "2024-03-13T16:49:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 121311db57ab500e652a4473df70cb45eb1e1c1d0b4b31388eaf70d68c2c0cd1
      cni.projectcalico.org/podIP: 10.42.97.207/32
      cni.projectcalico.org/podIPs: 10.42.97.207/32
    creationTimestamp: "2024-03-13T16:49:40Z"
    generateName: rook-ceph-mon-a-5c7dcd9676-
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: a
      ceph_daemon_type: mon
      mon: a
      mon_cluster: rook-ceph
      pod-template-hash: 5c7dcd9676
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-a-5c7dcd9676-4l8gc
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-mon-a-5c7dcd9676
      uid: 6aae3a45-fcf5-458a-9f3d-0f2122ed061c
    resourceVersion: "13085447"
    uid: 1d354e56-c434-411d-9bfd-9eb26334c18f
  spec:
    affinity: {}
    containers:
    - args:
      - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=a
      - --setuser=ceph
      - --setgroup=ceph
      - --foreground
      - --public-addr=10.43.110.147
      - --setuser-match-path=/var/lib/ceph/mon/ceph-a/store.db
      - --public-bind-addr=$(ROOK_POD_IP)
      command:
      - ceph-mon
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v17.2.6
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with the
            following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: mon
      ports:
      - containerPort: 3300
        name: tcp-msgr2
        protocol: TCP
      - containerPort: 6789
        name: tcp-msgr1
        protocol: TCP
      resources:
        limits:
          cpu: 1500m
          memory: 1536Mi
        requests:
          cpu: 350m
          memory: 512Mi
      securityContext:
        privileged: false
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with the
            following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
        failureThreshold: 6
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-a
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-df89v
        readOnly: true
      workingDir: /var/log/ceph
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-mon.a\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-df89v
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      - /var/lib/ceph/mon/ceph-a
      command:
      - chown
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: 1500m
          memory: 1536Mi
        requests:
          cpu: 350m
          memory: 512Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-a
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-df89v
        readOnly: true
    - args:
      - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=a
      - --setuser=ceph
      - --setgroup=ceph
      - --public-addr=10.43.110.147
      - --mkfs
      command:
      - ceph-mon
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v17.2.6
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: init-mon-fs
      resources:
        limits:
          cpu: 1500m
          memory: 1536Mi
        requests:
          cpu: 350m
          memory: 512Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-a
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-df89v
        readOnly: true
    nodeName: worker-3
    nodeSelector:
      kubernetes.io/hostname: worker-3
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - name: rook-ceph-mons-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-mons-keyring
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /var/lib/rook/mon-a/data
        type: ""
      name: ceph-daemon-data
    - name: kube-api-access-df89v
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:10Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:10Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:48Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://72c8d92cd1f90e1b7a1d8479fdf15627bffbfc1af9f213bd7b2ed9e13d97de8e
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: log-collector
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:53Z"
    - containerID: docker://44ecda54a3bf690a680090c82b582cb4e7db2384c1cf6dc1b874afd2ff8f89cf
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: mon
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:52Z"
    hostIP: 10.2.1.185
    initContainerStatuses:
    - containerID: docker://fc328d4c8f43552fe4ccc044f67faeb7efa579e4ff26e4386ee6210817c13b6a
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://fc328d4c8f43552fe4ccc044f67faeb7efa579e4ff26e4386ee6210817c13b6a
          exitCode: 0
          finishedAt: "2024-03-13T16:49:50Z"
          reason: Completed
          startedAt: "2024-03-13T16:49:50Z"
    - containerID: docker://0bd5fb381daa4a19d5df07c38fc5b3004f86342f6887fa0397202bd631797a28
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: init-mon-fs
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://0bd5fb381daa4a19d5df07c38fc5b3004f86342f6887fa0397202bd631797a28
          exitCode: 0
          finishedAt: "2024-03-13T16:49:51Z"
          reason: Completed
          startedAt: "2024-03-13T16:49:51Z"
    phase: Running
    podIP: 10.42.97.207
    podIPs:
    - ip: 10.42.97.207
    qosClass: Burstable
    startTime: "2024-03-13T16:49:48Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: dd3f34f6a4fd340b544ea4351b2ec83f8d433f5c37be1fa45385cb07c9d74f8f
      cni.projectcalico.org/podIP: 10.42.226.80/32
      cni.projectcalico.org/podIPs: 10.42.226.80/32
    creationTimestamp: "2024-03-13T16:49:40Z"
    generateName: rook-ceph-mon-c-59cdc6599b-
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: c
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: c
      ceph_daemon_type: mon
      mon: c
      mon_cluster: rook-ceph
      pod-template-hash: 59cdc6599b
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-c-59cdc6599b-j728f
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-mon-c-59cdc6599b
      uid: 5c071ef8-7a7b-456b-8c8a-97a77bb945b4
    resourceVersion: "13085375"
    uid: 29125d61-3bbc-4bc4-90ed-be5749f45b78
  spec:
    affinity: {}
    containers:
    - args:
      - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=c
      - --setuser=ceph
      - --setgroup=ceph
      - --foreground
      - --public-addr=10.43.221.137
      - --setuser-match-path=/var/lib/ceph/mon/ceph-c/store.db
      - --public-bind-addr=$(ROOK_POD_IP)
      command:
      - ceph-mon
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v17.2.6
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.c.asok mon_status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with the
            following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: mon
      ports:
      - containerPort: 3300
        name: tcp-msgr2
        protocol: TCP
      - containerPort: 6789
        name: tcp-msgr1
        protocol: TCP
      resources:
        limits:
          cpu: 1500m
          memory: 1536Mi
        requests:
          cpu: 350m
          memory: 512Mi
      securityContext:
        privileged: false
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.c.asok mon_status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with the
            following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
        failureThreshold: 6
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-c
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-68j67
        readOnly: true
      workingDir: /var/log/ceph
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-mon.c\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-68j67
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      - /var/lib/ceph/mon/ceph-c
      command:
      - chown
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: 1500m
          memory: 1536Mi
        requests:
          cpu: 350m
          memory: 512Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-c
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-68j67
        readOnly: true
    - args:
      - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=c
      - --setuser=ceph
      - --setgroup=ceph
      - --public-addr=10.43.221.137
      - --mkfs
      command:
      - ceph-mon
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v17.2.6
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: init-mon-fs
      resources:
        limits:
          cpu: 1500m
          memory: 1536Mi
        requests:
          cpu: 350m
          memory: 512Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-c
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-68j67
        readOnly: true
    nodeName: worker-1
    nodeSelector:
      kubernetes.io/hostname: worker-1
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - name: rook-ceph-mons-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-mons-keyring
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /var/lib/rook/mon-c/data
        type: ""
      name: ceph-daemon-data
    - name: kube-api-access-68j67
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:45Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:03Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:03Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://88a6a141bd8bf6f4580c36a584818a642da0b5f60bbba3429db2d543554061b5
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: log-collector
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:45Z"
    - containerID: docker://c31b84ee21ae32e51a9fbe878e643f105c06a5e8633158b43de79283fdca1e40
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: mon
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:45Z"
    hostIP: 10.2.1.183
    initContainerStatuses:
    - containerID: docker://93399d27ffeb8658c37ed83158212e8e17e00bb46f025e2a030a5003379e4eb9
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://93399d27ffeb8658c37ed83158212e8e17e00bb46f025e2a030a5003379e4eb9
          exitCode: 0
          finishedAt: "2024-03-13T16:49:43Z"
          reason: Completed
          startedAt: "2024-03-13T16:49:43Z"
    - containerID: docker://556a569f5d8698516d78318d5aeda0552d552d0e5c28993f162952258f8a01fd
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: init-mon-fs
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://556a569f5d8698516d78318d5aeda0552d552d0e5c28993f162952258f8a01fd
          exitCode: 0
          finishedAt: "2024-03-13T16:49:44Z"
          reason: Completed
          startedAt: "2024-03-13T16:49:44Z"
    phase: Running
    podIP: 10.42.226.80
    podIPs:
    - ip: 10.42.226.80
    qosClass: Burstable
    startTime: "2024-03-13T16:49:40Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: c1c305d8832853283abefd6ddcb5c007c7b7bd783d83c1b43498da8d65ec896e
      cni.projectcalico.org/podIP: 10.42.38.77/32
      cni.projectcalico.org/podIPs: 10.42.38.77/32
    creationTimestamp: "2024-03-13T16:49:40Z"
    generateName: rook-ceph-mon-e-76cdf858cf-
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: e
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: e
      ceph_daemon_type: mon
      mon: e
      mon_cluster: rook-ceph
      pod-template-hash: 76cdf858cf
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-e-76cdf858cf-vqdqz
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-mon-e-76cdf858cf
      uid: ea06ad40-5e4a-4076-9e75-45e550b5083e
    resourceVersion: "13085470"
    uid: cf8d9993-1f9b-419d-ba3c-9b3c731ec34e
  spec:
    affinity: {}
    containers:
    - args:
      - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=e
      - --setuser=ceph
      - --setgroup=ceph
      - --foreground
      - --public-addr=10.43.19.85
      - --setuser-match-path=/var/lib/ceph/mon/ceph-e/store.db
      - --public-bind-addr=$(ROOK_POD_IP)
      command:
      - ceph-mon
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v17.2.6
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.e.asok mon_status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with the
            following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: mon
      ports:
      - containerPort: 3300
        name: tcp-msgr2
        protocol: TCP
      - containerPort: 6789
        name: tcp-msgr1
        protocol: TCP
      resources:
        limits:
          cpu: 1500m
          memory: 1536Mi
        requests:
          cpu: 350m
          memory: 512Mi
      securityContext:
        privileged: false
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.e.asok mon_status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with the
            following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
        failureThreshold: 6
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-e
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6nwqd
        readOnly: true
      workingDir: /var/log/ceph
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-mon.e\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6nwqd
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      - /var/lib/ceph/mon/ceph-e
      command:
      - chown
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: 1500m
          memory: 1536Mi
        requests:
          cpu: 350m
          memory: 512Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-e
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6nwqd
        readOnly: true
    - args:
      - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=e
      - --setuser=ceph
      - --setgroup=ceph
      - --public-addr=10.43.19.85
      - --mkfs
      command:
      - ceph-mon
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v17.2.6
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: init-mon-fs
      resources:
        limits:
          cpu: 1500m
          memory: 1536Mi
        requests:
          cpu: 350m
          memory: 512Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-e
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6nwqd
        readOnly: true
    nodeName: worker-4
    nodeSelector:
      kubernetes.io/hostname: worker-4
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - name: rook-ceph-mons-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-mons-keyring
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /var/lib/rook/mon-e/data
        type: ""
      name: ceph-daemon-data
    - name: kube-api-access-6nwqd
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:53Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:11Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:11Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:48Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://f267cd7dde69fcf3583d05d48f2cb310aaa373b77498287024da26b79a7bfc73
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: log-collector
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:54Z"
    - containerID: docker://5a273083ef34264d855d1246466181a54023968f1095a08c5eb84b9d2185ad2a
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: mon
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:54Z"
    hostIP: 10.2.1.186
    initContainerStatuses:
    - containerID: docker://956eda8853a921ccbc30c7162a2fb5b0878875a33ee52b087db96130cc3534fc
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://956eda8853a921ccbc30c7162a2fb5b0878875a33ee52b087db96130cc3534fc
          exitCode: 0
          finishedAt: "2024-03-13T16:49:51Z"
          reason: Completed
          startedAt: "2024-03-13T16:49:51Z"
    - containerID: docker://2f60ecb453e983a7f771eebdd46110642fe999db4af070e8b3d5382488d302b5
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: init-mon-fs
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://2f60ecb453e983a7f771eebdd46110642fe999db4af070e8b3d5382488d302b5
          exitCode: 0
          finishedAt: "2024-03-13T16:49:52Z"
          reason: Completed
          startedAt: "2024-03-13T16:49:52Z"
    phase: Running
    podIP: 10.42.38.77
    podIPs:
    - ip: 10.42.38.77
    qosClass: Burstable
    startTime: "2024-03-13T16:49:48Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: c1860d6fff2cfb56bb8023cf362eaf63141784c593b050a41dd24255d9cc7593
      cni.projectcalico.org/podIP: 10.42.38.76/32
      cni.projectcalico.org/podIPs: 10.42.38.76/32
    creationTimestamp: "2024-03-13T16:49:43Z"
    generateName: rook-ceph-operator-54bbffd5dd-
    labels:
      app: rook-ceph-operator
      helm.sh/chart: rook-ceph-v1.13.1
      pod-template-hash: 54bbffd5dd
    name: rook-ceph-operator-54bbffd5dd-q7q96
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-operator-54bbffd5dd
      uid: 569a024d-fd89-4b27-b746-af72a9bbdcfa
    resourceVersion: "13085212"
    uid: c8b82d02-497e-4ac7-b33e-196378ba5108
  spec:
    containers:
    - args:
      - ceph
      - operator
      env:
      - name: ROOK_CURRENT_NAMESPACE_ONLY
        value: "false"
      - name: ROOK_HOSTPATH_REQUIRES_PRIVILEGED
        value: "false"
      - name: ROOK_DISABLE_DEVICE_HOTPLUG
        value: "false"
      - name: ROOK_DISCOVER_DEVICES_INTERVAL
        value: 60m
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: rook/ceph:v1.13.1
      imagePullPolicy: IfNotPresent
      name: rook-ceph-operator
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 128Mi
      securityContext:
        capabilities:
          drop:
          - ALL
        runAsGroup: 2016
        runAsNonRoot: true
        runAsUser: 2016
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-config
      - mountPath: /etc/ceph
        name: default-config-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8nll7
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker-4
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-system
    serviceAccountName: rook-ceph-system
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 5
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: rook-config
    - emptyDir: {}
      name: default-config-dir
    - name: kube-api-access-8nll7
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:47Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:51Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:51Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:47Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://2ebc3c58a3149ff9c21a744023a34baa85d9548ccdcf9d5cdcceebce86c0ccc4
      image: rook/ceph:v1.13.1
      imageID: docker-pullable://rook/ceph@sha256:bf7833f0b3a65a71be36c7a87b83fb22b5df78dba058e4401169cdabe0b09e05
      lastState: {}
      name: rook-ceph-operator
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:50Z"
    hostIP: 10.2.1.186
    phase: Running
    podIP: 10.42.38.76
    podIPs:
    - ip: 10.42.38.76
    qosClass: Burstable
    startTime: "2024-03-13T16:49:47Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: c3a647e209fda2c0d7574ea18cd8f50c27faa5b1c4520e11de16fef245c33044
      cni.projectcalico.org/podIP: 10.42.97.208/32
      cni.projectcalico.org/podIPs: 10.42.97.208/32
    creationTimestamp: "2024-03-13T16:49:41Z"
    generateName: rook-ceph-osd-0-7b44966dc6-
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "0"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "0"
      ceph_daemon_id: "0"
      ceph_daemon_type: osd
      device-class: ssd
      failure-domain: worker-3
      osd: "0"
      osd-store: bluestore
      pod-template-hash: 7b44966dc6
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: worker-3
      topology-location-root: default
    name: rook-ceph-osd-0-7b44966dc6-pdwkb
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-osd-0-7b44966dc6
      uid: d2f34659-5ba4-4d6a-8fcd-6cfa687f8feb
    resourceVersion: "13085847"
    uid: 7339d6a2-a590-4e5b-acd7-6398f71ac3ea
  spec:
    affinity: {}
    containers:
    - args:
      - --foreground
      - --id
      - "0"
      - --fsid
      - 6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
      - --setuser
      - ceph
      - --setgroup
      - ceph
      - --crush-location=root=default host=worker-3
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --ms-learn-addr-from-peer=false
      command:
      - ceph-osd
      env:
      - name: ROOK_NODE_NAME
        value: worker-3
      - name: ROOK_CLUSTER_ID
        value: 02274ff6-97d3-4d09-b941-31edb9bdad99
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: worker-3
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v17.2.6
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_OSD_RESTART_INTERVAL
        value: "0"
      - name: ROOK_OSD_UUID
        value: 9b721017-bd88-40c6-84cb-3ef9a79e3a0c
      - name: ROOK_OSD_ID
        value: "0"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdb
      - name: ROOK_CV_MODE
        value: raw
      - name: ROOK_OSD_DEVICE_CLASS
        value: ssd
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.0.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: osd
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 512Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.0.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 720
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-0
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8qgpr
        readOnly: true
      workingDir: /var/log/ceph
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-osd.0\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8qgpr
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - /bin/bash
      - -c
      - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables are
        unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4\nOSD_UUID=9b721017-bd88-40c6-84cb-3ef9a79e3a0c\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
        \"ceph.conf\" must have the \"fsid\" global configuration to activate encrypted
        OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
        This limitation will be removed later. After that, we can remove this\n# fsid
        injection code. Probably a good time is when to remove Quincy support.\n#
        https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
        /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
        = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif not
        config.has_section('global'):\n    config['global'] = {}\n\nif not config.has_option('global','fsid'):\n
        \   config['global']['fsid'] = '$CEPH_FSID'\n\nwith open('/etc/ceph/ceph.conf',
        'w') as configfile:\n    config.write(configfile)\n\"\n\n# create new keyring\nceph
        -n client.admin auth get-or-create osd.\"$OSD_ID\" mon 'allow profile osd'
        mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
        \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary directory\n\t#
        this is needed because when the init container exits, the tmpfs goes away
        and its content with it\n\t# this will result in the emptydir to be empty
        when accessed by the main osd container\n\tcp --verbose --no-dereference \"$OSD_DATA_DIR\"/*
        \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't need it anymore\n\tumount
        \"$OSD_DATA_DIR\"\n\n\t# copy back the content of the tmpfs into the original
        osd directory\n\tcp --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t#
        retain ownership of files to the ceph user/group\n\tchown --verbose --recursive
        ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm --recursive
        --force \"$TMP_DIR\"\nelse\n\t# 'ceph-volume raw list' (which the osd-prepare
        job uses to report OSDs on nodes)\n\t#  returns user-friendly device names
        which can change when systems reboot. To\n\t# keep OSD pods from crashing
        repeatedly after a reboot, we need to check if the\n\t# block device we have
        is still correct, and if it isn't correct, we need to\n\t# scan all the disks
        to find the right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device()
        {\n\t\t# jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
        python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
        sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
        == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device: '
        + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
        \ # don't keep processing once the disk is found\nsys.exit('no disk found
        with OSD ID $OSD_ID')\n\"\n\t}\n\n\tceph-volume raw list \"$DEVICE\" > \"$OSD_LIST\"\n\tcat
        \"$OSD_LIST\"\n\n\tif ! find_device < \"$OSD_LIST\"; then\n\t\tceph-volume
        raw list > \"$OSD_LIST\"\n\t\tcat \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device
        < \"$OSD_LIST\")\"\n\tfi\n\t[[ -z \"$DEVICE\" ]] && { echo \"no device\" ;
        exit 1 ; }\n\n\t# If a kernel device name change happens and a block device
        file\n\t# in the OSD directory becomes missing, this OSD fails to start\n\t#
        continuously. This problem can be resolved by confirming\n\t# the validity
        of the device file and recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
        [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ; then\n\t\trm
        $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports bluestore
        so we don't need to pass a store flag\n\tceph-volume raw activate --device
        \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
      env:
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdb
      - name: ROOK_METADATA_DEVICE
      - name: ROOK_WAL_DEVICE
      - name: ROOK_OSD_ID
        value: "0"
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: activate
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 512Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-0
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/temp-ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/admin-keyring-store/
        name: rook-ceph-admin-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8qgpr
        readOnly: true
    - args:
      - bluefs-bdev-expand
      - --path
      - /var/lib/ceph/osd/ceph-0
      command:
      - ceph-bluestore-tool
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: expand-bluefs
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 512Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-0
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8qgpr
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 512Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-0
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8qgpr
        readOnly: true
    nodeName: worker-3
    nodeSelector:
      kubernetes.io/hostname: worker-3
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: run-udev
    - hostPath:
        path: /var/lib/rook/rook-ceph/6f3631ca-fa9d-4b5e-ba5c-965ec65214c4_9b721017-bd88-40c6-84cb-3ef9a79e3a0c
        type: DirectoryOrCreate
      name: activate-osd
    - name: rook-ceph-admin-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-admin-keyring
    - name: kube-api-access-8qgpr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:33Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:50Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:50Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:49Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://b22a8937bf1d364b8048efc5ab5dee9bfaaa089dab51f14633ca570043f3207e
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: log-collector
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:50:33Z"
    - containerID: docker://fb2931965ea53569714d80a39fcf7fe03980893d14fa0a0ca4616d6a1356be26
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: osd
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:50:33Z"
    hostIP: 10.2.1.185
    initContainerStatuses:
    - containerID: docker://eaaa0844ddcc6661db7e86dadda32aa3588f5bca634ff4d57e8a0bd900f7b7a3
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: activate
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://eaaa0844ddcc6661db7e86dadda32aa3588f5bca634ff4d57e8a0bd900f7b7a3
          exitCode: 0
          finishedAt: "2024-03-13T16:50:13Z"
          reason: Completed
          startedAt: "2024-03-13T16:49:51Z"
    - containerID: docker://9bff26cb119069a51070dc6a4542f500ac16554b1efa2764fcc1529dc5b87d0e
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: expand-bluefs
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://9bff26cb119069a51070dc6a4542f500ac16554b1efa2764fcc1529dc5b87d0e
          exitCode: 0
          finishedAt: "2024-03-13T16:50:31Z"
          reason: Completed
          startedAt: "2024-03-13T16:50:13Z"
    - containerID: docker://8aec3f295f9642ef5e007814c4add0ac8fb4739deecfc162bc65a50be26009a6
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://8aec3f295f9642ef5e007814c4add0ac8fb4739deecfc162bc65a50be26009a6
          exitCode: 0
          finishedAt: "2024-03-13T16:50:32Z"
          reason: Completed
          startedAt: "2024-03-13T16:50:32Z"
    phase: Running
    podIP: 10.42.97.208
    podIPs:
    - ip: 10.42.97.208
    qosClass: Burstable
    startTime: "2024-03-13T16:49:49Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 15999bba33790cee8ae36239ecbbd757108630b600b8ce648fdc623be71e1484
      cni.projectcalico.org/podIP: 10.42.226.81/32
      cni.projectcalico.org/podIPs: 10.42.226.81/32
    creationTimestamp: "2024-03-13T16:49:41Z"
    generateName: rook-ceph-osd-1-78f8d45768-
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "1"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "1"
      ceph_daemon_id: "1"
      ceph_daemon_type: osd
      device-class: ssd
      failure-domain: worker-1
      osd: "1"
      osd-store: bluestore
      pod-template-hash: 78f8d45768
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: worker-1
      topology-location-root: default
    name: rook-ceph-osd-1-78f8d45768-754sv
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-osd-1-78f8d45768
      uid: f1a3c3e2-9068-4b2c-b0c3-6339ec9e38e4
    resourceVersion: "13085852"
    uid: 45732365-9fb6-41ae-84d4-84215afd71eb
  spec:
    affinity: {}
    containers:
    - args:
      - --foreground
      - --id
      - "1"
      - --fsid
      - 6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
      - --setuser
      - ceph
      - --setgroup
      - ceph
      - --crush-location=root=default host=worker-1
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --ms-learn-addr-from-peer=false
      command:
      - ceph-osd
      env:
      - name: ROOK_NODE_NAME
        value: worker-1
      - name: ROOK_CLUSTER_ID
        value: 02274ff6-97d3-4d09-b941-31edb9bdad99
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: worker-1
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v17.2.6
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_OSD_RESTART_INTERVAL
        value: "0"
      - name: ROOK_OSD_UUID
        value: 098de1f8-d466-4e51-aef3-874dee214609
      - name: ROOK_OSD_ID
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdb
      - name: ROOK_CV_MODE
        value: raw
      - name: ROOK_OSD_DEVICE_CLASS
        value: ssd
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.1.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: osd
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 512Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.1.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 720
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-1
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lrdqc
        readOnly: true
      workingDir: /var/log/ceph
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-osd.1\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lrdqc
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - /bin/bash
      - -c
      - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables are
        unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4\nOSD_UUID=098de1f8-d466-4e51-aef3-874dee214609\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
        \"ceph.conf\" must have the \"fsid\" global configuration to activate encrypted
        OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
        This limitation will be removed later. After that, we can remove this\n# fsid
        injection code. Probably a good time is when to remove Quincy support.\n#
        https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
        /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
        = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif not
        config.has_section('global'):\n    config['global'] = {}\n\nif not config.has_option('global','fsid'):\n
        \   config['global']['fsid'] = '$CEPH_FSID'\n\nwith open('/etc/ceph/ceph.conf',
        'w') as configfile:\n    config.write(configfile)\n\"\n\n# create new keyring\nceph
        -n client.admin auth get-or-create osd.\"$OSD_ID\" mon 'allow profile osd'
        mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
        \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary directory\n\t#
        this is needed because when the init container exits, the tmpfs goes away
        and its content with it\n\t# this will result in the emptydir to be empty
        when accessed by the main osd container\n\tcp --verbose --no-dereference \"$OSD_DATA_DIR\"/*
        \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't need it anymore\n\tumount
        \"$OSD_DATA_DIR\"\n\n\t# copy back the content of the tmpfs into the original
        osd directory\n\tcp --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t#
        retain ownership of files to the ceph user/group\n\tchown --verbose --recursive
        ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm --recursive
        --force \"$TMP_DIR\"\nelse\n\t# 'ceph-volume raw list' (which the osd-prepare
        job uses to report OSDs on nodes)\n\t#  returns user-friendly device names
        which can change when systems reboot. To\n\t# keep OSD pods from crashing
        repeatedly after a reboot, we need to check if the\n\t# block device we have
        is still correct, and if it isn't correct, we need to\n\t# scan all the disks
        to find the right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device()
        {\n\t\t# jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
        python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
        sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
        == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device: '
        + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
        \ # don't keep processing once the disk is found\nsys.exit('no disk found
        with OSD ID $OSD_ID')\n\"\n\t}\n\n\tceph-volume raw list \"$DEVICE\" > \"$OSD_LIST\"\n\tcat
        \"$OSD_LIST\"\n\n\tif ! find_device < \"$OSD_LIST\"; then\n\t\tceph-volume
        raw list > \"$OSD_LIST\"\n\t\tcat \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device
        < \"$OSD_LIST\")\"\n\tfi\n\t[[ -z \"$DEVICE\" ]] && { echo \"no device\" ;
        exit 1 ; }\n\n\t# If a kernel device name change happens and a block device
        file\n\t# in the OSD directory becomes missing, this OSD fails to start\n\t#
        continuously. This problem can be resolved by confirming\n\t# the validity
        of the device file and recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
        [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ; then\n\t\trm
        $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports bluestore
        so we don't need to pass a store flag\n\tceph-volume raw activate --device
        \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
      env:
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdb
      - name: ROOK_METADATA_DEVICE
      - name: ROOK_WAL_DEVICE
      - name: ROOK_OSD_ID
        value: "1"
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: activate
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 512Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-1
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/temp-ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/admin-keyring-store/
        name: rook-ceph-admin-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lrdqc
        readOnly: true
    - args:
      - bluefs-bdev-expand
      - --path
      - /var/lib/ceph/osd/ceph-1
      command:
      - ceph-bluestore-tool
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: expand-bluefs
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 512Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-1
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lrdqc
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 512Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-1
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lrdqc
        readOnly: true
    nodeName: worker-1
    nodeSelector:
      kubernetes.io/hostname: worker-1
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: run-udev
    - hostPath:
        path: /var/lib/rook/rook-ceph/6f3631ca-fa9d-4b5e-ba5c-965ec65214c4_098de1f8-d466-4e51-aef3-874dee214609
        type: DirectoryOrCreate
      name: activate-osd
    - name: rook-ceph-admin-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-admin-keyring
    - name: kube-api-access-lrdqc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:35Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:51Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:51Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:49Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://5c31e147db945c2642115835bbe3b85746d4a0ebde685d11a8e4b3596a58d025
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: log-collector
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:50:35Z"
    - containerID: docker://bf133dfe25b61c15346a07d568492424fbb5736f54645f6862708a811fc964c4
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: osd
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:50:35Z"
    hostIP: 10.2.1.183
    initContainerStatuses:
    - containerID: docker://248f8736205dbebae0798cd313ea3817991cef802c11c853b086e73e99a47d6b
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: activate
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://248f8736205dbebae0798cd313ea3817991cef802c11c853b086e73e99a47d6b
          exitCode: 0
          finishedAt: "2024-03-13T16:50:15Z"
          reason: Completed
          startedAt: "2024-03-13T16:49:51Z"
    - containerID: docker://320c08157bebf2d2389de64856cf8cd06ff87baa0878565baeed196c76402b4d
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: expand-bluefs
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://320c08157bebf2d2389de64856cf8cd06ff87baa0878565baeed196c76402b4d
          exitCode: 0
          finishedAt: "2024-03-13T16:50:33Z"
          reason: Completed
          startedAt: "2024-03-13T16:50:15Z"
    - containerID: docker://095943c1318bed90cf3e025a155890fb7cde79dec33a65cc4b9aee4a31a2b9a3
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://095943c1318bed90cf3e025a155890fb7cde79dec33a65cc4b9aee4a31a2b9a3
          exitCode: 0
          finishedAt: "2024-03-13T16:50:34Z"
          reason: Completed
          startedAt: "2024-03-13T16:50:34Z"
    phase: Running
    podIP: 10.42.226.81
    podIPs:
    - ip: 10.42.226.81
    qosClass: Burstable
    startTime: "2024-03-13T16:49:49Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: f0cb913161b40176bcd3031397fb413477758f33293bcb40e6324c9ff70cb311
      cni.projectcalico.org/podIP: 10.42.38.79/32
      cni.projectcalico.org/podIPs: 10.42.38.79/32
    creationTimestamp: "2024-03-13T16:49:41Z"
    generateName: rook-ceph-osd-2-77f4cc765-
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "2"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "2"
      ceph_daemon_id: "2"
      ceph_daemon_type: osd
      device-class: ssd
      failure-domain: worker-4
      osd: "2"
      osd-store: bluestore
      pod-template-hash: 77f4cc765
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: worker-4
      topology-location-root: default
    name: rook-ceph-osd-2-77f4cc765-wzhtz
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-osd-2-77f4cc765
      uid: 26f9962c-e9dd-43a9-9ced-4ec774c34ccd
    resourceVersion: "13085788"
    uid: 592d39e1-bff3-403d-a6de-55dba090bc45
  spec:
    affinity: {}
    containers:
    - args:
      - --foreground
      - --id
      - "2"
      - --fsid
      - 6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
      - --setuser
      - ceph
      - --setgroup
      - ceph
      - --crush-location=root=default host=worker-4
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --ms-learn-addr-from-peer=false
      command:
      - ceph-osd
      env:
      - name: ROOK_NODE_NAME
        value: worker-4
      - name: ROOK_CLUSTER_ID
        value: 02274ff6-97d3-4d09-b941-31edb9bdad99
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: worker-4
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v17.2.6
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_OSD_RESTART_INTERVAL
        value: "0"
      - name: ROOK_OSD_UUID
        value: 4dfb6650-7784-48ca-8101-ef967416f326
      - name: ROOK_OSD_ID
        value: "2"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdb
      - name: ROOK_CV_MODE
        value: raw
      - name: ROOK_OSD_DEVICE_CLASS
        value: ssd
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.2.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: osd
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 512Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.2.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 720
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-2
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hb45j
        readOnly: true
      workingDir: /var/log/ceph
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-osd.2\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hb45j
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - /bin/bash
      - -c
      - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables are
        unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4\nOSD_UUID=4dfb6650-7784-48ca-8101-ef967416f326\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
        \"ceph.conf\" must have the \"fsid\" global configuration to activate encrypted
        OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
        This limitation will be removed later. After that, we can remove this\n# fsid
        injection code. Probably a good time is when to remove Quincy support.\n#
        https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
        /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
        = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif not
        config.has_section('global'):\n    config['global'] = {}\n\nif not config.has_option('global','fsid'):\n
        \   config['global']['fsid'] = '$CEPH_FSID'\n\nwith open('/etc/ceph/ceph.conf',
        'w') as configfile:\n    config.write(configfile)\n\"\n\n# create new keyring\nceph
        -n client.admin auth get-or-create osd.\"$OSD_ID\" mon 'allow profile osd'
        mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
        \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary directory\n\t#
        this is needed because when the init container exits, the tmpfs goes away
        and its content with it\n\t# this will result in the emptydir to be empty
        when accessed by the main osd container\n\tcp --verbose --no-dereference \"$OSD_DATA_DIR\"/*
        \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't need it anymore\n\tumount
        \"$OSD_DATA_DIR\"\n\n\t# copy back the content of the tmpfs into the original
        osd directory\n\tcp --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t#
        retain ownership of files to the ceph user/group\n\tchown --verbose --recursive
        ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm --recursive
        --force \"$TMP_DIR\"\nelse\n\t# 'ceph-volume raw list' (which the osd-prepare
        job uses to report OSDs on nodes)\n\t#  returns user-friendly device names
        which can change when systems reboot. To\n\t# keep OSD pods from crashing
        repeatedly after a reboot, we need to check if the\n\t# block device we have
        is still correct, and if it isn't correct, we need to\n\t# scan all the disks
        to find the right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device()
        {\n\t\t# jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
        python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
        sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
        == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device: '
        + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
        \ # don't keep processing once the disk is found\nsys.exit('no disk found
        with OSD ID $OSD_ID')\n\"\n\t}\n\n\tceph-volume raw list \"$DEVICE\" > \"$OSD_LIST\"\n\tcat
        \"$OSD_LIST\"\n\n\tif ! find_device < \"$OSD_LIST\"; then\n\t\tceph-volume
        raw list > \"$OSD_LIST\"\n\t\tcat \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device
        < \"$OSD_LIST\")\"\n\tfi\n\t[[ -z \"$DEVICE\" ]] && { echo \"no device\" ;
        exit 1 ; }\n\n\t# If a kernel device name change happens and a block device
        file\n\t# in the OSD directory becomes missing, this OSD fails to start\n\t#
        continuously. This problem can be resolved by confirming\n\t# the validity
        of the device file and recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
        [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ; then\n\t\trm
        $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports bluestore
        so we don't need to pass a store flag\n\tceph-volume raw activate --device
        \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
      env:
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdb
      - name: ROOK_METADATA_DEVICE
      - name: ROOK_WAL_DEVICE
      - name: ROOK_OSD_ID
        value: "2"
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: activate
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 512Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-2
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/temp-ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/admin-keyring-store/
        name: rook-ceph-admin-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hb45j
        readOnly: true
    - args:
      - bluefs-bdev-expand
      - --path
      - /var/lib/ceph/osd/ceph-2
      command:
      - ceph-bluestore-tool
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: expand-bluefs
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 512Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-2
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hb45j
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 512Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-2
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hb45j
        readOnly: true
    nodeName: worker-4
    nodeSelector:
      kubernetes.io/hostname: worker-4
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: run-udev
    - hostPath:
        path: /var/lib/rook/rook-ceph/6f3631ca-fa9d-4b5e-ba5c-965ec65214c4_4dfb6650-7784-48ca-8101-ef967416f326
        type: DirectoryOrCreate
      name: activate-osd
    - name: rook-ceph-admin-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-admin-keyring
    - name: kube-api-access-hb45j
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:28Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:42Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:42Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://5f186280af4f54597774733b5c33b03c3d51d98b1b717c379aa69c3183bdc145
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: log-collector
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:50:29Z"
    - containerID: docker://f54f8001f714de0392975805ef1ff4c68bd30ec2afbc71e5a6b474ca600e959b
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: osd
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:50:29Z"
    hostIP: 10.2.1.186
    initContainerStatuses:
    - containerID: docker://b95b2860b451f439c0493c215884e38547d3bd27905ca4634a807f45e9ef0779
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: activate
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://b95b2860b451f439c0493c215884e38547d3bd27905ca4634a807f45e9ef0779
          exitCode: 0
          finishedAt: "2024-03-13T16:50:13Z"
          reason: Completed
          startedAt: "2024-03-13T16:49:53Z"
    - containerID: docker://698a24c60c1bb2fca64f8ff1d32ee0cdec87cd638dc6a25b0ce2d18e314f0e99
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: expand-bluefs
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://698a24c60c1bb2fca64f8ff1d32ee0cdec87cd638dc6a25b0ce2d18e314f0e99
          exitCode: 0
          finishedAt: "2024-03-13T16:50:27Z"
          reason: Completed
          startedAt: "2024-03-13T16:50:13Z"
    - containerID: docker://9fbbddd57ec3938e7a59d42b77c8bf82521967f464f7285e749139f375967a66
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://9fbbddd57ec3938e7a59d42b77c8bf82521967f464f7285e749139f375967a66
          exitCode: 0
          finishedAt: "2024-03-13T16:50:28Z"
          reason: Completed
          startedAt: "2024-03-13T16:50:28Z"
    phase: Running
    podIP: 10.42.38.79
    podIPs:
    - ip: 10.42.38.79
    qosClass: Burstable
    startTime: "2024-03-13T16:49:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 58a363a244e838e73672d14216f456b78a22935b1915bca6235b09dfdb98979f
      cni.projectcalico.org/podIP: 10.42.133.203/32
      cni.projectcalico.org/podIPs: 10.42.133.203/32
    creationTimestamp: "2024-03-13T16:49:41Z"
    generateName: rook-ceph-osd-3-665577cf9d-
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "3"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "3"
      ceph_daemon_id: "3"
      ceph_daemon_type: osd
      device-class: ssd
      failure-domain: worker-2
      osd: "3"
      osd-store: bluestore
      pod-template-hash: 665577cf9d
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: worker-2
      topology-location-root: default
    name: rook-ceph-osd-3-665577cf9d-5fxsh
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-osd-3-665577cf9d
      uid: 651937da-d641-4914-929d-309013c10ab7
    resourceVersion: "13085659"
    uid: 206efd4e-12c6-47ea-a3f7-04ae8ef7d9b4
  spec:
    affinity: {}
    containers:
    - args:
      - --foreground
      - --id
      - "3"
      - --fsid
      - 6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
      - --setuser
      - ceph
      - --setgroup
      - ceph
      - --crush-location=root=default host=worker-2
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --ms-learn-addr-from-peer=false
      command:
      - ceph-osd
      env:
      - name: ROOK_NODE_NAME
        value: worker-2
      - name: ROOK_CLUSTER_ID
        value: 02274ff6-97d3-4d09-b941-31edb9bdad99
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: worker-2
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v17.2.6
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_OSD_RESTART_INTERVAL
        value: "0"
      - name: ROOK_OSD_UUID
        value: 3e4616df-9857-4b48-8919-f1f41a479006
      - name: ROOK_OSD_ID
        value: "3"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdb
      - name: ROOK_CV_MODE
        value: raw
      - name: ROOK_OSD_DEVICE_CLASS
        value: ssd
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.3.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: osd
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 512Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.3.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 720
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-3
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-74drf
        readOnly: true
      workingDir: /var/log/ceph
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-osd.3\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-74drf
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - /bin/bash
      - -c
      - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables are
        unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4\nOSD_UUID=3e4616df-9857-4b48-8919-f1f41a479006\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
        \"ceph.conf\" must have the \"fsid\" global configuration to activate encrypted
        OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
        This limitation will be removed later. After that, we can remove this\n# fsid
        injection code. Probably a good time is when to remove Quincy support.\n#
        https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
        /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
        = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif not
        config.has_section('global'):\n    config['global'] = {}\n\nif not config.has_option('global','fsid'):\n
        \   config['global']['fsid'] = '$CEPH_FSID'\n\nwith open('/etc/ceph/ceph.conf',
        'w') as configfile:\n    config.write(configfile)\n\"\n\n# create new keyring\nceph
        -n client.admin auth get-or-create osd.\"$OSD_ID\" mon 'allow profile osd'
        mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
        \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary directory\n\t#
        this is needed because when the init container exits, the tmpfs goes away
        and its content with it\n\t# this will result in the emptydir to be empty
        when accessed by the main osd container\n\tcp --verbose --no-dereference \"$OSD_DATA_DIR\"/*
        \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't need it anymore\n\tumount
        \"$OSD_DATA_DIR\"\n\n\t# copy back the content of the tmpfs into the original
        osd directory\n\tcp --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t#
        retain ownership of files to the ceph user/group\n\tchown --verbose --recursive
        ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm --recursive
        --force \"$TMP_DIR\"\nelse\n\t# 'ceph-volume raw list' (which the osd-prepare
        job uses to report OSDs on nodes)\n\t#  returns user-friendly device names
        which can change when systems reboot. To\n\t# keep OSD pods from crashing
        repeatedly after a reboot, we need to check if the\n\t# block device we have
        is still correct, and if it isn't correct, we need to\n\t# scan all the disks
        to find the right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device()
        {\n\t\t# jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
        python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
        sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
        == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device: '
        + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
        \ # don't keep processing once the disk is found\nsys.exit('no disk found
        with OSD ID $OSD_ID')\n\"\n\t}\n\n\tceph-volume raw list \"$DEVICE\" > \"$OSD_LIST\"\n\tcat
        \"$OSD_LIST\"\n\n\tif ! find_device < \"$OSD_LIST\"; then\n\t\tceph-volume
        raw list > \"$OSD_LIST\"\n\t\tcat \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device
        < \"$OSD_LIST\")\"\n\tfi\n\t[[ -z \"$DEVICE\" ]] && { echo \"no device\" ;
        exit 1 ; }\n\n\t# If a kernel device name change happens and a block device
        file\n\t# in the OSD directory becomes missing, this OSD fails to start\n\t#
        continuously. This problem can be resolved by confirming\n\t# the validity
        of the device file and recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
        [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ; then\n\t\trm
        $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports bluestore
        so we don't need to pass a store flag\n\tceph-volume raw activate --device
        \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
      env:
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdb
      - name: ROOK_METADATA_DEVICE
      - name: ROOK_WAL_DEVICE
      - name: ROOK_OSD_ID
        value: "3"
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: activate
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 512Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-3
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/temp-ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/admin-keyring-store/
        name: rook-ceph-admin-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-74drf
        readOnly: true
    - args:
      - bluefs-bdev-expand
      - --path
      - /var/lib/ceph/osd/ceph-3
      command:
      - ceph-bluestore-tool
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: expand-bluefs
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 512Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-3
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-74drf
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 512Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-3
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-74drf
        readOnly: true
    nodeName: worker-2
    nodeSelector:
      kubernetes.io/hostname: worker-2
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: run-udev
    - hostPath:
        path: /var/lib/rook/rook-ceph/6f3631ca-fa9d-4b5e-ba5c-965ec65214c4_3e4616df-9857-4b48-8919-f1f41a479006
        type: DirectoryOrCreate
      name: activate-osd
    - name: rook-ceph-admin-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-admin-keyring
    - name: kube-api-access-74drf
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:16Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:27Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:50:27Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:46Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://d699778396aceb05c167040b41c1c94dd4b6aae8e8226a61a8cec6f99dcb1e91
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: log-collector
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:50:16Z"
    - containerID: docker://b500df3b3bc483711873ddadcaa66aa53c57090089c31a2486765c19374c21d6
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: osd
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:50:16Z"
    hostIP: 10.2.1.184
    initContainerStatuses:
    - containerID: docker://b1a2f83f078cb1b2ecb484b6948fb357f89f1d8b5512167dd58c697106f23ecd
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: activate
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://b1a2f83f078cb1b2ecb484b6948fb357f89f1d8b5512167dd58c697106f23ecd
          exitCode: 0
          finishedAt: "2024-03-13T16:50:12Z"
          reason: Completed
          startedAt: "2024-03-13T16:49:48Z"
    - containerID: docker://67deb4cc3e0ea1e53ad566468e3c87d38cf8f98daee6af1c735cd5f78b4019d6
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: expand-bluefs
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://67deb4cc3e0ea1e53ad566468e3c87d38cf8f98daee6af1c735cd5f78b4019d6
          exitCode: 0
          finishedAt: "2024-03-13T16:50:14Z"
          reason: Completed
          startedAt: "2024-03-13T16:50:13Z"
    - containerID: docker://64d176b0dde87447a4c8d91b95ff1a138c12a303a4861f1c2586e4e408c01007
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://64d176b0dde87447a4c8d91b95ff1a138c12a303a4861f1c2586e4e408c01007
          exitCode: 0
          finishedAt: "2024-03-13T16:50:15Z"
          reason: Completed
          startedAt: "2024-03-13T16:50:15Z"
    phase: Running
    podIP: 10.42.133.203
    podIPs:
    - ip: 10.42.133.203
    qosClass: Burstable
    startTime: "2024-03-13T16:49:46Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 738b04f4df16d3a9c4099622b3ea871475146791cb2a9f6de35f4a467559a9f9
      cni.projectcalico.org/podIP: ""
      cni.projectcalico.org/podIPs: ""
    creationTimestamp: "2024-03-25T09:16:59Z"
    generateName: rook-ceph-osd-prepare-worker-1-
    labels:
      app: rook-ceph-osd-prepare
      ceph.rook.io/pvc: ""
      controller-uid: 09eabab4-05b4-4416-9540-2e62b38a70af
      job-name: rook-ceph-osd-prepare-worker-1
      rook_cluster: rook-ceph
    name: rook-ceph-osd-prepare-worker-1-hknpc
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: rook-ceph-osd-prepare-worker-1
      uid: 09eabab4-05b4-4416-9540-2e62b38a70af
    resourceVersion: "16380840"
    uid: 10b4d53e-cc68-4eee-9294-c95ab8892ee5
  spec:
    affinity: {}
    containers:
    - args:
      - ceph
      - osd
      - provision
      command:
      - /rook/rook
      env:
      - name: ROOK_NODE_NAME
        value: worker-1
      - name: ROOK_CLUSTER_ID
        value: 02274ff6-97d3-4d09-b941-31edb9bdad99
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CEPH_USERNAME
        valueFrom:
          secretKeyRef:
            key: ceph-username
            name: rook-ceph-mon
      - name: ROOK_FSID
        valueFrom:
          secretKeyRef:
            key: fsid
            name: rook-ceph-mon
      - name: ROOK_OSD_STORE_TYPE
        value: bluestore
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: worker-1
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: ROOK_LOG_LEVEL
        value: DEBUG
      - name: ROOK_DATA_DEVICE_FILTER
        value: ^sd.
      - name: ROOK_CEPH_VERSION
        value: ceph version 17.2.6-0 quincy
      - name: ROOK_OSD_CRUSH_DEVICE_CLASS
      - name: ROOK_OSD_CRUSH_INITIAL_WEIGHT
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: provision
      resources:
        requests:
          cpu: 500m
          memory: 50Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: ceph-conf-emptydir
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: udev
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /var/lib/rook-ceph-mon
        name: ceph-admin-secret
        readOnly: true
      - mountPath: /rootfs
        name: rootfs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lbqcx
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - --archive
      - --force
      - --verbose
      - /usr/local/bin/rook
      - /rook
      command:
      - cp
      image: rook/ceph:v1.13.1
      imagePullPolicy: IfNotPresent
      name: copy-bins
      resources:
        requests:
          cpu: 500m
          memory: 50Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lbqcx
        readOnly: true
    nodeName: worker-1
    nodeSelector:
      kubernetes.io/hostname: worker-1
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - emptyDir: {}
      name: ceph-conf-emptydir
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - emptyDir: {}
      name: rook-binaries
    - hostPath:
        path: /run/udev
        type: ""
      name: udev
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - name: ceph-admin-secret
      secret:
        defaultMode: 420
        items:
        - key: ceph-secret
          path: secret.keyring
        secretName: rook-ceph-mon
    - hostPath:
        path: /
        type: ""
      name: rootfs
    - name: kube-api-access-lbqcx
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-25T09:17:01Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-25T09:17:05Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-25T09:17:05Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-25T09:16:59Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://69a7c2693d283af15853ab4066bce081a24bd3814b9d8bcf02f5307fb4a3495d
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: provision
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: docker://69a7c2693d283af15853ab4066bce081a24bd3814b9d8bcf02f5307fb4a3495d
          exitCode: 0
          finishedAt: "2024-03-25T09:17:04Z"
          reason: Completed
          startedAt: "2024-03-25T09:17:01Z"
    hostIP: 10.2.1.183
    initContainerStatuses:
    - containerID: docker://3a15823e791a4dad83cac99e934856ed8a3d02c320767811014cc6449cd78598
      image: rook/ceph:v1.13.1
      imageID: docker-pullable://rook/ceph@sha256:bf7833f0b3a65a71be36c7a87b83fb22b5df78dba058e4401169cdabe0b09e05
      lastState: {}
      name: copy-bins
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://3a15823e791a4dad83cac99e934856ed8a3d02c320767811014cc6449cd78598
          exitCode: 0
          finishedAt: "2024-03-25T09:17:00Z"
          reason: Completed
          startedAt: "2024-03-25T09:17:00Z"
    phase: Succeeded
    podIP: 10.42.226.78
    podIPs:
    - ip: 10.42.226.78
    qosClass: Burstable
    startTime: "2024-03-25T09:16:59Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 273964a3199cb9b832d31158001797ab3277a7da4ea898f82ebe64dbee814c75
      cni.projectcalico.org/podIP: ""
      cni.projectcalico.org/podIPs: ""
    creationTimestamp: "2024-03-25T09:16:59Z"
    generateName: rook-ceph-osd-prepare-worker-2-
    labels:
      app: rook-ceph-osd-prepare
      ceph.rook.io/pvc: ""
      controller-uid: 8a24f70b-fecd-41c1-9238-e046792991a0
      job-name: rook-ceph-osd-prepare-worker-2
      rook_cluster: rook-ceph
    name: rook-ceph-osd-prepare-worker-2-rvlv6
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: rook-ceph-osd-prepare-worker-2
      uid: 8a24f70b-fecd-41c1-9238-e046792991a0
    resourceVersion: "16380892"
    uid: d50d0e70-3578-4c64-91e6-eb1a7c1f2637
  spec:
    affinity: {}
    containers:
    - args:
      - ceph
      - osd
      - provision
      command:
      - /rook/rook
      env:
      - name: ROOK_NODE_NAME
        value: worker-2
      - name: ROOK_CLUSTER_ID
        value: 02274ff6-97d3-4d09-b941-31edb9bdad99
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CEPH_USERNAME
        valueFrom:
          secretKeyRef:
            key: ceph-username
            name: rook-ceph-mon
      - name: ROOK_FSID
        valueFrom:
          secretKeyRef:
            key: fsid
            name: rook-ceph-mon
      - name: ROOK_OSD_STORE_TYPE
        value: bluestore
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: worker-2
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: ROOK_LOG_LEVEL
        value: DEBUG
      - name: ROOK_DATA_DEVICE_FILTER
        value: ^sd.
      - name: ROOK_CEPH_VERSION
        value: ceph version 17.2.6-0 quincy
      - name: ROOK_OSD_CRUSH_DEVICE_CLASS
      - name: ROOK_OSD_CRUSH_INITIAL_WEIGHT
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: provision
      resources:
        requests:
          cpu: 500m
          memory: 50Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: ceph-conf-emptydir
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: udev
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /var/lib/rook-ceph-mon
        name: ceph-admin-secret
        readOnly: true
      - mountPath: /rootfs
        name: rootfs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nxzpl
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - --archive
      - --force
      - --verbose
      - /usr/local/bin/rook
      - /rook
      command:
      - cp
      image: rook/ceph:v1.13.1
      imagePullPolicy: IfNotPresent
      name: copy-bins
      resources:
        requests:
          cpu: 500m
          memory: 50Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nxzpl
        readOnly: true
    nodeName: worker-2
    nodeSelector:
      kubernetes.io/hostname: worker-2
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - emptyDir: {}
      name: ceph-conf-emptydir
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - emptyDir: {}
      name: rook-binaries
    - hostPath:
        path: /run/udev
        type: ""
      name: udev
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - name: ceph-admin-secret
      secret:
        defaultMode: 420
        items:
        - key: ceph-secret
          path: secret.keyring
        secretName: rook-ceph-mon
    - hostPath:
        path: /
        type: ""
      name: rootfs
    - name: kube-api-access-nxzpl
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-25T09:17:01Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-25T09:17:10Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-25T09:17:10Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-25T09:16:59Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://1120ee9e34c7a3d67caeb53eed7d3f254ce60bba7d98824270cfd89a95d86ce6
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: provision
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: docker://1120ee9e34c7a3d67caeb53eed7d3f254ce60bba7d98824270cfd89a95d86ce6
          exitCode: 0
          finishedAt: "2024-03-25T09:17:10Z"
          reason: Completed
          startedAt: "2024-03-25T09:17:01Z"
    hostIP: 10.2.1.184
    initContainerStatuses:
    - containerID: docker://e5ddb887c35d14a9f6bae2f018d12e6c9aa4cbe910a19488d3420dbd9eed5958
      image: rook/ceph:v1.13.1
      imageID: docker-pullable://rook/ceph@sha256:bf7833f0b3a65a71be36c7a87b83fb22b5df78dba058e4401169cdabe0b09e05
      lastState: {}
      name: copy-bins
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://e5ddb887c35d14a9f6bae2f018d12e6c9aa4cbe910a19488d3420dbd9eed5958
          exitCode: 0
          finishedAt: "2024-03-25T09:17:01Z"
          reason: Completed
          startedAt: "2024-03-25T09:17:01Z"
    phase: Succeeded
    podIP: 10.42.133.235
    podIPs:
    - ip: 10.42.133.235
    qosClass: Burstable
    startTime: "2024-03-25T09:17:00Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: d77d6f1a90865e1eb90834f65f6ffed0656460c689d90a146d8c8d001bc78da0
      cni.projectcalico.org/podIP: ""
      cni.projectcalico.org/podIPs: ""
    creationTimestamp: "2024-03-25T09:17:00Z"
    generateName: rook-ceph-osd-prepare-worker-3-
    labels:
      app: rook-ceph-osd-prepare
      ceph.rook.io/pvc: ""
      controller-uid: d5b4f8b6-4ded-47ee-8bf3-25296a1deff2
      job-name: rook-ceph-osd-prepare-worker-3
      rook_cluster: rook-ceph
    name: rook-ceph-osd-prepare-worker-3-j48rz
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: rook-ceph-osd-prepare-worker-3
      uid: d5b4f8b6-4ded-47ee-8bf3-25296a1deff2
    resourceVersion: "16380900"
    uid: cea9464f-1192-49cf-b347-7c92d26ea427
  spec:
    affinity: {}
    containers:
    - args:
      - ceph
      - osd
      - provision
      command:
      - /rook/rook
      env:
      - name: ROOK_NODE_NAME
        value: worker-3
      - name: ROOK_CLUSTER_ID
        value: 02274ff6-97d3-4d09-b941-31edb9bdad99
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CEPH_USERNAME
        valueFrom:
          secretKeyRef:
            key: ceph-username
            name: rook-ceph-mon
      - name: ROOK_FSID
        valueFrom:
          secretKeyRef:
            key: fsid
            name: rook-ceph-mon
      - name: ROOK_OSD_STORE_TYPE
        value: bluestore
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: worker-3
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: ROOK_LOG_LEVEL
        value: DEBUG
      - name: ROOK_DATA_DEVICE_FILTER
        value: ^sd.
      - name: ROOK_CEPH_VERSION
        value: ceph version 17.2.6-0 quincy
      - name: ROOK_OSD_CRUSH_DEVICE_CLASS
      - name: ROOK_OSD_CRUSH_INITIAL_WEIGHT
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: provision
      resources:
        requests:
          cpu: 500m
          memory: 50Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: ceph-conf-emptydir
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: udev
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /var/lib/rook-ceph-mon
        name: ceph-admin-secret
        readOnly: true
      - mountPath: /rootfs
        name: rootfs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-62xtj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - --archive
      - --force
      - --verbose
      - /usr/local/bin/rook
      - /rook
      command:
      - cp
      image: rook/ceph:v1.13.1
      imagePullPolicy: IfNotPresent
      name: copy-bins
      resources:
        requests:
          cpu: 500m
          memory: 50Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-62xtj
        readOnly: true
    nodeName: worker-3
    nodeSelector:
      kubernetes.io/hostname: worker-3
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - emptyDir: {}
      name: ceph-conf-emptydir
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - emptyDir: {}
      name: rook-binaries
    - hostPath:
        path: /run/udev
        type: ""
      name: udev
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - name: ceph-admin-secret
      secret:
        defaultMode: 420
        items:
        - key: ceph-secret
          path: secret.keyring
        secretName: rook-ceph-mon
    - hostPath:
        path: /
        type: ""
      name: rootfs
    - name: kube-api-access-62xtj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-25T09:17:03Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-25T09:17:11Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-25T09:17:11Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-25T09:17:00Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://c2929ffcf1592492e1a670b199991e2e74090f6816e0ff6baea3aad3034c0583
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: provision
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: docker://c2929ffcf1592492e1a670b199991e2e74090f6816e0ff6baea3aad3034c0583
          exitCode: 0
          finishedAt: "2024-03-25T09:17:10Z"
          reason: Completed
          startedAt: "2024-03-25T09:17:04Z"
    hostIP: 10.2.1.185
    initContainerStatuses:
    - containerID: docker://297e6e24d97392316a25830bc968d26e09943024045366e1bbc2614bc4f51a59
      image: rook/ceph:v1.13.1
      imageID: docker-pullable://rook/ceph@sha256:bf7833f0b3a65a71be36c7a87b83fb22b5df78dba058e4401169cdabe0b09e05
      lastState: {}
      name: copy-bins
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://297e6e24d97392316a25830bc968d26e09943024045366e1bbc2614bc4f51a59
          exitCode: 0
          finishedAt: "2024-03-25T09:17:03Z"
          reason: Completed
          startedAt: "2024-03-25T09:17:01Z"
    phase: Succeeded
    podIP: 10.42.97.217
    podIPs:
    - ip: 10.42.97.217
    qosClass: Burstable
    startTime: "2024-03-25T09:17:00Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: cd426d489ba80bd48a5178374a5909723e8bb2b1b3c5b697965e2fc951fe654a
      cni.projectcalico.org/podIP: ""
      cni.projectcalico.org/podIPs: ""
    creationTimestamp: "2024-03-25T09:17:04Z"
    generateName: rook-ceph-osd-prepare-worker-4-
    labels:
      app: rook-ceph-osd-prepare
      ceph.rook.io/pvc: ""
      controller-uid: a0de7042-62d3-4acf-9fb9-2b5143a4a700
      job-name: rook-ceph-osd-prepare-worker-4
      rook_cluster: rook-ceph
    name: rook-ceph-osd-prepare-worker-4-2fczw
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: rook-ceph-osd-prepare-worker-4
      uid: a0de7042-62d3-4acf-9fb9-2b5143a4a700
    resourceVersion: "16380938"
    uid: be1df1a8-4050-415c-8e11-6fe4f8037621
  spec:
    affinity: {}
    containers:
    - args:
      - ceph
      - osd
      - provision
      command:
      - /rook/rook
      env:
      - name: ROOK_NODE_NAME
        value: worker-4
      - name: ROOK_CLUSTER_ID
        value: 02274ff6-97d3-4d09-b941-31edb9bdad99
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CEPH_USERNAME
        valueFrom:
          secretKeyRef:
            key: ceph-username
            name: rook-ceph-mon
      - name: ROOK_FSID
        valueFrom:
          secretKeyRef:
            key: fsid
            name: rook-ceph-mon
      - name: ROOK_OSD_STORE_TYPE
        value: bluestore
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: worker-4
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: ROOK_LOG_LEVEL
        value: DEBUG
      - name: ROOK_DATA_DEVICE_FILTER
        value: ^sd.
      - name: ROOK_CEPH_VERSION
        value: ceph version 17.2.6-0 quincy
      - name: ROOK_OSD_CRUSH_DEVICE_CLASS
      - name: ROOK_OSD_CRUSH_INITIAL_WEIGHT
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: provision
      resources:
        requests:
          cpu: 500m
          memory: 50Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: ceph-conf-emptydir
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: udev
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /var/lib/rook-ceph-mon
        name: ceph-admin-secret
        readOnly: true
      - mountPath: /rootfs
        name: rootfs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jxn42
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - --archive
      - --force
      - --verbose
      - /usr/local/bin/rook
      - /rook
      command:
      - cp
      image: rook/ceph:v1.13.1
      imagePullPolicy: IfNotPresent
      name: copy-bins
      resources:
        requests:
          cpu: 500m
          memory: 50Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jxn42
        readOnly: true
    nodeName: worker-4
    nodeSelector:
      kubernetes.io/hostname: worker-4
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - emptyDir: {}
      name: ceph-conf-emptydir
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - emptyDir: {}
      name: rook-binaries
    - hostPath:
        path: /run/udev
        type: ""
      name: udev
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - name: ceph-admin-secret
      secret:
        defaultMode: 420
        items:
        - key: ceph-secret
          path: secret.keyring
        secretName: rook-ceph-mon
    - hostPath:
        path: /
        type: ""
      name: rootfs
    - name: kube-api-access-jxn42
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-25T09:17:11Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-25T09:17:18Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-25T09:17:18Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-25T09:17:04Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://45ce72fef6117deb89c1cb972f757343c86a65bc866e0106a21a77b7b6b85a90
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: provision
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: docker://45ce72fef6117deb89c1cb972f757343c86a65bc866e0106a21a77b7b6b85a90
          exitCode: 0
          finishedAt: "2024-03-25T09:17:17Z"
          reason: Completed
          startedAt: "2024-03-25T09:17:11Z"
    hostIP: 10.2.1.186
    initContainerStatuses:
    - containerID: docker://fd6bb117819a522802d2c3417437297927b4f7405109e8d693d4914d2733b9c1
      image: rook/ceph:v1.13.1
      imageID: docker-pullable://rook/ceph@sha256:bf7833f0b3a65a71be36c7a87b83fb22b5df78dba058e4401169cdabe0b09e05
      lastState: {}
      name: copy-bins
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://fd6bb117819a522802d2c3417437297927b4f7405109e8d693d4914d2733b9c1
          exitCode: 0
          finishedAt: "2024-03-25T09:17:10Z"
          reason: Completed
          startedAt: "2024-03-25T09:17:08Z"
    phase: Succeeded
    podIP: 10.42.38.123
    podIPs:
    - ip: 10.42.38.123
    qosClass: Burstable
    startTime: "2024-03-25T09:17:04Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: ffcda5874862289f9845dcd003b42677b375c60d6f63876f759f3fec457780b5
      cni.projectcalico.org/podIP: 10.42.38.73/32
      cni.projectcalico.org/podIPs: 10.42.38.73/32
    creationTimestamp: "2024-03-13T16:49:42Z"
    generateName: rook-ceph-rgw-ceph-objectstore-a-7cb6c484c4-
    labels:
      app: rook-ceph-rgw
      app.kubernetes.io/component: cephobjectstores.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: ceph-objectstore
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-rgw
      app.kubernetes.io/part-of: ceph-objectstore
      ceph_daemon_id: ceph-objectstore
      ceph_daemon_type: rgw
      pod-template-hash: 7cb6c484c4
      rgw: ceph-objectstore
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      rook_object_store: ceph-objectstore
    name: rook-ceph-rgw-ceph-objectstore-a-7cb6c484c4-fb7zq
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-rgw-ceph-objectstore-a-7cb6c484c4
      uid: e8253e36-ea2c-4a1a-a05b-44b000818a7a
    resourceVersion: "13086166"
    uid: 128b53d9-9d96-4100-8b44-428ceb1c3723
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app: rook-ceph-rgw
                ceph_daemon_id: ceph-objectstore
                rgw: ceph-objectstore
                rook_cluster: rook-ceph
                rook_object_store: ceph-objectstore
            topologyKey: kubernetes.io/hostname
          weight: 50
    containers:
    - args:
      - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=rgw.ceph.objectstore.a
      - --setuser=ceph
      - --setgroup=ceph
      - --foreground
      - --rgw-frontends=beast port=8080
      - --host=$(POD_NAME)
      - --rgw-mime-types-file=/etc/ceph/rgw/mime.types
      - --rgw-realm=ceph-objectstore
      - --rgw-zonegroup=ceph-objectstore
      - --rgw-zone=ceph-objectstore
      command:
      - radosgw
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v17.2.6
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: rgw
      readinessProbe:
        exec:
          command:
          - bash
          - -c
          - |
            #!/usr/bin/env bash

            PROBE_TYPE="readiness"
            PROBE_PORT="8080"
            PROBE_PROTOCOL="HTTP"

            # standard bash codes start at 126 and progress upward. pick error codes from 125 downward for
            # script as to allow curl to output new error codes and still return a distinctive number.
            USAGE_ERR_CODE=125
            PROBE_ERR_CODE=124
            # curl error codes: 1-123

            STARTUP_TYPE='startup'
            READINESS_TYPE='readiness'

            RGW_URL="$PROBE_PROTOCOL://0.0.0.0:$PROBE_PORT"

            function check() {
              local URL="$1"
              # --insecure - don't validate ssl if using secure port only
              # --silent - don't output progress info
              # --output /dev/stderr - output HTML header to stdout (good for debugging)
              # --write-out '%{response_code}' - print the HTTP response code to stdout
              curl --insecure --silent --output /dev/stderr --write-out '%{response_code}' "$URL"
            }

            http_response="$(check "$RGW_URL")"
            retcode=$?

            if [[ $retcode -ne 0 ]]; then
              # if this is the startup probe, always returning failure. if startup probe passes, all subsequent
              # probes can rely on the assumption that the health check was once succeeding without errors.
              # if this is the readiness probe, we know that curl was previously working correctly in the
              # startup probe, so curl error most likely means some new error with the RGW.
              echo "RGW health check failed with error code: $retcode. the RGW likely cannot be reached by clients" >/dev/stderr
              exit $retcode
            fi

            RGW_RATE_LIMITING_RESPONSE=503
            RGW_MISCONFIGURATION_RESPONSE=500

            if [[ $http_response -ge 200 ]] && [[ $http_response -lt 400 ]]; then
              # 200-399 are successful responses. same behavior as Kubernetes' HTTP probe
              exit 0

            elif [[ $http_response -eq $RGW_RATE_LIMITING_RESPONSE ]]; then
              # S3's '503: slow down' code is not an error but an indication that RGW is throttling client
              # traffic. failing the readiness check here would only cause an increase in client connections on
              # other RGWs and likely cause those to fail also in a cascade. i.e., a special healthy response.
              echo "INFO: RGW is rate limiting" 2>/dev/stderr
              exit 0

            elif [[ $http_response -eq $RGW_MISCONFIGURATION_RESPONSE ]]; then
              # can't specifically determine if the RGW is running or not. most likely a misconfiguration.
              case "$PROBE_TYPE" in
              "$STARTUP_TYPE")
                # fail until we can accurately get a valid healthy response when runtime starts.
                echo 'FAIL: HTTP code 500 suggests an RGW misconfiguration.' >/dev/stderr
                exit $PROBE_ERR_CODE
                ;;
              "$READINESS_TYPE")
                # config likely modified at runtime which could result in all RGWs failing this check.
                # occasional client failures are still better than total failure, so ignore this
                echo 'WARN: HTTP code 500 suggests an RGW misconfiguration' >/dev/stderr
                exit 0
                ;;
              *)
                # prior arg validation means this path should never be activated, but keep to be safe
                echo "ERROR: probe type is unknown: $PROBE_TYPE" >/dev/stderr
                exit $USAGE_ERR_CODE
                ;;
              esac

            else
              # anything else is a failing response. same behavior as Kubernetes' HTTP probe
              echo "FAIL: received an HTTP error code: $http_response"
              exit $PROBE_ERR_CODE

            fi
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 3
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 256Mi
      securityContext:
        privileged: false
      startupProbe:
        exec:
          command:
          - bash
          - -c
          - |
            #!/usr/bin/env bash

            PROBE_TYPE="startup"
            PROBE_PORT="8080"
            PROBE_PROTOCOL="HTTP"

            # standard bash codes start at 126 and progress upward. pick error codes from 125 downward for
            # script as to allow curl to output new error codes and still return a distinctive number.
            USAGE_ERR_CODE=125
            PROBE_ERR_CODE=124
            # curl error codes: 1-123

            STARTUP_TYPE='startup'
            READINESS_TYPE='readiness'

            RGW_URL="$PROBE_PROTOCOL://0.0.0.0:$PROBE_PORT"

            function check() {
              local URL="$1"
              # --insecure - don't validate ssl if using secure port only
              # --silent - don't output progress info
              # --output /dev/stderr - output HTML header to stdout (good for debugging)
              # --write-out '%{response_code}' - print the HTTP response code to stdout
              curl --insecure --silent --output /dev/stderr --write-out '%{response_code}' "$URL"
            }

            http_response="$(check "$RGW_URL")"
            retcode=$?

            if [[ $retcode -ne 0 ]]; then
              # if this is the startup probe, always returning failure. if startup probe passes, all subsequent
              # probes can rely on the assumption that the health check was once succeeding without errors.
              # if this is the readiness probe, we know that curl was previously working correctly in the
              # startup probe, so curl error most likely means some new error with the RGW.
              echo "RGW health check failed with error code: $retcode. the RGW likely cannot be reached by clients" >/dev/stderr
              exit $retcode
            fi

            RGW_RATE_LIMITING_RESPONSE=503
            RGW_MISCONFIGURATION_RESPONSE=500

            if [[ $http_response -ge 200 ]] && [[ $http_response -lt 400 ]]; then
              # 200-399 are successful responses. same behavior as Kubernetes' HTTP probe
              exit 0

            elif [[ $http_response -eq $RGW_RATE_LIMITING_RESPONSE ]]; then
              # S3's '503: slow down' code is not an error but an indication that RGW is throttling client
              # traffic. failing the readiness check here would only cause an increase in client connections on
              # other RGWs and likely cause those to fail also in a cascade. i.e., a special healthy response.
              echo "INFO: RGW is rate limiting" 2>/dev/stderr
              exit 0

            elif [[ $http_response -eq $RGW_MISCONFIGURATION_RESPONSE ]]; then
              # can't specifically determine if the RGW is running or not. most likely a misconfiguration.
              case "$PROBE_TYPE" in
              "$STARTUP_TYPE")
                # fail until we can accurately get a valid healthy response when runtime starts.
                echo 'FAIL: HTTP code 500 suggests an RGW misconfiguration.' >/dev/stderr
                exit $PROBE_ERR_CODE
                ;;
              "$READINESS_TYPE")
                # config likely modified at runtime which could result in all RGWs failing this check.
                # occasional client failures are still better than total failure, so ignore this
                echo 'WARN: HTTP code 500 suggests an RGW misconfiguration' >/dev/stderr
                exit 0
                ;;
              *)
                # prior arg validation means this path should never be activated, but keep to be safe
                echo "ERROR: probe type is unknown: $PROBE_TYPE" >/dev/stderr
                exit $USAGE_ERR_CODE
                ;;
              esac

            else
              # anything else is a failing response. same behavior as Kubernetes' HTTP probe
              echo "FAIL: received an HTTP error code: $http_response"
              exit $PROBE_ERR_CODE

            fi
        failureThreshold: 33
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-rgw-ceph-objectstore-a-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/rgw/ceph-ceph-objectstore
        name: ceph-daemon-data
      - mountPath: /etc/ceph/rgw
        name: rook-ceph-rgw-ceph-objectstore-mime-types
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-h6cfb
        readOnly: true
      workingDir: /var/log/ceph
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-client.rgw.ceph.objectstore.a\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-h6cfb
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      - /var/lib/ceph/rgw/ceph-ceph-objectstore
      command:
      - chown
      image: quay.io/ceph/ceph:v17.2.6
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 256Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-rgw-ceph-objectstore-a-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/rgw/ceph-ceph-objectstore
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-h6cfb
        readOnly: true
    nodeName: worker-4
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-rgw
    serviceAccountName: rook-ceph-rgw
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 5
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - name: rook-ceph-rgw-ceph-objectstore-a-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-rgw-ceph-objectstore-a-keyring
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - emptyDir: {}
      name: ceph-daemon-data
    - configMap:
        defaultMode: 420
        name: rook-ceph-rgw-ceph-objectstore-mime-types
      name: rook-ceph-rgw-ceph-objectstore-mime-types
    - name: kube-api-access-h6cfb
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:51Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:51:18Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:51:18Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:46Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://2238928a9ff32b7b0693278c2c480b403c6e79a7676db653ff388e13489c2b5d
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: log-collector
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:51Z"
    - containerID: docker://efe7161342120e43c21e38c841472fcd78e3f3b3894aa2ef12d0e070fe2e6ee5
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: rgw
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:51Z"
    hostIP: 10.2.1.186
    initContainerStatuses:
    - containerID: docker://d41b58348c5291c4eb78be0188dde61820753c1cd0346a319f942f3b1d0b7813
      image: quay.io/ceph/ceph:v17.2.6
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:e40c19cd70e047d14d70f5ec3cf501da081395a670cd59ca881ff56119660c8f
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://d41b58348c5291c4eb78be0188dde61820753c1cd0346a319f942f3b1d0b7813
          exitCode: 0
          finishedAt: "2024-03-13T16:49:49Z"
          reason: Completed
          startedAt: "2024-03-13T16:49:49Z"
    phase: Running
    podIP: 10.42.38.73
    podIPs:
    - ip: 10.42.38.73
    qosClass: Burstable
    startTime: "2024-03-13T16:49:46Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: e311de5e1b6870fd17687ba8e345321eab8b67ea5909438f246bb9176261713c
      cni.projectcalico.org/podIP: 10.42.97.206/32
      cni.projectcalico.org/podIPs: 10.42.97.206/32
    creationTimestamp: "2024-03-13T16:49:42Z"
    generateName: rook-ceph-tools-6d6d694fb9-
    labels:
      app: rook-ceph-tools
      pod-template-hash: 6d6d694fb9
    name: rook-ceph-tools-6d6d694fb9-v9xp7
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-tools-6d6d694fb9
      uid: aae30c56-fcc3-400b-b138-85f447de9eaf
    resourceVersion: "13085043"
    uid: 01b825a2-9234-4cff-affa-89c31d09da34
  spec:
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        # Replicate the script from toolbox.sh inline so the ceph image
        # can be run directly, instead of requiring the rook toolbox
        CEPH_CONFIG="/etc/ceph/ceph.conf"
        MON_CONFIG="/etc/rook/mon-endpoints"
        KEYRING_FILE="/etc/ceph/keyring"

        # create a ceph config file in its default location so ceph/rados tools can be used
        # without specifying any arguments
        write_endpoints() {
          endpoints=$(cat ${MON_CONFIG})

          # filter out the mon names
          # external cluster can have numbers or hyphens in mon names, handling them in regex
          # shellcheck disable=SC2001
          mon_endpoints=$(echo "${endpoints}"| sed 's/[a-z0-9_-]\+=//g')

          DATE=$(date)
          echo "$DATE writing mon endpoints to ${CEPH_CONFIG}: ${endpoints}"
            cat <<EOF > ${CEPH_CONFIG}
        [global]
        mon_host = ${mon_endpoints}

        [client.admin]
        keyring = ${KEYRING_FILE}
        EOF
        }

        # watch the endpoints config file and update if the mon endpoints ever change
        watch_endpoints() {
          # get the timestamp for the target of the soft link
          real_path=$(realpath ${MON_CONFIG})
          initial_time=$(stat -c %Z "${real_path}")
          while true; do
            real_path=$(realpath ${MON_CONFIG})
            latest_time=$(stat -c %Z "${real_path}")

            if [[ "${latest_time}" != "${initial_time}" ]]; then
              write_endpoints
              initial_time=${latest_time}
            fi

            sleep 10
          done
        }

        # read the secret from an env var (for backward compatibility), or from the secret file
        ceph_secret=${ROOK_CEPH_SECRET}
        if [[ "$ceph_secret" == "" ]]; then
          ceph_secret=$(cat /var/lib/rook-ceph-mon/secret.keyring)
        fi

        # create the keyring file
        cat <<EOF > ${KEYRING_FILE}
        [${ROOK_CEPH_USERNAME}]
        key = ${ceph_secret}
        EOF

        # write the initial config file
        write_endpoints

        # continuously update the mon endpoints if they fail over
        watch_endpoints
      env:
      - name: ROOK_CEPH_USERNAME
        valueFrom:
          secretKeyRef:
            key: ceph-username
            name: rook-ceph-mon
      image: quay.io/ceph/ceph:v18.2.1
      imagePullPolicy: IfNotPresent
      name: rook-ceph-tools
      resources: {}
      securityContext:
        capabilities:
          drop:
          - ALL
        runAsGroup: 2016
        runAsNonRoot: true
        runAsUser: 2016
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/ceph
        name: ceph-config
      - mountPath: /etc/rook
        name: mon-endpoint-volume
      - mountPath: /var/lib/rook-ceph-mon
        name: ceph-admin-secret
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2bmlj
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    nodeName: worker-3
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 5
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: ceph-admin-secret
      secret:
        defaultMode: 420
        items:
        - key: ceph-secret
          path: secret.keyring
        optional: false
        secretName: rook-ceph-mon
    - configMap:
        defaultMode: 420
        items:
        - key: data
          path: mon-endpoints
        name: rook-ceph-mon-endpoints
      name: mon-endpoint-volume
    - emptyDir: {}
      name: ceph-config
    - name: kube-api-access-2bmlj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:42Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:43Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:43Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:49:42Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://2c1ecf9c7d7f44f528214a090500fbb14e8bc546fdd2bee50e1c8a218bc0a35c
      image: quay.io/ceph/ceph:v18.2.1
      imageID: docker-pullable://quay.io/ceph/ceph@sha256:9f35728f6070a596500c0804814a12ab6b98e05067316dc64876fb4b28d04af3
      lastState: {}
      name: rook-ceph-tools
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:49:43Z"
    hostIP: 10.2.1.185
    phase: Running
    podIP: 10.42.97.206
    podIPs:
    - ip: 10.42.97.206
    qosClass: BestEffort
    startTime: "2024-03-13T16:49:42Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-13T16:11:39Z"
    generateName: tigera-operator-5cc9b4b697-
    labels:
      k8s-app: tigera-operator
      name: tigera-operator
      pod-template-hash: 5cc9b4b697
    name: tigera-operator-5cc9b4b697-dp5hw
    namespace: tigera-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: tigera-operator-5cc9b4b697
      uid: feafa0d3-4666-44c3-920d-6da969025ffb
    resourceVersion: "13072669"
    uid: f116964d-fe08-4531-8cf0-6b20026824e6
  spec:
    containers:
    - command:
      - operator
      env:
      - name: WATCH_NAMESPACE
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: OPERATOR_NAME
        value: tigera-operator
      - name: TIGERA_OPERATOR_INIT_IMAGE_VERSION
        value: master
      envFrom:
      - configMapRef:
          name: kubernetes-services-endpoint
          optional: true
      image: quay.io/tigera/operator:master
      imagePullPolicy: IfNotPresent
      name: tigera-operator
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/calico
        name: var-lib-calico
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-l9glc
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    nodeName: master-1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: tigera-operator
    serviceAccountName: tigera-operator
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/calico
        type: ""
      name: var-lib-calico
    - name: kube-api-access-l9glc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:11:39Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:11:49Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:11:49Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-13T16:11:39Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://bf8f124d038416104d590ac60b32dabb98a89ae3352d9685061030c516b3fad5
      image: quay.io/tigera/operator:master
      imageID: docker-pullable://quay.io/tigera/operator@sha256:2618ac4f4250752585942692f1e615d6bc3474087037e61df4771e76d8b56f9f
      lastState: {}
      name: tigera-operator
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-13T16:11:48Z"
    hostIP: 10.2.1.180
    phase: Running
    podIP: 10.2.1.180
    podIPs:
    - ip: 10.2.1.180
    qosClass: BestEffort
    startTime: "2024-03-13T16:11:39Z"
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-02-13T10:42:14Z"
    labels:
      app: airflow
      app.kubernetes.io/managed-by: Helm
      chart: airflow-8.8.0
      component: pgbouncer
      heritage: Helm
      release: airflow
    name: airflow-pgbouncer
    namespace: airflow
    resourceVersion: "4561421"
    uid: 4a40ad12-592c-4337-9429-be5674a90975
  spec:
    clusterIP: 10.43.18.170
    clusterIPs:
    - 10.43.18.170
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: pgbouncer
      port: 6432
      protocol: TCP
      targetPort: 6432
    selector:
      app: airflow
      component: pgbouncer
      release: airflow
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-02-13T10:42:14Z"
    labels:
      app: airflow
      app.kubernetes.io/managed-by: Helm
      chart: airflow-8.8.0
      component: web
      heritage: Helm
      release: airflow
    name: airflow-web
    namespace: airflow
    resourceVersion: "4561419"
    uid: abaa2c23-5185-42ea-a44d-587dff648533
  spec:
    clusterIP: 10.43.179.96
    clusterIPs:
    - 10.43.179.96
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: web
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app: airflow
      component: web
      release: airflow
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: postgresql-ha
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-02-13T12:27:05Z"
    labels:
      app.kubernetes.io/component: pgpool
      app.kubernetes.io/instance: postgresql-ha
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql-ha
      app.kubernetes.io/version: 4.5.0
      helm.sh/chart: postgresql-ha-13.2.3
    name: postgresql-ha-pgpool
    namespace: airflow
    resourceVersion: "4583700"
    uid: a0d34fa0-7e5b-47f8-a4a8-fd0563dfaaf2
  spec:
    clusterIP: 10.43.112.64
    clusterIPs:
    - 10.43.112.64
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: postgresql
      port: 5432
      protocol: TCP
      targetPort: postgresql
    selector:
      app.kubernetes.io/component: pgpool
      app.kubernetes.io/instance: postgresql-ha
      app.kubernetes.io/name: postgresql-ha
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: postgresql-ha
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-02-13T12:27:05Z"
    labels:
      app.kubernetes.io/component: postgresql
      app.kubernetes.io/instance: postgresql-ha
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql-ha
      app.kubernetes.io/version: 16.2.0
      helm.sh/chart: postgresql-ha-13.2.3
    name: postgresql-ha-postgresql
    namespace: airflow
    resourceVersion: "4583695"
    uid: 4868b0ff-b002-45f0-8fd8-22262edf467d
  spec:
    clusterIP: 10.43.161.54
    clusterIPs:
    - 10.43.161.54
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: postgresql
      port: 5432
      protocol: TCP
      targetPort: postgresql
    selector:
      app.kubernetes.io/component: postgresql
      app.kubernetes.io/instance: postgresql-ha
      app.kubernetes.io/name: postgresql-ha
      role: data
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: postgresql-ha
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-02-13T12:27:05Z"
    labels:
      app.kubernetes.io/instance: postgresql-ha
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql-ha
      app.kubernetes.io/version: 16.2.0
      helm.sh/chart: postgresql-ha-13.2.3
    name: postgresql-ha-postgresql-headless
    namespace: airflow
    resourceVersion: "4583692"
    uid: f5c30343-d69b-469e-847a-bdf1fa77ff31
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: postgresql
      port: 5432
      protocol: TCP
      targetPort: postgresql
    selector:
      app.kubernetes.io/component: postgresql
      app.kubernetes.io/instance: postgresql-ha
      app.kubernetes.io/name: postgresql-ha
      role: data
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus-statsd-exporter
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-02-15T03:44:40Z"
    labels:
      app.kubernetes.io/instance: prometheus-statsd-exporter
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-statsd-exporter
      app.kubernetes.io/version: v0.26.0
      helm.sh/chart: prometheus-statsd-exporter-0.2.0
    name: prometheus-statsd-exporter
    namespace: airflow
    resourceVersion: "6548317"
    uid: 91a935bd-93a0-4c6a-b5c6-d2099f030645
  spec:
    clusterIP: 10.43.88.72
    clusterIPs:
    - 10.43.88.72
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: web
      port: 9102
      protocol: TCP
      targetPort: web
    - name: statsd-tcp
      port: 9125
      protocol: TCP
      targetPort: statsd-tcp
    - name: statsd-udp
      port: 9125
      protocol: UDP
      targetPort: statsd-udp
    selector:
      app.kubernetes.io/instance: prometheus-statsd-exporter
      app.kubernetes.io/name: prometheus-statsd-exporter
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-03-13T16:45:36Z"
    labels:
      k8s-app: tigera-api
    name: calico-api
    namespace: calico-apiserver
    ownerReferences:
    - apiVersion: operator.tigera.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: APIServer
      name: default
      uid: 35db4f2b-9175-436e-b7e1-3e599376612f
    resourceVersion: "13082330"
    uid: 9945c4df-0ae3-43c5-a48c-65f0267b16e0
  spec:
    clusterIP: 10.43.201.179
    clusterIPs:
    - 10.43.201.179
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: apiserver
      port: 443
      protocol: TCP
      targetPort: 5443
    selector:
      apiserver: "true"
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      prometheus.io/port: "9094"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-03-13T16:12:07Z"
    labels:
      k8s-app: calico-kube-controllers
    name: calico-kube-controllers-metrics
    namespace: calico-system
    ownerReferences:
    - apiVersion: operator.tigera.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: Installation
      name: default
      uid: c849cc8b-dd17-411b-ae0e-26a4af04b1f8
    resourceVersion: "13072841"
    uid: d142b551-b1e3-4d37-b3da-4949bed29187
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: metrics-port
      port: 9094
      protocol: TCP
      targetPort: 9094
    selector:
      k8s-app: calico-kube-controllers
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-03-13T16:12:06Z"
    labels:
      k8s-app: calico-typha
    name: calico-typha
    namespace: calico-system
    ownerReferences:
    - apiVersion: operator.tigera.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: Installation
      name: default
      uid: c849cc8b-dd17-411b-ae0e-26a4af04b1f8
    resourceVersion: "13072742"
    uid: 6de3ef39-0e3e-4048-a879-3f176a437429
  spec:
    clusterIP: 10.43.192.137
    clusterIPs:
    - 10.43.192.137
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: calico-typha
      port: 5473
      protocol: TCP
      targetPort: calico-typha
    selector:
      k8s-app: calico-typha
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-01-26T07:17:59Z"
    labels:
      component: apiserver
      provider: kubernetes
    name: kubernetes
    namespace: default
    resourceVersion: "192"
    uid: 087ca602-62f0-4813-8e46-c65a934b5094
  spec:
    clusterIP: 10.43.0.1
    clusterIPs:
    - 10.43.0.1
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 6443
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: ingress-nginx
      meta.helm.sh/release-namespace: ingress-nginx
    creationTimestamp: "2024-02-21T05:23:25Z"
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.9.6
      helm.sh/chart: ingress-nginx-4.9.1
    name: ingress-nginx-controller
    namespace: ingress-nginx
    resourceVersion: "6767461"
    uid: 039019fb-a8de-4864-9c8e-186131312f79
  spec:
    clusterIP: 10.43.26.74
    clusterIPs:
    - 10.43.26.74
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - appProtocol: http
      name: http
      nodePort: 30876
      port: 80
      protocol: TCP
      targetPort: http
    - appProtocol: https
      name: https
      nodePort: 30756
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: ingress-nginx
      meta.helm.sh/release-namespace: ingress-nginx
    creationTimestamp: "2024-02-21T05:23:25Z"
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.9.6
      helm.sh/chart: ingress-nginx-4.9.1
    name: ingress-nginx-controller-admission
    namespace: ingress-nginx
    resourceVersion: "6762746"
    uid: 8d049d54-6728-41fc-a2f1-87a792b283ca
  spec:
    clusterIP: 10.43.84.138
    clusterIPs:
    - 10.43.84.138
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - appProtocol: https
      name: https-webhook
      port: 443
      protocol: TCP
      targetPort: webhook
    selector:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{"prometheus.io/port":"9153","prometheus.io/scrape":"true"},"labels":{"k8s-app":"kube-dns","kubernetes.io/cluster-service":"true","kubernetes.io/name":"CoreDNS"},"name":"kube-dns","namespace":"kube-system"},"spec":{"clusterIP":"10.43.0.10","ports":[{"name":"dns","port":53,"protocol":"UDP"},{"name":"dns-tcp","port":53,"protocol":"TCP"},{"name":"metrics","port":9153,"protocol":"TCP"}],"selector":{"k8s-app":"kube-dns"}}}
      prometheus.io/port: "9153"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-01-26T07:18:49Z"
    labels:
      k8s-app: kube-dns
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: CoreDNS
    name: kube-dns
    namespace: kube-system
    resourceVersion: "445"
    uid: 77aba0c7-7efa-4d2a-a35b-87ab08f2299c
  spec:
    clusterIP: 10.43.0.10
    clusterIPs:
    - 10.43.0.10
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: dns
      port: 53
      protocol: UDP
      targetPort: 53
    - name: dns-tcp
      port: 53
      protocol: TCP
      targetPort: 53
    - name: metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-02-02T10:31:12Z"
    labels:
      app: kube-prometheus-stack-coredns
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 55.5.1
      chart: kube-prometheus-stack-55.5.1
      heritage: Helm
      jobLabel: coredns
      release: kube-prometheus-stack
    name: kube-prometheus-stack-coredns
    namespace: kube-system
    resourceVersion: "1638207"
    uid: 4d89e517-da31-4e87-8235-85c78030b091
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-21T13:27:15Z"
    labels:
      app: kube-prometheus-stack-kube-controller-manager
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 55.5.1
      chart: kube-prometheus-stack-55.5.1
      heritage: Helm
      jobLabel: kube-controller-manager
      release: kube-prometheus-stack
    name: kube-prometheus-stack-kube-controller-manager
    namespace: kube-system
    resourceVersion: "15292816"
    uid: 7bb91039-8772-4c59-a592-60996ebba0ac
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 10257
      protocol: TCP
      targetPort: 10257
    selector:
      component: kube-controller-manager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-02-02T10:31:12Z"
    labels:
      app: kube-prometheus-stack-kube-etcd
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 55.5.1
      chart: kube-prometheus-stack-55.5.1
      heritage: Helm
      jobLabel: kube-etcd
      release: kube-prometheus-stack
    name: kube-prometheus-stack-kube-etcd
    namespace: kube-system
    resourceVersion: "1638206"
    uid: beac0945-cb87-4dd2-b7b2-55bdf75aa576
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 2381
      protocol: TCP
      targetPort: 2381
    selector:
      component: etcd
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-21T13:25:16Z"
    labels:
      app: kube-prometheus-stack-kube-scheduler
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 55.5.1
      chart: kube-prometheus-stack-55.5.1
      heritage: Helm
      jobLabel: kube-scheduler
      release: kube-prometheus-stack
    name: kube-prometheus-stack-kube-scheduler
    namespace: kube-system
    resourceVersion: "15292327"
    uid: 94cdee00-567d-4a5a-a38f-a0fdf7a50c2f
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 10259
      protocol: TCP
      targetPort: 10259
    selector:
      component: kube-scheduler
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-02-02T10:31:23Z"
    labels:
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: kubelet
      k8s-app: kubelet
    name: kube-prometheus-stack-kubelet
    namespace: kube-system
    resourceVersion: "1638481"
    uid: 9ce99b9e-36e7-4d48-88f8-1adfa806af60
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    - IPv6
    ipFamilyPolicy: RequireDualStack
    ports:
    - name: https-metrics
      port: 10250
      protocol: TCP
      targetPort: 10250
    - name: http-metrics
      port: 10255
      protocol: TCP
      targetPort: 10255
    - name: cadvisor
      port: 4194
      protocol: TCP
      targetPort: 4194
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"k8s-app":"metrics-server"},"name":"metrics-server","namespace":"kube-system"},"spec":{"ports":[{"name":"https","port":443,"protocol":"TCP","targetPort":"https"}],"selector":{"k8s-app":"metrics-server"}}}
    creationTimestamp: "2024-01-26T07:18:59Z"
    labels:
      k8s-app: metrics-server
    name: metrics-server
    namespace: kube-system
    resourceVersion: "553"
    uid: 1024bc62-7051-4dc6-a994-5366f5b1e9ca
  spec:
    clusterIP: 10.43.227.215
    clusterIPs:
    - 10.43.227.215
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      k8s-app: metrics-server
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-02-02T10:31:24Z"
    labels:
      operated-alertmanager: "true"
    name: alertmanager-operated
    namespace: monitoring
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      kind: Alertmanager
      name: kube-prometheus-stack-alertmanager
      uid: 8f669cdb-5756-4c52-8339-056f5c5dbf22
    resourceVersion: "1638493"
    uid: e616f2d5-d9cb-4e47-a1d0-a8eae2d63e62
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9093
      protocol: TCP
      targetPort: http-web
    - name: tcp-mesh
      port: 9094
      protocol: TCP
      targetPort: 9094
    - name: udp-mesh
      port: 9094
      protocol: UDP
      targetPort: 9094
    selector:
      app.kubernetes.io/name: alertmanager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-02-13T04:37:52Z"
    labels:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 10.2.2
      helm.sh/chart: grafana-7.0.19
    name: grafana-service
    namespace: monitoring
    resourceVersion: "4483804"
    uid: 38289048-c079-4ef1-97e3-0d95225e3fb7
  spec:
    clusterIP: 10.43.239.105
    clusterIPs:
    - 10.43.239.105
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 8082
      protocol: TCP
      targetPort: 3000
    selector:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/name: grafana
      pod-template-hash: 9d46b9bcd
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-02-02T10:31:12Z"
    labels:
      app: kube-prometheus-stack-alertmanager
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 55.5.1
      chart: kube-prometheus-stack-55.5.1
      heritage: Helm
      release: kube-prometheus-stack
      self-monitor: "true"
    name: kube-prometheus-stack-alertmanager
    namespace: monitoring
    resourceVersion: "1638235"
    uid: ec90f198-4f21-42d4-a369-cf50dbfc3570
  spec:
    clusterIP: 10.43.207.47
    clusterIPs:
    - 10.43.207.47
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9093
      protocol: TCP
      targetPort: 9093
    - appProtocol: http
      name: reloader-web
      port: 8080
      protocol: TCP
      targetPort: reloader-web
    selector:
      alertmanager: kube-prometheus-stack-alertmanager
      app.kubernetes.io/name: alertmanager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-02-02T10:31:12Z"
    labels:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 10.2.2
      helm.sh/chart: grafana-7.0.19
    name: kube-prometheus-stack-grafana
    namespace: monitoring
    resourceVersion: "1638212"
    uid: f9c58e0d-5ae7-4f47-bfac-14a7b68557b0
  spec:
    clusterIP: 10.43.157.251
    clusterIPs:
    - 10.43.157.251
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 80
      protocol: TCP
      targetPort: 3000
    selector:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/name: grafana
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-02-02T10:31:12Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.10.1
      helm.sh/chart: kube-state-metrics-5.15.2
      release: kube-prometheus-stack
    name: kube-prometheus-stack-kube-state-metrics
    namespace: monitoring
    resourceVersion: "1638216"
    uid: 42f8e62f-2b69-4bd9-9ca7-de0da55779a8
  spec:
    clusterIP: 10.43.25.128
    clusterIPs:
    - 10.43.25.128
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/name: kube-state-metrics
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-02-02T10:31:12Z"
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 55.5.1
      chart: kube-prometheus-stack-55.5.1
      heritage: Helm
      release: kube-prometheus-stack
    name: kube-prometheus-stack-operator
    namespace: monitoring
    resourceVersion: "1638226"
    uid: 8134d8c7-fde4-44cb-81c7-045a5d40506c
  spec:
    clusterIP: 10.43.15.187
    clusterIPs:
    - 10.43.15.187
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      app: kube-prometheus-stack-operator
      release: kube-prometheus-stack
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-02-02T10:31:12Z"
    labels:
      app: kube-prometheus-stack-prometheus
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 55.5.1
      chart: kube-prometheus-stack-55.5.1
      heritage: Helm
      release: kube-prometheus-stack
      self-monitor: "true"
    name: kube-prometheus-stack-prometheus
    namespace: monitoring
    resourceVersion: "2908871"
    uid: f681d73c-5c40-48c7-8e8d-9bb91076c7b4
  spec:
    clusterIP: 10.43.103.64
    clusterIPs:
    - 10.43.103.64
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9090
      protocol: TCP
      targetPort: 9090
    - appProtocol: http
      name: reloader-web
      port: 8080
      protocol: TCP
      targetPort: reloader-web
    selector:
      app.kubernetes.io/name: prometheus
      operator.prometheus.io/name: kube-prometheus-stack-prometheus
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-02-02T10:31:12Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.7.0
      helm.sh/chart: prometheus-node-exporter-4.24.0
      jobLabel: node-exporter
      release: kube-prometheus-stack
    name: kube-prometheus-stack-prometheus-node-exporter
    namespace: monitoring
    resourceVersion: "1638214"
    uid: 5282a168-8fce-4eeb-a01f-def885376c7d
  spec:
    clusterIP: 10.43.113.234
    clusterIPs:
    - 10.43.113.234
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 9100
      protocol: TCP
      targetPort: 9100
    selector:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/name: prometheus-node-exporter
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-02-02T10:31:24Z"
    labels:
      operated-prometheus: "true"
    name: prometheus-operated
    namespace: monitoring
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      kind: Prometheus
      name: kube-prometheus-stack-prometheus
      uid: 98d1fc63-6335-4d02-9f54-749c6a7870f8
    resourceVersion: "1638518"
    uid: 67eb1928-822e-4fd9-8b82-8f2893c16ec9
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9090
      protocol: TCP
      targetPort: http-web
    selector:
      app.kubernetes.io/name: prometheus
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-02-13T05:13:03Z"
    name: prometheus-service
    namespace: monitoring
    resourceVersion: "4491387"
    uid: c8d44cb3-4102-44ae-82af-4c5ab959d88d
  spec:
    clusterIP: 10.43.44.184
    clusterIPs:
    - 10.43.44.184
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9090
      protocol: TCP
      targetPort: 9090
    - appProtocol: http
      name: reloader-web
      port: 8080
      protocol: TCP
      targetPort: reloader-web
    selector:
      app.kubernetes.io/instance: kube-prometheus-stack-prometheus
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/version: 2.48.1
      controller-revision-hash: prometheus-kube-prometheus-stack-prometheus-76dc8d49f4
      operator.prometheus.io/name: kube-prometheus-stack-prometheus
      operator.prometheus.io/shard: "0"
      prometheus: kube-prometheus-stack-prometheus
      statefulset.kubernetes.io/pod-name: prometheus-kube-prometheus-stack-prometheus-0
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-01-31T12:42:36Z"
    labels:
      app: rook-ceph-mgr
      rook_cluster: rook-ceph
    name: rook-ceph-mgr
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 02274ff6-97d3-4d09-b941-31edb9bdad99
    resourceVersion: "1139787"
    uid: 1b001c6c-b4da-4d80-be2d-2e3383d5b665
  spec:
    clusterIP: 10.43.78.104
    clusterIPs:
    - 10.43.78.104
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 9283
      protocol: TCP
      targetPort: 9283
    selector:
      app: rook-ceph-mgr
      mgr_role: active
      rook_cluster: rook-ceph
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-01-31T12:42:36Z"
    labels:
      app: rook-ceph-mgr
      rook_cluster: rook-ceph
    name: rook-ceph-mgr-dashboard
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 02274ff6-97d3-4d09-b941-31edb9bdad99
    resourceVersion: "13085953"
    uid: 6acdc731-cfe2-418f-a4f7-f772da09dd81
  spec:
    clusterIP: 10.43.159.173
    clusterIPs:
    - 10.43.159.173
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https-dashboard
      port: 8443
      protocol: TCP
      targetPort: 8443
    selector:
      app: rook-ceph-mgr
      mgr_role: active
      rook_cluster: rook-ceph
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-01-31T12:41:14Z"
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: a
      ceph_daemon_type: mon
      mon: a
      mon_cluster: rook-ceph
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-a
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      kind: CephCluster
      name: rook-ceph
      uid: 02274ff6-97d3-4d09-b941-31edb9bdad99
    resourceVersion: "1139111"
    uid: be46c8b3-de78-4877-900c-0e4a6d7a4b24
  spec:
    clusterIP: 10.43.110.147
    clusterIPs:
    - 10.43.110.147
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-msgr1
      port: 6789
      protocol: TCP
      targetPort: 6789
    - name: tcp-msgr2
      port: 3300
      protocol: TCP
      targetPort: 3300
    selector:
      app: rook-ceph-mon
      ceph_daemon_id: a
      mon: a
      mon_cluster: rook-ceph
      rook_cluster: rook-ceph
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-01-31T12:41:55Z"
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: c
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: c
      ceph_daemon_type: mon
      mon: c
      mon_cluster: rook-ceph
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-c
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      kind: CephCluster
      name: rook-ceph
      uid: 02274ff6-97d3-4d09-b941-31edb9bdad99
    resourceVersion: "1139386"
    uid: 5731ec0b-cda1-46f4-9bf1-aee8173c8e5c
  spec:
    clusterIP: 10.43.221.137
    clusterIPs:
    - 10.43.221.137
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-msgr1
      port: 6789
      protocol: TCP
      targetPort: 6789
    - name: tcp-msgr2
      port: 3300
      protocol: TCP
      targetPort: 3300
    selector:
      app: rook-ceph-mon
      ceph_daemon_id: c
      mon: c
      mon_cluster: rook-ceph
      rook_cluster: rook-ceph
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-02-28T07:57:57Z"
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: e
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: e
      ceph_daemon_type: mon
      mon: e
      mon_cluster: rook-ceph
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-e
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      kind: CephCluster
      name: rook-ceph
      uid: 02274ff6-97d3-4d09-b941-31edb9bdad99
    resourceVersion: "9061660"
    uid: dc19ba25-bbf0-479b-a317-445e549cc94c
  spec:
    clusterIP: 10.43.19.85
    clusterIPs:
    - 10.43.19.85
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-msgr1
      port: 6789
      protocol: TCP
      targetPort: 6789
    - name: tcp-msgr2
      port: 3300
      protocol: TCP
      targetPort: 3300
    selector:
      app: rook-ceph-mon
      ceph_daemon_id: e
      mon: e
      mon_cluster: rook-ceph
      rook_cluster: rook-ceph
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-01-31T12:45:31Z"
    labels:
      app: rook-ceph-rgw
      app.kubernetes.io/component: cephobjectstores.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: ceph-objectstore
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-rgw
      app.kubernetes.io/part-of: ceph-objectstore
      ceph_daemon_id: ceph-objectstore
      ceph_daemon_type: rgw
      rgw: ceph-objectstore
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      rook_object_store: ceph-objectstore
    name: rook-ceph-rgw-ceph-objectstore
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephObjectStore
      name: ceph-objectstore
      uid: 6f4d07cd-379c-4e9f-8d20-2795daa08703
    resourceVersion: "1140513"
    uid: c1e85967-e63a-4aa1-a33f-3b21659978bc
  spec:
    clusterIP: 10.43.144.1
    clusterIPs:
    - 10.43.144.1
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 8080
    selector:
      app: rook-ceph-rgw
      ceph_daemon_id: ceph-objectstore
      rgw: ceph-objectstore
      rook_cluster: rook-ceph
      rook_object_store: ceph-objectstore
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2024-03-13T16:12:07Z"
    generation: 1
    name: calico-node
    namespace: calico-system
    ownerReferences:
    - apiVersion: operator.tigera.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: Installation
      name: default
      uid: c849cc8b-dd17-411b-ae0e-26a4af04b1f8
    resourceVersion: "13082265"
    uid: 82403527-ae8d-4ba4-a2a0-e2ebe48bf965
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: calico-node
    template:
      metadata:
        annotations:
          hash.operator.tigera.io/cni-config: 67d9927589bcf84d027e873bcb62d5ee6677ab82
          hash.operator.tigera.io/system: fdde45054a8ae4f629960ce37570929502e59449
          tigera-operator.hash.operator.tigera.io/tigera-ca-private: 4f5267a1bedc8badce3a9288fba115ecfdb158ed
        creationTimestamp: null
        labels:
          app.kubernetes.io/name: calico-node
          k8s-app: calico-node
      spec:
        containers:
        - env:
          - name: DATASTORE_TYPE
            value: kubernetes
          - name: WAIT_FOR_DATASTORE
            value: "true"
          - name: CLUSTER_TYPE
            value: k8s,operator,bgp
          - name: CALICO_DISABLE_FILE_LOGGING
            value: "false"
          - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
            value: ACCEPT
          - name: FELIX_HEALTHENABLED
            value: "true"
          - name: FELIX_HEALTHPORT
            value: "9099"
          - name: NODENAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: FELIX_TYPHAK8SNAMESPACE
            value: calico-system
          - name: FELIX_TYPHAK8SSERVICENAME
            value: calico-typha
          - name: FELIX_TYPHACAFILE
            value: /etc/pki/tls/certs/tigera-ca-bundle.crt
          - name: FELIX_TYPHACERTFILE
            value: /node-certs/tls.crt
          - name: FELIX_TYPHAKEYFILE
            value: /node-certs/tls.key
          - name: FIPS_MODE_ENABLED
            value: "false"
          - name: FELIX_TYPHACN
            value: typha-server
          - name: CALICO_MANAGE_CNI
            value: "true"
          - name: CALICO_IPV4POOL_CIDR
            value: 10.42.0.0/16
          - name: CALICO_IPV4POOL_VXLAN
            value: CrossSubnet
          - name: CALICO_IPV4POOL_BLOCK_SIZE
            value: "26"
          - name: CALICO_IPV4POOL_NODE_SELECTOR
            value: all()
          - name: CALICO_IPV4POOL_DISABLE_BGP_EXPORT
            value: "false"
          - name: CALICO_NETWORKING_BACKEND
            value: bird
          - name: IP
            value: autodetect
          - name: IP_AUTODETECTION_METHOD
            value: first-found
          - name: IP6
            value: none
          - name: FELIX_IPV6SUPPORT
            value: "false"
          - name: KUBERNETES_SERVICE_HOST
            value: 10.43.0.1
          - name: KUBERNETES_SERVICE_PORT
            value: "443"
          image: docker.io/calico/node:master
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/calico-node
                - -shutdown
          livenessProbe:
            failureThreshold: 3
            httpGet:
              host: localhost
              path: /liveness
              port: 9099
              scheme: HTTP
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 10
          name: calico-node
          readinessProbe:
            exec:
              command:
              - /bin/calico-node
              - -bird-ready
              - -felix-ready
            failureThreshold: 3
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          resources: {}
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              drop:
              - ALL
            privileged: true
            runAsGroup: 0
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/pki/tls/certs
            name: tigera-ca-bundle
            readOnly: true
          - mountPath: /etc/pki/tls/cert.pem
            name: tigera-ca-bundle
            readOnly: true
            subPath: ca-bundle.crt
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /run/xtables.lock
            name: xtables-lock
          - mountPath: /var/run/nodeagent
            name: policysync
          - mountPath: /node-certs
            name: node-certs
            readOnly: true
          - mountPath: /var/run/calico
            name: var-run-calico
          - mountPath: /var/lib/calico
            name: var-lib-calico
          - mountPath: /var/log/calico/cni
            name: cni-log-dir
          - mountPath: /host/etc/cni/net.d
            name: cni-net-dir
        dnsPolicy: ClusterFirst
        hostNetwork: true
        initContainers:
        - image: docker.io/calico/pod2daemon-flexvol:master
          imagePullPolicy: IfNotPresent
          name: flexvol-driver
          resources: {}
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              drop:
              - ALL
            privileged: true
            runAsGroup: 0
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/driver
            name: flexvol-driver-host
        - command:
          - /opt/cni/bin/install
          env:
          - name: CNI_CONF_NAME
            value: 10-calico.conflist
          - name: SLEEP
            value: "false"
          - name: CNI_NET_DIR
            value: /etc/cni/net.d
          - name: CNI_NETWORK_CONFIG
            valueFrom:
              configMapKeyRef:
                key: config
                name: cni-config
          - name: KUBERNETES_SERVICE_HOST
            value: 10.43.0.1
          - name: KUBERNETES_SERVICE_PORT
            value: "443"
          image: docker.io/calico/cni:master
          imagePullPolicy: IfNotPresent
          name: install-cni
          resources: {}
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              drop:
              - ALL
            privileged: true
            runAsGroup: 0
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/opt/cni/bin
            name: cni-bin-dir
          - mountPath: /host/etc/cni/net.d
            name: cni-net-dir
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-node
        serviceAccountName: calico-node
        terminationGracePeriodSeconds: 5
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          operator: Exists
        - effect: NoExecute
          operator: Exists
        volumes:
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
        - hostPath:
            path: /var/run/nodeagent
            type: DirectoryOrCreate
          name: policysync
        - configMap:
            defaultMode: 420
            name: tigera-ca-bundle
          name: tigera-ca-bundle
        - name: node-certs
          secret:
            defaultMode: 420
            secretName: node-certs
        - hostPath:
            path: /var/run/calico
            type: ""
          name: var-run-calico
        - hostPath:
            path: /var/lib/calico
            type: ""
          name: var-lib-calico
        - hostPath:
            path: /opt/cni/bin
            type: ""
          name: cni-bin-dir
        - hostPath:
            path: /etc/cni/net.d
            type: ""
          name: cni-net-dir
        - hostPath:
            path: /var/log/calico/cni
            type: ""
          name: cni-log-dir
        - hostPath:
            path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds
            type: DirectoryOrCreate
          name: flexvol-driver-host
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 7
    desiredNumberScheduled: 7
    numberAvailable: 7
    numberMisscheduled: 0
    numberReady: 7
    observedGeneration: 1
    updatedNumberScheduled: 7
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2024-03-13T16:12:07Z"
    generation: 1
    name: csi-node-driver
    namespace: calico-system
    ownerReferences:
    - apiVersion: operator.tigera.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: Installation
      name: default
      uid: c849cc8b-dd17-411b-ae0e-26a4af04b1f8
    resourceVersion: "13081946"
    uid: fdae6af8-076d-4c08-9d47-2295f1381898
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: csi-node-driver
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/name: csi-node-driver
          k8s-app: csi-node-driver
          name: csi-node-driver
      spec:
        containers:
        - args:
          - --nodeid=$(KUBE_NODE_NAME)
          - --loglevel=$(LOG_LEVEL)
          env:
          - name: LOG_LEVEL
            value: warn
          - name: KUBE_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: docker.io/calico/csi:master
          imagePullPolicy: IfNotPresent
          name: calico-csi
          resources: {}
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              drop:
              - ALL
            privileged: true
            runAsGroup: 0
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/run
            name: varrun
          - mountPath: /csi
            name: socket-dir
          - mountPath: /var/lib/kubelet
            mountPropagation: Bidirectional
            name: kubelet-dir
        - args:
          - --v=5
          - --csi-address=$(ADDRESS)
          - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)
          env:
          - name: ADDRESS
            value: /csi/csi.sock
          - name: DRIVER_REG_SOCK_PATH
            value: /var/lib/kubelet/plugins/csi.tigera.io/csi.sock
          - name: KUBE_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: docker.io/calico/node-driver-registrar:master
          imagePullPolicy: IfNotPresent
          name: csi-node-driver-registrar
          resources: {}
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              drop:
              - ALL
            privileged: true
            runAsGroup: 0
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
          - mountPath: /registration
            name: registration-dir
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          operator: Exists
        - effect: NoExecute
          operator: Exists
        volumes:
        - hostPath:
            path: /var/run
            type: ""
          name: varrun
        - hostPath:
            path: /var/lib/kubelet
            type: Directory
          name: kubelet-dir
        - hostPath:
            path: /var/lib/kubelet/plugins/csi.tigera.io
            type: DirectoryOrCreate
          name: socket-dir
        - hostPath:
            path: /var/lib/kubelet/plugins_registry
            type: Directory
          name: registration-dir
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 7
    desiredNumberScheduled: 7
    numberAvailable: 7
    numberMisscheduled: 0
    numberReady: 7
    observedGeneration: 1
    updatedNumberScheduled: 7
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-02-02T10:31:12Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.7.0
      helm.sh/chart: prometheus-node-exporter-4.24.0
      jobLabel: node-exporter
      release: kube-prometheus-stack
    name: kube-prometheus-stack-prometheus-node-exporter
    namespace: monitoring
    resourceVersion: "13081610"
    uid: 437693ec-8560-429d-9ac6-3e2d4f8b1c2b
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: prometheus-node-exporter
    template:
      metadata:
        annotations:
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-node-exporter
          app.kubernetes.io/part-of: prometheus-node-exporter
          app.kubernetes.io/version: 1.7.0
          helm.sh/chart: prometheus-node-exporter-4.24.0
          jobLabel: node-exporter
          release: kube-prometheus-stack
      spec:
        automountServiceAccountToken: false
        containers:
        - args:
          - --path.procfs=/host/proc
          - --path.sysfs=/host/sys
          - --path.rootfs=/host/root
          - --path.udev.data=/host/root/run/udev/data
          - --web.listen-address=[$(HOST_IP)]:9100
          - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
          - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
          env:
          - name: HOST_IP
            value: 0.0.0.0
          image: quay.io/prometheus/node-exporter:v1.7.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 9100
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: node-exporter
          ports:
          - containerPort: 9100
            hostPort: 9100
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 9100
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/proc
            name: proc
            readOnly: true
          - mountPath: /host/sys
            name: sys
            readOnly: true
          - mountPath: /host/root
            mountPropagation: HostToContainer
            name: root
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        hostPID: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: kube-prometheus-stack-prometheus-node-exporter
        serviceAccountName: kube-prometheus-stack-prometheus-node-exporter
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          operator: Exists
        volumes:
        - hostPath:
            path: /proc
            type: ""
          name: proc
        - hostPath:
            path: /sys
            type: ""
          name: sys
        - hostPath:
            path: /
            type: ""
          name: root
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 4
    desiredNumberScheduled: 4
    numberAvailable: 4
    numberMisscheduled: 0
    numberReady: 4
    observedGeneration: 1
    updatedNumberScheduled: 4
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2024-02-12T11:44:41Z"
    generation: 1
    name: csi-cephfsplugin
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: false
      controller: true
      kind: Deployment
      name: rook-ceph-operator
      uid: 7a3cf0ff-4a2c-4d49-8d00-fbf6db090b18
    resourceVersion: "13085266"
    uid: 5366696a-b570-4ac6-8e32-91d05178bbf4
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: csi-cephfsplugin
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: csi-cephfsplugin
          contains: csi-cephfsplugin-metrics
      spec:
        affinity:
          nodeAffinity: {}
        containers:
        - args:
          - --v=0
          - --csi-address=/csi/csi.sock
          - --kubelet-registration-path=/var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/csi.sock
          env:
          - name: KUBE_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.1
          imagePullPolicy: IfNotPresent
          name: driver-registrar
          resources:
            limits:
              cpu: 100m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          securityContext:
            capabilities:
              drop:
              - ALL
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: plugin-dir
          - mountPath: /registration
            name: registration-dir
        - args:
          - --nodeid=$(NODE_ID)
          - --type=cephfs
          - --endpoint=$(CSI_ENDPOINT)
          - --v=0
          - --nodeserver=true
          - --drivername=rook-ceph.cephfs.csi.ceph.com
          - --pidlimit=-1
          - --forcecephkernelclient=true
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CSI_ENDPOINT
            value: unix:///csi/csi.sock
          image: quay.io/cephcsi/cephcsi:v3.10.1
          imagePullPolicy: IfNotPresent
          name: csi-cephfsplugin
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              add:
              - SYS_ADMIN
              drop:
              - ALL
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: plugin-dir
          - mountPath: /csi/mountinfo
            name: ceph-csi-mountinfo
          - mountPath: /var/lib/kubelet/plugins
            mountPropagation: Bidirectional
            name: csi-plugins-dir
          - mountPath: /var/lib/kubelet/pods
            mountPropagation: Bidirectional
            name: pods-mount-dir
          - mountPath: /sys
            name: host-sys
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /dev
            name: host-dev
          - mountPath: /etc/ceph-csi-config/
            name: ceph-csi-config
          - mountPath: /tmp/csi/keys
            name: keys-tmp-dir
          - mountPath: /run/mount
            name: host-run-mount
        dnsPolicy: ClusterFirstWithHostNet
        hostNetwork: true
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-csi-cephfs-plugin-sa
        serviceAccountName: rook-csi-cephfs-plugin-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/mountinfo
            type: DirectoryOrCreate
          name: ceph-csi-mountinfo
        - hostPath:
            path: /var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/
            type: DirectoryOrCreate
          name: plugin-dir
        - hostPath:
            path: /var/lib/kubelet/plugins
            type: Directory
          name: csi-plugins-dir
        - hostPath:
            path: /var/lib/kubelet/plugins_registry/
            type: Directory
          name: registration-dir
        - hostPath:
            path: /var/lib/kubelet/pods
            type: Directory
          name: pods-mount-dir
        - hostPath:
            path: /sys
            type: ""
          name: host-sys
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - hostPath:
            path: /dev
            type: ""
          name: host-dev
        - configMap:
            defaultMode: 420
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
          name: ceph-csi-config
        - emptyDir:
            medium: Memory
          name: keys-tmp-dir
        - hostPath:
            path: /run/mount
            type: ""
          name: host-run-mount
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 4
    desiredNumberScheduled: 4
    numberAvailable: 4
    numberMisscheduled: 0
    numberReady: 4
    observedGeneration: 1
    updatedNumberScheduled: 4
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2024-01-31T12:41:09Z"
    generation: 1
    name: csi-rbdplugin
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: false
      controller: true
      kind: Deployment
      name: rook-ceph-operator
      uid: 7a3cf0ff-4a2c-4d49-8d00-fbf6db090b18
    resourceVersion: "13085325"
    uid: 6ce54ca4-f627-468b-82db-9479e25076fb
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: csi-rbdplugin
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: csi-rbdplugin
          contains: csi-rbdplugin-metrics
      spec:
        affinity:
          nodeAffinity: {}
        containers:
        - args:
          - --v=0
          - --csi-address=/csi/csi.sock
          - --kubelet-registration-path=/var/lib/kubelet/plugins/rook-ceph.rbd.csi.ceph.com/csi.sock
          env:
          - name: KUBE_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.1
          imagePullPolicy: IfNotPresent
          name: driver-registrar
          resources:
            limits:
              cpu: 100m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          securityContext:
            capabilities:
              drop:
              - ALL
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: plugin-dir
          - mountPath: /registration
            name: registration-dir
        - args:
          - --nodeid=$(NODE_ID)
          - --endpoint=$(CSI_ENDPOINT)
          - --v=0
          - --type=rbd
          - --nodeserver=true
          - --drivername=rook-ceph.rbd.csi.ceph.com
          - --pidlimit=-1
          - --stagingpath=/var/lib/kubelet/plugins/kubernetes.io/csi/
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CSI_ENDPOINT
            value: unix:///csi/csi.sock
          image: quay.io/cephcsi/cephcsi:v3.10.1
          imagePullPolicy: IfNotPresent
          name: csi-rbdplugin
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              add:
              - SYS_ADMIN
              drop:
              - ALL
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: plugin-dir
          - mountPath: /var/lib/kubelet/pods
            mountPropagation: Bidirectional
            name: pods-mount-dir
          - mountPath: /var/lib/kubelet/plugins
            mountPropagation: Bidirectional
            name: plugin-mount-dir
          - mountPath: /dev
            name: host-dev
          - mountPath: /sys
            name: host-sys
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /etc/ceph-csi-config/
            name: ceph-csi-configs
          - mountPath: /tmp/csi/keys
            name: keys-tmp-dir
          - mountPath: /run/mount
            name: host-run-mount
          - mountPath: /run/secrets/tokens
            name: oidc-token
            readOnly: true
        dnsPolicy: ClusterFirstWithHostNet
        hostNetwork: true
        hostPID: true
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-csi-rbd-plugin-sa
        serviceAccountName: rook-csi-rbd-plugin-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/kubelet/plugins/rook-ceph.rbd.csi.ceph.com
            type: DirectoryOrCreate
          name: plugin-dir
        - hostPath:
            path: /var/lib/kubelet/plugins
            type: Directory
          name: plugin-mount-dir
        - hostPath:
            path: /var/lib/kubelet/plugins_registry/
            type: Directory
          name: registration-dir
        - hostPath:
            path: /var/lib/kubelet/pods
            type: Directory
          name: pods-mount-dir
        - hostPath:
            path: /dev
            type: ""
          name: host-dev
        - hostPath:
            path: /sys
            type: ""
          name: host-sys
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - name: ceph-csi-configs
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: csi-cluster-config-json
                  path: config.json
                name: rook-ceph-csi-config
            - configMap:
                items:
                - key: csi-mapping-config-json
                  path: cluster-mapping.json
                name: rook-ceph-csi-mapping-config
        - emptyDir:
            medium: Memory
          name: keys-tmp-dir
        - hostPath:
            path: /run/mount
            type: ""
          name: host-run-mount
        - name: oidc-token
          projected:
            defaultMode: 420
            sources:
            - serviceAccountToken:
                audience: ceph-csi-kms
                expirationSeconds: 3600
                path: oidc-token
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 4
    desiredNumberScheduled: 4
    numberAvailable: 4
    numberMisscheduled: 0
    numberReady: 4
    observedGeneration: 1
    updatedNumberScheduled: 4
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "25"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-02-13T10:42:14Z"
    generation: 25
    labels:
      app: airflow
      app.kubernetes.io/managed-by: Helm
      chart: airflow-8.8.0
      component: db-migrations
      heritage: Helm
      release: airflow
    name: airflow-db-migrations
    namespace: airflow
    resourceVersion: "13231687"
    uid: c39f7ef6-f9c4-4aea-a1b6-59e4ec4d8015
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: airflow
        component: db-migrations
        release: airflow
    strategy:
      type: Recreate
    template:
      metadata:
        annotations:
          checksum/db-migrations-script: 37898f38b90abd06081105d992362ec9e0d0015123b69e758e59031a9e6ddfc9
          checksum/secret-config-envs: 7b5b4b568e7c694c0f44651459436d8876c0d6f337d3876162036cffceec3b68
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: db-migrations
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - python
          - -u
          - /mnt/scripts/db_migrations.py
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.11
          imagePullPolicy: IfNotPresent
          name: db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /mnt/scripts
            name: scripts
            readOnly: true
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.11
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-logs
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: scripts
          secret:
            defaultMode: 420
            secretName: airflow-db-migrations
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-03-14T05:13:28Z"
      lastUpdateTime: "2024-03-14T05:13:28Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-03-06T14:18:10Z"
      lastUpdateTime: "2024-03-14T05:13:28Z"
      message: ReplicaSet "airflow-db-migrations-77b99788cf" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 25
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "17"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-02-13T10:42:14Z"
    generation: 17
    labels:
      app: airflow
      app.kubernetes.io/managed-by: Helm
      chart: airflow-8.8.0
      component: pgbouncer
      heritage: Helm
      release: airflow
    name: airflow-pgbouncer
    namespace: airflow
    resourceVersion: "13231439"
    uid: c1b2bef6-58f7-4144-b0ff-e1c0c351766d
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: airflow
        component: pgbouncer
        release: airflow
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 7b5b4b568e7c694c0f44651459436d8876c0d6f337d3876162036cffceec3b68
          checksum/secret-pgbouncer: cbb28505a66ba6d70bfda4f3495790f613816b1e9ac5761a955018cdb67ad44e
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: pgbouncer
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - /bin/sh
          - -c
          - |-
            /home/pgbouncer/config/gen_self_signed_cert.sh && \
            /home/pgbouncer/config/gen_auth_file.sh && \
            exec pgbouncer /home/pgbouncer/config/pgbouncer.ini
          command:
          - /usr/bin/dumb-init
          - --rewrite=15:2
          - --
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: ghcr.io/airflow-helm/pgbouncer:1.18.0-patch.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - psql $(eval $DATABASE_PSQL_CMD) --tuples-only --command="SELECT 1;"
                | grep -q "1"
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: pgbouncer
          ports:
          - containerPort: 6432
            name: pgbouncer
            protocol: TCP
          resources: {}
          securityContext:
            runAsGroup: 1001
            runAsUser: 1001
          startupProbe:
            failureThreshold: 30
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 6432
            timeoutSeconds: 15
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /home/pgbouncer/config
            name: pgbouncer-config
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 120
        volumes:
        - name: pgbouncer-config
          secret:
            defaultMode: 420
            items:
            - key: gen_auth_file.sh
              mode: 493
              path: gen_auth_file.sh
            - key: gen_self_signed_cert.sh
              mode: 493
              path: gen_self_signed_cert.sh
            - key: pgbouncer.ini
              path: pgbouncer.ini
            secretName: airflow-pgbouncer
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-03-13T16:56:07Z"
      lastUpdateTime: "2024-03-13T16:56:07Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-02-15T04:26:50Z"
      lastUpdateTime: "2024-03-14T05:12:24Z"
      message: ReplicaSet "airflow-pgbouncer-55f8877f8c" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 17
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "25"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-02-13T10:42:14Z"
    generation: 25
    labels:
      app: airflow
      app.kubernetes.io/managed-by: Helm
      chart: airflow-8.8.0
      component: scheduler
      heritage: Helm
      release: airflow
    name: airflow-scheduler
    namespace: airflow
    resourceVersion: "13231823"
    uid: e92ea920-8ca8-4806-ad4e-ff5a9af671c1
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: airflow
        component: scheduler
        release: airflow
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config-pod-template: 1a8683c073a2d13ce4ef577bf3256def776cd1e7666f47cc1d08d00f446fe1c9
          checksum/secret-config-envs: 7b5b4b568e7c694c0f44651459436d8876c0d6f337d3876162036cffceec3b68
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: scheduler
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow scheduler -n -1
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/dumb-init
              - --
              - /entrypoint
              - python
              - -Wignore
              - -c
              - |
                import os
                import sys

                # suppress logs triggered from importing airflow packages
                os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                # shared imports
                try:
                    from airflow.jobs.job import Job
                except ImportError:
                    # `BaseJob` was renamed to `Job` in airflow 2.6.0
                    from airflow.jobs.base_job import BaseJob as Job
                from airflow.utils.db import create_session
                from airflow.utils.net import get_hostname

                # heartbeat check imports
                try:
                    from airflow.jobs.scheduler_job_runner import SchedulerJobRunner
                except ImportError:
                    # `SchedulerJob` is wrapped by `SchedulerJobRunner` since airflow 2.6.0
                    from airflow.jobs.scheduler_job import SchedulerJob as SchedulerJobRunner

                with create_session() as session:
                    ########################
                    # heartbeat check
                    ########################
                    # ensure the SchedulerJob with most recent heartbeat for this `hostname` is alive
                    hostname = get_hostname()
                    scheduler_job = session \
                        .query(Job) \
                        .filter_by(job_type=SchedulerJobRunner.job_type) \
                        .filter_by(hostname=hostname) \
                        .order_by(Job.latest_heartbeat.desc()) \
                        .limit(1) \
                        .first()
                    if (scheduler_job is not None) and scheduler_job.is_alive():
                        pass
                    else:
                        sys.exit(f"The SchedulerJob (id={scheduler_job.id}) for hostname '{hostname}' is not alive")
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: airflow-scheduler
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /opt/airflow/pod_templates/pod_template.yaml
            name: pod-template
            readOnly: true
            subPath: pod_template.yaml
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.11
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.11
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-logs
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - configMap:
            defaultMode: 420
            name: airflow-pod-template
          name: pod-template
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-03-13T16:56:42Z"
      lastUpdateTime: "2024-03-13T16:56:42Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-03-06T14:18:10Z"
      lastUpdateTime: "2024-03-14T05:13:44Z"
      message: ReplicaSet "airflow-scheduler-54bf954c6d" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 25
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "25"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-02-13T10:42:14Z"
    generation: 25
    labels:
      app: airflow
      app.kubernetes.io/managed-by: Helm
      chart: airflow-8.8.0
      component: sync-users
      heritage: Helm
      release: airflow
    name: airflow-sync-users
    namespace: airflow
    resourceVersion: "13231784"
    uid: 29b6ea14-c5af-46dd-9172-07088904cb02
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: airflow
        component: sync-users
        release: airflow
    strategy:
      type: Recreate
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 7b5b4b568e7c694c0f44651459436d8876c0d6f337d3876162036cffceec3b68
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          checksum/sync-users-script: 0e7bdc9596783274f1496d1dc37fa2eda2d72d60a2b56e3b61274b347f7e10ee
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: sync-users
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - python
          - -u
          - /mnt/scripts/sync_users.py
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.11
          imagePullPolicy: IfNotPresent
          name: sync-airflow-users
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /mnt/scripts
            name: scripts
            readOnly: true
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.11
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.11
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-logs
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: scripts
          secret:
            defaultMode: 420
            secretName: airflow-sync-users
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-03-14T05:13:41Z"
      lastUpdateTime: "2024-03-14T05:13:41Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-03-06T14:18:10Z"
      lastUpdateTime: "2024-03-14T05:13:41Z"
      message: ReplicaSet "airflow-sync-users-7ffb469495" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 25
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "25"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-02-13T10:42:14Z"
    generation: 25
    labels:
      app: airflow
      app.kubernetes.io/managed-by: Helm
      chart: airflow-8.8.0
      component: triggerer
      heritage: Helm
      release: airflow
    name: airflow-triggerer
    namespace: airflow
    resourceVersion: "13231750"
    uid: df1ec31d-c210-44a1-8a99-70bd09b513ca
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: airflow
        component: triggerer
        release: airflow
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 7b5b4b568e7c694c0f44651459436d8876c0d6f337d3876162036cffceec3b68
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: triggerer
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow triggerer
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/dumb-init
              - --
              - /entrypoint
              - python
              - -Wignore
              - -c
              - |
                import os
                import sys

                # suppress logs triggered from importing airflow packages
                os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                # shared imports
                try:
                    from airflow.jobs.job import Job
                except ImportError:
                    # `BaseJob` was renamed to `Job` in airflow 2.6.0
                    from airflow.jobs.base_job import BaseJob as Job
                from airflow.utils.db import create_session
                from airflow.utils.net import get_hostname

                # heartbeat check imports
                try:
                    from airflow.jobs.triggerer_job_runner import TriggererJobRunner
                except ImportError:
                    # `TriggererJob` is wrapped by `TriggererJobRunner` since airflow 2.6.0
                    from airflow.jobs.triggerer_job import TriggererJob as TriggererJobRunner

                with create_session() as session:
                    # ensure the TriggererJob with most recent heartbeat for this `hostname` is alive
                    hostname = get_hostname()
                    triggerer_job = session \
                        .query(Job) \
                        .filter_by(job_type=TriggererJobRunner.job_type) \
                        .filter_by(hostname=hostname) \
                        .order_by(Job.latest_heartbeat.desc()) \
                        .limit(1) \
                        .first()
                    if (triggerer_job is not None) and triggerer_job.is_alive():
                        pass
                    else:
                        sys.exit(f"The TriggererJob (id={triggerer_job.id}) for hostname '{hostname}' is not alive")
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: airflow-triggerer
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.11
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.11
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-logs
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-03-13T16:57:09Z"
      lastUpdateTime: "2024-03-13T16:57:09Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-02-28T08:31:26Z"
      lastUpdateTime: "2024-03-14T05:13:36Z"
      message: ReplicaSet "airflow-triggerer-54b68d597f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 25
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "25"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-02-13T10:42:14Z"
    generation: 25
    labels:
      app: airflow
      app.kubernetes.io/managed-by: Helm
      chart: airflow-8.8.0
      component: web
      heritage: Helm
      release: airflow
    name: airflow-web
    namespace: airflow
    resourceVersion: "13232000"
    uid: 4522bcf9-665c-4869-9368-7c5ca6500338
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: airflow
        component: web
        release: airflow
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config-webserver-config: 08f3293f0aaa3978bcbcae61eee58760bcba270cf691c5ff9c788d678ba8d579
          checksum/secret-config-envs: 7b5b4b568e7c694c0f44651459436d8876c0d6f337d3876162036cffceec3b68
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: web
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow webserver
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /airflow/health
              port: web
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: airflow-web
          ports:
          - containerPort: 8080
            name: web
            protocol: TCP
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /airflow/health
              port: web
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /opt/airflow/webserver_config.py
            name: webserver-config
            readOnly: true
            subPath: webserver_config.py
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.11
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.11
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-logs
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: webserver-config
          secret:
            defaultMode: 420
            secretName: airflow-webserver-config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-03-13T16:58:03Z"
      lastUpdateTime: "2024-03-13T16:58:03Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-02-28T08:32:41Z"
      lastUpdateTime: "2024-03-14T05:14:29Z"
      message: ReplicaSet "airflow-web-5c757c7656" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 25
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: postgresql-ha
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-02-13T12:27:05Z"
    generation: 1
    labels:
      app.kubernetes.io/component: pgpool
      app.kubernetes.io/instance: postgresql-ha
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql-ha
      app.kubernetes.io/version: 4.5.0
      helm.sh/chart: postgresql-ha-13.2.3
    name: postgresql-ha-pgpool
    namespace: airflow
    resourceVersion: "13087971"
    uid: c051ad9a-2262-4220-9062-0ee86cd88699
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: pgpool
        app.kubernetes.io/instance: postgresql-ha
        app.kubernetes.io/name: postgresql-ha
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: pgpool
          app.kubernetes.io/instance: postgresql-ha
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: postgresql-ha
          app.kubernetes.io/version: 4.5.0
          helm.sh/chart: postgresql-ha-13.2.3
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/component: pgpool
                  app.kubernetes.io/instance: postgresql-ha
                  app.kubernetes.io/name: postgresql-ha
              topologyKey: kubernetes.io/hostname
        automountServiceAccountToken: false
        containers:
        - env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: PGPOOL_BACKEND_NODES
            value: 0:postgresql-ha-postgresql-0.postgresql-ha-postgresql-headless:5432,1:postgresql-ha-postgresql-1.postgresql-ha-postgresql-headless:5432,2:postgresql-ha-postgresql-2.postgresql-ha-postgresql-headless:5432,
          - name: PGPOOL_SR_CHECK_USER
            value: repmgr
          - name: PGPOOL_SR_CHECK_PASSWORD
            valueFrom:
              secretKeyRef:
                key: repmgr-password
                name: postgresql-ha-postgresql
          - name: PGPOOL_SR_CHECK_DATABASE
            value: postgres
          - name: PGPOOL_ENABLE_LDAP
            value: "no"
          - name: PGPOOL_POSTGRES_USERNAME
            value: postgres
          - name: PGPOOL_POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: PGPOOL_ADMIN_USERNAME
            value: admin
          - name: PGPOOL_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: postgresql-ha-pgpool
          - name: PGPOOL_AUTHENTICATION_METHOD
            value: scram-sha-256
          - name: PGPOOL_ENABLE_LOAD_BALANCING
            value: "yes"
          - name: PGPOOL_DISABLE_LOAD_BALANCE_ON_WRITE
            value: transaction
          - name: PGPOOL_ENABLE_LOG_CONNECTIONS
            value: "no"
          - name: PGPOOL_ENABLE_LOG_HOSTNAME
            value: "yes"
          - name: PGPOOL_ENABLE_LOG_PER_NODE_STATEMENT
            value: "no"
          - name: PGPOOL_RESERVED_CONNECTIONS
            value: "1"
          - name: PGPOOL_CHILD_LIFE_TIME
          - name: PGPOOL_ENABLE_TLS
            value: "no"
          - name: PGPOOL_HEALTH_CHECK_PSQL_TIMEOUT
            value: "6"
          image: docker.io/bitnami/pgpool:4.5.0-debian-11-r4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /opt/bitnami/scripts/pgpool/healthcheck.sh
            failureThreshold: 5
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: pgpool
          ports:
          - containerPort: 5432
            name: postgresql
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - bash
              - -ec
              - PGPASSWORD=${PGPOOL_POSTGRES_PASSWORD} psql -U "postgres" -d "airflow"
                -h /opt/bitnami/pgpool/tmp -tA -c "SELECT 1" >/dev/null
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          fsGroupChangePolicy: Always
        serviceAccount: postgresql-ha
        serviceAccountName: postgresql-ha
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-02-13T12:27:05Z"
      lastUpdateTime: "2024-02-13T12:32:40Z"
      message: ReplicaSet "postgresql-ha-pgpool-76bcdbc66f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-03-13T16:56:19Z"
      lastUpdateTime: "2024-03-13T16:56:19Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: prometheus-statsd-exporter
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-02-15T03:44:40Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: prometheus-statsd-exporter
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-statsd-exporter
      app.kubernetes.io/version: v0.26.0
      helm.sh/chart: prometheus-statsd-exporter-0.2.0
    name: prometheus-statsd-exporter
    namespace: airflow
    resourceVersion: "13082250"
    uid: e2847bef-392a-40fb-b221-24b1c54bbc6c
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus-statsd-exporter
        app.kubernetes.io/name: prometheus-statsd-exporter
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: prometheus-statsd-exporter
          app.kubernetes.io/name: prometheus-statsd-exporter
      spec:
        containers:
        - args:
          - --web.listen-address=:9102
          - --web.telemetry-path=/metrics
          - --statsd.listen-udp=:9125
          - --statsd.listen-tcp=:9125
          - --statsd.cache-size=1000
          - --statsd.event-queue-size=10000
          - --statsd.event-flush-threshold=1000
          - --statsd.event-flush-interval=200ms
          - --statsd.mapping-config=/etc/prometheus-statsd-exporter/statsd-mapping.conf
          image: prom/statsd-exporter:v0.26.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /metrics
              port: web
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus-statsd-exporter
          ports:
          - containerPort: 9102
            name: web
            protocol: TCP
          - containerPort: 9125
            name: statsd-tcp
            protocol: TCP
          - containerPort: 9125
            name: statsd-udp
            protocol: UDP
          resources: {}
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/prometheus-statsd-exporter
            name: statsd-mapping-config
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: prometheus-statsd-exporter
        serviceAccountName: prometheus-statsd-exporter
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: statsd.mappingConf
              path: statsd-mapping.conf
            name: prometheus-statsd-exporter
          name: statsd-mapping-config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-02-15T03:44:40Z"
      lastUpdateTime: "2024-02-20T10:57:42Z"
      message: ReplicaSet "prometheus-statsd-exporter-56d8b89dfd" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-03-13T16:45:30Z"
      lastUpdateTime: "2024-03-13T16:45:30Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-03-13T16:45:36Z"
    generation: 1
    labels:
      apiserver: "true"
      app.kubernetes.io/name: calico-apiserver
      k8s-app: calico-apiserver
    name: calico-apiserver
    namespace: calico-apiserver
    ownerReferences:
    - apiVersion: operator.tigera.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: APIServer
      name: default
      uid: 35db4f2b-9175-436e-b7e1-3e599376612f
    resourceVersion: "13082596"
    uid: ce5075a2-6544-4452-bcb1-f8b513eb02ac
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        apiserver: "true"
    strategy:
      type: Recreate
    template:
      metadata:
        annotations:
          tigera-operator.hash.operator.tigera.io/calico-apiserver-certs: f486d634d991c86be36e698588a498a9965d8a2f
        creationTimestamp: null
        labels:
          apiserver: "true"
          app.kubernetes.io/name: calico-apiserver
          k8s-app: calico-apiserver
        name: calico-apiserver
        namespace: calico-apiserver
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    k8s-app: calico-apiserver
                namespaces:
                - calico-apiserver
                topologyKey: kubernetes.io/hostname
              weight: 100
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    k8s-app: calico-apiserver
                namespaces:
                - calico-apiserver
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - args:
          - --secure-port=5443
          - --tls-private-key-file=/calico-apiserver-certs/tls.key
          - --tls-cert-file=/calico-apiserver-certs/tls.crt
          env:
          - name: DATASTORE_TYPE
            value: kubernetes
          - name: KUBERNETES_SERVICE_HOST
            value: 10.43.0.1
          - name: KUBERNETES_SERVICE_PORT
            value: "443"
          - name: MULTI_INTERFACE_MODE
            value: none
          image: docker.io/calico/apiserver:master
          imagePullPolicy: IfNotPresent
          name: calico-apiserver
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 5443
              scheme: HTTPS
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            runAsGroup: 0
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /calico-apiserver-certs
            name: calico-apiserver-certs
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-apiserver
        serviceAccountName: calico-apiserver
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
        volumes:
        - name: calico-apiserver-certs
          secret:
            defaultMode: 420
            secretName: calico-apiserver-certs
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2024-03-13T16:45:54Z"
      lastUpdateTime: "2024-03-13T16:45:54Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-03-13T16:45:36Z"
      lastUpdateTime: "2024-03-13T16:45:54Z"
      message: ReplicaSet "calico-apiserver-fd8fdffff" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-03-13T16:12:07Z"
    generation: 1
    labels:
      app.kubernetes.io/name: calico-kube-controllers
      k8s-app: calico-kube-controllers
    name: calico-kube-controllers
    namespace: calico-system
    ownerReferences:
    - apiVersion: operator.tigera.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: Installation
      name: default
      uid: c849cc8b-dd17-411b-ae0e-26a4af04b1f8
    resourceVersion: "13075262"
    uid: e72128af-ec6f-4b3a-8fc9-b0460369c249
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: calico-kube-controllers
    strategy:
      type: Recreate
    template:
      metadata:
        annotations:
          hash.operator.tigera.io/system: fdde45054a8ae4f629960ce37570929502e59449
          tigera-operator.hash.operator.tigera.io/tigera-ca-private: 4f5267a1bedc8badce3a9288fba115ecfdb158ed
        creationTimestamp: null
        labels:
          app.kubernetes.io/name: calico-kube-controllers
          k8s-app: calico-kube-controllers
        name: calico-kube-controllers
        namespace: calico-system
      spec:
        containers:
        - env:
          - name: KUBE_CONTROLLERS_CONFIG_NAME
            value: default
          - name: DATASTORE_TYPE
            value: kubernetes
          - name: ENABLED_CONTROLLERS
            value: node
          - name: FIPS_MODE_ENABLED
            value: "false"
          - name: DISABLE_KUBE_CONTROLLERS_CONFIG_API
            value: "false"
          - name: KUBERNETES_SERVICE_HOST
            value: 10.43.0.1
          - name: KUBERNETES_SERVICE_PORT
            value: "443"
          - name: CA_CRT_PATH
            value: /etc/pki/tls/certs/tigera-ca-bundle.crt
          image: docker.io/calico/kube-controllers:master
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/check-status
              - -l
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 10
          name: calico-kube-controllers
          readinessProbe:
            exec:
              command:
              - /usr/bin/check-status
              - -r
            failureThreshold: 3
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            runAsGroup: 0
            runAsNonRoot: true
            runAsUser: 999
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/pki/tls/certs
            name: tigera-ca-bundle
            readOnly: true
          - mountPath: /etc/pki/tls/cert.pem
            name: tigera-ca-bundle
            readOnly: true
            subPath: ca-bundle.crt
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-kube-controllers
        serviceAccountName: calico-kube-controllers
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: tigera-ca-bundle
          name: tigera-ca-bundle
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-03-13T16:18:40Z"
      lastUpdateTime: "2024-03-13T16:18:40Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-03-13T16:12:07Z"
      lastUpdateTime: "2024-03-13T16:18:40Z"
      message: ReplicaSet "calico-kube-controllers-55bd969cb" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-03-13T16:12:07Z"
    generation: 2
    labels:
      app.kubernetes.io/name: calico-typha
      k8s-app: calico-typha
    name: calico-typha
    namespace: calico-system
    ownerReferences:
    - apiVersion: operator.tigera.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: Installation
      name: default
      uid: c849cc8b-dd17-411b-ae0e-26a4af04b1f8
    resourceVersion: "13073190"
    uid: d8a884f0-b9f4-4c67-8e8c-700b30f06e87
  spec:
    progressDeadlineSeconds: 600
    replicas: 3
    revisionHistoryLimit: 2
    selector:
      matchLabels:
        k8s-app: calico-typha
    strategy:
      rollingUpdate:
        maxSurge: 100%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        annotations:
          hash.operator.tigera.io/system: fdde45054a8ae4f629960ce37570929502e59449
          tigera-operator.hash.operator.tigera.io/tigera-ca-private: 4f5267a1bedc8badce3a9288fba115ecfdb158ed
          tigera-operator.hash.operator.tigera.io/typha-certs: c9d75be38dacbdb51cf4b46c6f5d1bc93a95e3db
        creationTimestamp: null
        labels:
          app.kubernetes.io/name: calico-typha
          k8s-app: calico-typha
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: k8s-app
                    operator: In
                    values:
                    - calico-typha
                topologyKey: topology.kubernetes.io/zone
              weight: 1
        containers:
        - env:
          - name: TYPHA_LOGSEVERITYSCREEN
            value: info
          - name: TYPHA_LOGFILEPATH
            value: none
          - name: TYPHA_LOGSEVERITYSYS
            value: none
          - name: TYPHA_CONNECTIONREBALANCINGMODE
            value: kubernetes
          - name: TYPHA_DATASTORETYPE
            value: kubernetes
          - name: TYPHA_HEALTHENABLED
            value: "true"
          - name: TYPHA_HEALTHPORT
            value: "9098"
          - name: TYPHA_K8SNAMESPACE
            value: calico-system
          - name: TYPHA_CAFILE
            value: /etc/pki/tls/certs/tigera-ca-bundle.crt
          - name: TYPHA_SERVERCERTFILE
            value: /typha-certs/tls.crt
          - name: TYPHA_SERVERKEYFILE
            value: /typha-certs/tls.key
          - name: TYPHA_FIPSMODEENABLED
            value: "false"
          - name: TYPHA_SHUTDOWNTIMEOUTSECS
            value: "300"
          - name: TYPHA_CLIENTCN
            value: typha-client
          - name: KUBERNETES_SERVICE_HOST
            value: 10.43.0.1
          - name: KUBERNETES_SERVICE_PORT
            value: "443"
          image: docker.io/calico/typha:master
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              host: localhost
              path: /liveness
              port: 9098
              scheme: HTTP
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 10
          name: calico-typha
          ports:
          - containerPort: 5473
            hostPort: 5473
            name: calico-typha
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              host: localhost
              path: /readiness
              port: 9098
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            runAsGroup: 10001
            runAsNonRoot: true
            runAsUser: 10001
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/pki/tls/certs
            name: tigera-ca-bundle
            readOnly: true
          - mountPath: /etc/pki/tls/cert.pem
            name: tigera-ca-bundle
            readOnly: true
            subPath: ca-bundle.crt
          - mountPath: /typha-certs
            name: typha-certs
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-typha
        serviceAccountName: calico-typha
        terminationGracePeriodSeconds: 300
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          operator: Exists
        - effect: NoExecute
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: tigera-ca-bundle
          name: tigera-ca-bundle
        - name: typha-certs
          secret:
            defaultMode: 420
            secretName: typha-certs
  status:
    availableReplicas: 3
    conditions:
    - lastTransitionTime: "2024-03-13T16:12:57Z"
      lastUpdateTime: "2024-03-13T16:12:57Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-03-13T16:12:07Z"
      lastUpdateTime: "2024-03-13T16:12:57Z"
      message: ReplicaSet "calico-typha-5d4bd748b9" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 2
    readyReplicas: 3
    replicas: 3
    updatedReplicas: 3
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: ingress-nginx
      meta.helm.sh/release-namespace: ingress-nginx
    creationTimestamp: "2024-02-21T05:23:25Z"
    generation: 1
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.9.6
      helm.sh/chart: ingress-nginx-4.9.1
    name: ingress-nginx-controller
    namespace: ingress-nginx
    resourceVersion: "13085775"
    uid: ad4a9ef1-81d9-4bd6-b027-770e7124b256
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: ingress-nginx
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: ingress-nginx
          app.kubernetes.io/part-of: ingress-nginx
          app.kubernetes.io/version: 1.9.6
          helm.sh/chart: ingress-nginx-4.9.1
      spec:
        containers:
        - args:
          - /nginx-ingress-controller
          - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
          - --election-id=ingress-nginx-leader
          - --controller-class=k8s.io/ingress-nginx
          - --ingress-class=nginx
          - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
          - --validating-webhook=:8443
          - --validating-webhook-certificate=/usr/local/certificates/cert
          - --validating-webhook-key=/usr/local/certificates/key
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: LD_PRELOAD
            value: /usr/local/lib/libmimalloc.so
          image: registry.k8s.io/ingress-nginx/controller:v1.9.6@sha256:1405cc613bd95b2c6edd8b2a152510ae91c7e62aea4698500d23b2145960ab9c
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /wait-shutdown
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: controller
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          - containerPort: 443
            name: https
            protocol: TCP
          - containerPort: 8443
            name: webhook
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 90Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 101
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /usr/local/certificates/
            name: webhook-cert
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: ingress-nginx
        serviceAccountName: ingress-nginx
        terminationGracePeriodSeconds: 300
        volumes:
        - name: webhook-cert
          secret:
            defaultMode: 420
            secretName: ingress-nginx-admission
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-02-21T05:23:25Z"
      lastUpdateTime: "2024-02-21T05:23:55Z"
      message: ReplicaSet "ingress-nginx-controller-699bbd7596" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-03-13T16:50:39Z"
      lastUpdateTime: "2024-03-13T16:50:39Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"k8s-app":"kube-dns","kubernetes.io/name":"CoreDNS"},"name":"coredns","namespace":"kube-system"},"spec":{"replicas":1,"selector":{"matchLabels":{"k8s-app":"kube-dns"}},"strategy":{"rollingUpdate":{"maxSurge":0,"maxUnavailable":1},"type":"RollingUpdate"},"template":{"metadata":{"annotations":{"seccomp.security.alpha.kubernetes.io/pod":"docker/default"},"labels":{"k8s-app":"kube-dns"}},"spec":{"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchExpressions":[{"key":"node-role.kubernetes.io/worker","operator":"Exists"}]}]}},"podAntiAffinity":{"preferredDuringSchedulingIgnoredDuringExecution":[{"podAffinityTerm":{"labelSelector":{"matchExpressions":[{"key":"k8s-app","operator":"In","values":["kube-dns"]}]},"topologyKey":"kubernetes.io/hostname"},"weight":100}]}},"containers":[{"args":["-conf","/etc/coredns/Corefile"],"image":"rancher/mirrored-coredns-coredns:1.9.4","imagePullPolicy":"IfNotPresent","livenessProbe":{"failureThreshold":5,"httpGet":{"path":"/health","port":8080,"scheme":"HTTP"},"initialDelaySeconds":60,"successThreshold":1,"timeoutSeconds":5},"name":"coredns","ports":[{"containerPort":53,"name":"dns","protocol":"UDP"},{"containerPort":53,"name":"dns-tcp","protocol":"TCP"},{"containerPort":9153,"name":"metrics","protocol":"TCP"}],"readinessProbe":{"httpGet":{"path":"/ready","port":8181,"scheme":"HTTP"}},"resources":{"limits":{"memory":"170Mi"},"requests":{"cpu":"100m","memory":"70Mi"}},"securityContext":{"allowPrivilegeEscalation":false,"capabilities":{"add":["NET_BIND_SERVICE"],"drop":["all"]},"readOnlyRootFilesystem":true},"volumeMounts":[{"mountPath":"/etc/coredns","name":"config-volume","readOnly":true}]}],"dnsPolicy":"Default","nodeSelector":{"beta.kubernetes.io/os":"linux"},"priorityClassName":"system-cluster-critical","serviceAccountName":"coredns","tolerations":[{"key":"CriticalAddonsOnly","operator":"Exists"},{"effect":"NoExecute","operator":"Exists"},{"effect":"NoSchedule","operator":"Exists"}],"volumes":[{"configMap":{"items":[{"key":"Corefile","path":"Corefile"}],"name":"coredns"},"name":"config-volume"}]}}}}
    creationTimestamp: "2024-01-26T07:18:49Z"
    generation: 3
    labels:
      k8s-app: kube-dns
      kubernetes.io/name: CoreDNS
    name: coredns
    namespace: kube-system
    resourceVersion: "13078559"
    uid: 3e7ffa92-455d-46b1-bfd1-4137b69ee321
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-dns
    strategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2024-03-13T10:26:22+01:00"
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: node-role.kubernetes.io/worker
                  operator: Exists
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: k8s-app
                    operator: In
                    values:
                    - kube-dns
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: rancher/mirrored-coredns-coredns:1.9.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          beta.kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoExecute
          operator: Exists
        - effect: NoSchedule
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            name: coredns
          name: config-volume
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2024-01-29T11:34:12Z"
      lastUpdateTime: "2024-03-13T09:26:41Z"
      message: ReplicaSet "coredns-b9597578" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-03-13T16:31:26Z"
      lastUpdateTime: "2024-03-13T16:31:26Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 3
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"k8s-app":"coredns-autoscaler"},"name":"coredns-autoscaler","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"coredns-autoscaler"}},"template":{"metadata":{"labels":{"k8s-app":"coredns-autoscaler"}},"spec":{"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchExpressions":[{"key":"node-role.kubernetes.io/worker","operator":"Exists"}]}]}}},"containers":[{"command":["/cluster-proportional-autoscaler","--namespace=kube-system","--configmap=coredns-autoscaler","--target=Deployment/coredns","--default-params={\"linear\":{\"coresPerReplica\":128,\"nodesPerReplica\":4,\"min\":1,\"preventSinglePointFailure\":true}}","--nodelabels=node-role.kubernetes.io/worker=true,beta.kubernetes.io/os=linux","--logtostderr=true","--v=2"],"image":"rancher/mirrored-cluster-proportional-autoscaler:1.8.6","livenessProbe":{"failureThreshold":5,"httpGet":{"path":"/healthz","port":8080,"scheme":"HTTP"},"initialDelaySeconds":60,"periodSeconds":10,"successThreshold":1,"timeoutSeconds":5},"name":"autoscaler","readinessProbe":{"failureThreshold":3,"httpGet":{"path":"/healthz","port":8080,"scheme":"HTTP"},"periodSeconds":10,"successThreshold":1,"timeoutSeconds":1},"resources":{"requests":{"cpu":"20m","memory":"10Mi"}}}],"nodeSelector":{"beta.kubernetes.io/os":"linux"},"serviceAccountName":"coredns-autoscaler","tolerations":[{"effect":"NoExecute","operator":"Exists"},{"effect":"NoSchedule","operator":"Exists"}]}}}}
    creationTimestamp: "2024-01-26T07:18:49Z"
    generation: 1
    labels:
      k8s-app: coredns-autoscaler
    name: coredns-autoscaler
    namespace: kube-system
    resourceVersion: "13085735"
    uid: 774d0f20-d8cd-42f8-b9b6-164c79e30167
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: coredns-autoscaler
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: coredns-autoscaler
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: node-role.kubernetes.io/worker
                  operator: Exists
        containers:
        - command:
          - /cluster-proportional-autoscaler
          - --namespace=kube-system
          - --configmap=coredns-autoscaler
          - --target=Deployment/coredns
          - --default-params={"linear":{"coresPerReplica":128,"nodesPerReplica":4,"min":1,"preventSinglePointFailure":true}}
          - --nodelabels=node-role.kubernetes.io/worker=true,beta.kubernetes.io/os=linux
          - --logtostderr=true
          - --v=2
          image: rancher/mirrored-cluster-proportional-autoscaler:1.8.6
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: autoscaler
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 20m
              memory: 10Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns-autoscaler
        serviceAccountName: coredns-autoscaler
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          operator: Exists
        - effect: NoSchedule
          operator: Exists
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-01-29T11:34:33Z"
      lastUpdateTime: "2024-01-29T11:34:33Z"
      message: ReplicaSet "coredns-autoscaler-5567d8c485" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-03-13T16:50:35Z"
      lastUpdateTime: "2024-03-13T16:50:35Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"k8s-app":"metrics-server"},"name":"metrics-server","namespace":"kube-system"},"spec":{"replicas":1,"selector":{"matchLabels":{"k8s-app":"metrics-server"}},"strategy":{"rollingUpdate":{"maxSurge":"25%","maxUnavailable":"25%"},"type":"RollingUpdate"},"template":{"metadata":{"labels":{"k8s-app":"metrics-server"},"name":"metrics-server"},"spec":{"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchExpressions":[{"key":"beta.kubernetes.io/os","operator":"NotIn","values":["windows"]},{"key":"node-role.kubernetes.io/worker","operator":"Exists"}]}]}}},"containers":[{"args":["--cert-dir=/tmp","--secure-port=4443","--kubelet-insecure-tls","--kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname","--metric-resolution=15s"],"image":"rancher/mirrored-metrics-server:v0.6.3","imagePullPolicy":"IfNotPresent","livenessProbe":{"failureThreshold":3,"httpGet":{"path":"/livez","port":"https","scheme":"HTTPS"},"periodSeconds":10},"name":"metrics-server","ports":[{"containerPort":4443,"name":"https","protocol":"TCP"}],"readinessProbe":{"failureThreshold":3,"httpGet":{"path":"/readyz","port":"https","scheme":"HTTPS"},"initialDelaySeconds":20,"periodSeconds":10},"resources":{"requests":{"cpu":"100m","memory":"200Mi"}},"securityContext":{"allowPrivilegeEscalation":false,"readOnlyRootFilesystem":true,"runAsNonRoot":true,"runAsUser":1000},"volumeMounts":[{"mountPath":"/tmp","name":"tmp-dir"}]}],"priorityClassName":"system-cluster-critical","serviceAccountName":"metrics-server","tolerations":[{"effect":"NoExecute","operator":"Exists"},{"effect":"NoSchedule","operator":"Exists"}],"volumes":[{"emptyDir":{},"name":"tmp-dir"}]}}}}
    creationTimestamp: "2024-01-26T07:18:59Z"
    generation: 1
    labels:
      k8s-app: metrics-server
    name: metrics-server
    namespace: kube-system
    resourceVersion: "13078456"
    uid: 12f5dfc2-781a-49be-aab8-16489455b46f
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: metrics-server
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
        name: metrics-server
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: beta.kubernetes.io/os
                  operator: NotIn
                  values:
                  - windows
                - key: node-role.kubernetes.io/worker
                  operator: Exists
        containers:
        - args:
          - --cert-dir=/tmp
          - --secure-port=4443
          - --kubelet-insecure-tls
          - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
          - --metric-resolution=15s
          image: rancher/mirrored-metrics-server:v0.6.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: metrics-server
          ports:
          - containerPort: 4443
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: https
              scheme: HTTPS
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-dir
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          operator: Exists
        - effect: NoSchedule
          operator: Exists
        volumes:
        - emptyDir: {}
          name: tmp-dir
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-01-29T11:34:41Z"
      lastUpdateTime: "2024-01-29T11:34:41Z"
      message: ReplicaSet "metrics-server-7886b5f87c" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-03-13T16:31:56Z"
      lastUpdateTime: "2024-03-13T16:31:56Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "13"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-02-02T10:31:12Z"
    generation: 13
    labels:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 10.2.2
      helm.sh/chart: grafana-7.0.19
    name: kube-prometheus-stack-grafana
    namespace: monitoring
    resourceVersion: "13086988"
    uid: 60a86144-080d-4d57-9783-9241f5375a03
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: grafana
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: c8e876853580e8b0d807a852e9813e06a752130be7ce716cc267fb1881ca7e39
          checksum/sc-dashboard-provider-config: 593c0a8778b83f11fe80ccb21dfb20bc46705e2be3178df1dc4c89d164c8cd9c
          checksum/secret: 52393145826148a5c4d0489c34fdfe1036fe8275afda36f83c7eee420dd9eb7d
          kubectl.kubernetes.io/default-container: grafana
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/name: grafana
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:10.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        initContainers:
        - command:
          - chown
          - -R
          - 472:472
          - /var/lib/grafana
          image: docker.io/library/busybox:1.31.1
          imagePullPolicy: IfNotPresent
          name: init-chown-data
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/grafana
            name: storage
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: kube-prometheus-stack-grafana
        serviceAccountName: kube-prometheus-stack-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana
          name: config
        - name: storage
          persistentVolumeClaim:
            claimName: kube-prometheus-stack-grafana
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-02-02T10:31:12Z"
      lastUpdateTime: "2024-03-05T11:51:45Z"
      message: ReplicaSet "kube-prometheus-stack-grafana-7c77d9b866" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-03-13T16:52:56Z"
      lastUpdateTime: "2024-03-13T16:52:56Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 13
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-02-02T10:31:12Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.10.1
      helm.sh/chart: kube-state-metrics-5.15.2
      release: kube-prometheus-stack
    name: kube-prometheus-stack-kube-state-metrics
    namespace: monitoring
    resourceVersion: "13075254"
    uid: cde9c5d0-5146-4685-b32e-83e8819f65a9
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: kube-state-metrics
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.10.1
          helm.sh/chart: kube-state-metrics-5.15.2
          release: kube-prometheus-stack
      spec:
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.10.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: kube-prometheus-stack-kube-state-metrics
        serviceAccountName: kube-prometheus-stack-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-02-02T10:31:12Z"
      lastUpdateTime: "2024-02-02T10:31:32Z"
      message: ReplicaSet "kube-prometheus-stack-kube-state-metrics-d68548445" has
        successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-03-13T16:18:40Z"
      lastUpdateTime: "2024-03-13T16:18:40Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-02-02T10:31:12Z"
    generation: 1
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 55.5.1
      chart: kube-prometheus-stack-55.5.1
      heritage: Helm
      release: kube-prometheus-stack
    name: kube-prometheus-stack-operator
    namespace: monitoring
    resourceVersion: "13075179"
    uid: d3f6343f-4180-49fb-8026-9a6b9956f0e9
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: kube-prometheus-stack-operator
        release: kube-prometheus-stack
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: kube-prometheus-stack-operator
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/part-of: kube-prometheus-stack
          app.kubernetes.io/version: 55.5.1
          chart: kube-prometheus-stack-55.5.1
          heritage: Helm
          release: kube-prometheus-stack
      spec:
        containers:
        - args:
          - --kubelet-service=kube-system/kube-prometheus-stack-kubelet
          - --localhost=127.0.0.1
          - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
          - --config-reloader-cpu-request=0
          - --config-reloader-cpu-limit=0
          - --config-reloader-memory-request=0
          - --config-reloader-memory-limit=0
          - --thanos-default-base-image=quay.io/thanos/thanos:v0.32.5
          - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
          - --web.enable-tls=true
          - --web.cert-file=/cert/cert
          - --web.key-file=/cert/key
          - --web.listen-address=:10250
          - --web.tls-min-version=VersionTLS13
          env:
          - name: GOGC
            value: "30"
          image: quay.io/prometheus-operator/prometheus-operator:v0.70.0
          imagePullPolicy: IfNotPresent
          name: kube-prometheus-stack
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /cert
            name: tls-secret
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: kube-prometheus-stack-operator
        serviceAccountName: kube-prometheus-stack-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - name: tls-secret
          secret:
            defaultMode: 420
            secretName: kube-prometheus-stack-admission
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-02-02T10:31:12Z"
      lastUpdateTime: "2024-02-02T10:31:24Z"
      message: ReplicaSet "kube-prometheus-stack-operator-79b4dd5d7c" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-03-13T16:18:33Z"
      lastUpdateTime: "2024-03-13T16:18:33Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: UEsDBBQACAAIAAAAAAAAAAAAAAAAAAAAAAAIAAAAb3JpZ2luYWzcV11v2zYU/S9EHzpAVGzno6kAPwS1NxhbHCMZ9lIEBU1e2ZwpkiUpNVqg/z5cSo7lxGmTbRjWPRiWqMPLy3PO5cc9YVb+Bs5Lo0lGmLX+qBqShGykFiQjE7DK1AXoQBJSQGCCBUaye6JZASQj3EvKwa5zb1W5kppaZyqJ0cCRJKK8ZRyhzphNxJKEmC8a3DXk4EBz8CT7+GwiS2X45grxE1AQ4uecKQ8J4UYHZ5QCR7LgSjicdZfpw/DUWHAsGMyvlAh/x455PshzesJGnJ6Ik/f0XAwGNF/mZ2I5eD9YDs9Jc9skxFvgOHsHVknOPMlGCfGggGO87J4ULPD1L2wJyuMrs/ZbJDUYNjgWYFVjl1BbTPcauAMWgDQJCVBYhc84QE8D9ZpxWrqY1P4QsoDgJPdtNt0kWZ5LLUPMShsBF7v3JiHWiAsdZK+ROPhcSgdiUjqpVzd8DaJUUq9mK20emqd3wMtWxo/dFG4eEzi9sw48Jt5aYwN16wm0zla9jMw0SUjFVBkd9PXp30b9grFGmVX9cwy4KZfgNATwqTRHa+ND9Epziyx0bIHr3OlWcRBKq/GAJIRSHI4JgYmO37y9mEyupzc3P8RPCpgAR+O0pNFjNGf8EGQBpgzjUXE88Ieg9KFixv16eYpTwDxQUToWRxgevzscz4GGL1QAE0pqGA8Hz+KCq6kFJ40Yj848uU0I6CrOvaugbopbzklGjriX+OsznXrDN6S5TYgs2CpWHqykD65ON+eRaC9X1Afj2ApiZxYC42twWXWSnqQj0vVclEotjJIcpZrlcxMWDvxeTfd7k4Q48KZ0cT25J0oWMsQnbkuSkdFgUMQ1rDAOI45Ozy4lVhe6FnwfOnwEHY7OEdokpDKqLODSlDq0tijwccHCumNjlxvSAIEK6eLS0XfQV5yzNdc3jbLz1H/WQQhaMy0U0JY3KnXpgYJzxo3jGv4il5Va3mVHR/+M2Rx4+Qd6bZi+f7XXus7/d6u1SkodwFVMUR+YC+PTwaD4np0Id8Ex2m6qdLuLtrn/6ybsdc6q4/Ts1Ubc39i/FzPiIUKK8Zu386vJ9NNs0voQDzzjdt9uhdLCGqnD+M3bDzezT9P5ZHE1m/+6b9rd0c+Dq8DtTCicrMBhMjvXpW34lHuZtu+miGArReRrTIfxPTeOAyI2eDBQXEnQ4TmTLK4mn2aLrUd+dKZAKnMJSlxD/vDcseQDC6VPrRGzBWkaZKaL07Hx4kAWeIpUzuNhpR8JM5pfXE5vFhcfpi+Nt62FdHdY3wvaF+GvFsTnktVYCEht7ND+o/mHg3T4Svf3D3nfsv/pY0//9Iz5R6f7wNPh6O+ZP3kE9LXfAfGwSbHlCUzJ5VFhRKmgB1dySXeNDpi40qpubz1PIgioHg2ELU9gEHjUIe4R3Ohcro56PO9/edo9FDZqv4H+vPCNhsJu6/82IdZJ42SoPyjm/bzjqfYBCspV6QM4yp0MkjNF8DLlKsnhgnMcbN6/uz0o3/ca9YxsJWrVgcKGeiLb2wQIWRYkI5etrM1zSiFN7czuie0J1hxW7BC8L1xzULnnerbyHFSsZf+SWcTLAEX/QhQZ2VIYgfR3b7AkushtYxobUYonV+E9gb8m/ctIfax+e7fFdQ/vjM2fAQAA//9QSwcIEPx1W9QEAABtEAAAUEsBAhQAFAAIAAgAAAAAABD8dVvUBAAAbRAAAAgAAAAAAAAAAAAAAAAAAAAAAG9yaWdpbmFsUEsFBgAAAAABAAEANgAAAAoFAAAAAA==
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-02-12T11:44:41Z"
    generation: 5
    name: csi-cephfsplugin-provisioner
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: false
      controller: true
      kind: Deployment
      name: rook-ceph-operator
      uid: 7a3cf0ff-4a2c-4d49-8d00-fbf6db090b18
    resourceVersion: "13085222"
    uid: a5d18ec9-f5e7-4e72-a83a-bb496f8d18f1
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: csi-cephfsplugin-provisioner
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: csi-cephfsplugin-provisioner
          contains: csi-cephfsplugin-metrics
      spec:
        affinity:
          nodeAffinity: {}
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - csi-cephfsplugin-provisioner
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - --v=0
          - --csi-address=$(ADDRESS)
          - --leader-election=true
          - --timeout=2m30s
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=137s
          - --leader-election-renew-deadline=107s
          - --leader-election-retry-period=26s
          env:
          - name: ADDRESS
            value: /csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-attacher:v4.4.2
          imagePullPolicy: IfNotPresent
          name: csi-attacher
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=137s
          - --leader-election-renew-deadline=107s
          - --leader-election-retry-period=26s
          - --handle-volume-inuse-error=false
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-resizer:v1.9.2
          imagePullPolicy: IfNotPresent
          name: csi-resizer
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --retry-interval-start=500ms
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=137s
          - --leader-election-renew-deadline=107s
          - --leader-election-retry-period=26s
          - --extra-create-metadata=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-provisioner:v3.6.2
          imagePullPolicy: IfNotPresent
          name: csi-provisioner
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --nodeid=$(NODE_ID)
          - --type=cephfs
          - --endpoint=$(CSI_ENDPOINT)
          - --v=0
          - --controllerserver=true
          - --drivername=rook-ceph.cephfs.csi.ceph.com
          - --pidlimit=-1
          - --forcecephkernelclient=true
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CSI_ENDPOINT
            value: unix:///csi/csi-provisioner.sock
          image: quay.io/cephcsi/cephcsi:v3.10.1
          imagePullPolicy: IfNotPresent
          name: csi-cephfsplugin
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
          - mountPath: /sys
            name: host-sys
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /dev
            name: host-dev
          - mountPath: /etc/ceph-csi-config/
            name: ceph-csi-config
          - mountPath: /tmp/csi/keys
            name: keys-tmp-dir
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-csi-cephfs-provisioner-sa
        serviceAccountName: rook-csi-cephfs-provisioner-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: socket-dir
        - hostPath:
            path: /sys
            type: ""
          name: host-sys
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - hostPath:
            path: /dev
            type: ""
          name: host-dev
        - configMap:
            defaultMode: 420
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
          name: ceph-csi-config
        - emptyDir:
            medium: Memory
          name: keys-tmp-dir
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2024-02-12T11:44:41Z"
      lastUpdateTime: "2024-02-12T11:45:00Z"
      message: ReplicaSet "csi-cephfsplugin-provisioner-774788f85" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-03-13T16:49:58Z"
      lastUpdateTime: "2024-03-13T16:49:58Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 5
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-01-31T12:41:09Z"
    generation: 7
    name: csi-rbdplugin-provisioner
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: false
      controller: true
      kind: Deployment
      name: rook-ceph-operator
      uid: 7a3cf0ff-4a2c-4d49-8d00-fbf6db090b18
    resourceVersion: "13085220"
    uid: f8fd6120-dd82-4955-9d9d-af715e126301
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: csi-rbdplugin-provisioner
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: csi-rbdplugin-provisioner
          contains: csi-rbdplugin-metrics
      spec:
        affinity:
          nodeAffinity: {}
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - csi-rbdplugin-provisioner
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --retry-interval-start=500ms
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=137s
          - --leader-election-renew-deadline=107s
          - --leader-election-retry-period=26s
          - --default-fstype=ext4
          - --extra-create-metadata=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-provisioner:v3.6.2
          imagePullPolicy: IfNotPresent
          name: csi-provisioner
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=137s
          - --leader-election-renew-deadline=107s
          - --leader-election-retry-period=26s
          - --handle-volume-inuse-error=false
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-resizer:v1.9.2
          imagePullPolicy: IfNotPresent
          name: csi-resizer
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --v=0
          - --timeout=2m30s
          - --csi-address=$(ADDRESS)
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=137s
          - --leader-election-renew-deadline=107s
          - --leader-election-retry-period=26s
          - --default-fstype=ext4
          env:
          - name: ADDRESS
            value: /csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-attacher:v4.4.2
          imagePullPolicy: IfNotPresent
          name: csi-attacher
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=137s
          - --leader-election-renew-deadline=107s
          - --leader-election-retry-period=26s
          - --extra-create-metadata=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-snapshotter:v6.3.2
          imagePullPolicy: IfNotPresent
          name: csi-snapshotter
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --nodeid=$(NODE_ID)
          - --endpoint=$(CSI_ENDPOINT)
          - --v=0
          - --type=rbd
          - --controllerserver=true
          - --drivername=rook-ceph.rbd.csi.ceph.com
          - --pidlimit=-1
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CSI_ENDPOINT
            value: unix:///csi/csi-provisioner.sock
          image: quay.io/cephcsi/cephcsi:v3.10.1
          imagePullPolicy: IfNotPresent
          name: csi-rbdplugin
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
          - mountPath: /dev
            name: host-dev
          - mountPath: /sys
            name: host-sys
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /etc/ceph-csi-config/
            name: ceph-csi-configs
          - mountPath: /tmp/csi/keys
            name: keys-tmp-dir
          - mountPath: /run/secrets/tokens
            name: oidc-token
            readOnly: true
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-csi-rbd-provisioner-sa
        serviceAccountName: rook-csi-rbd-provisioner-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /dev
            type: ""
          name: host-dev
        - hostPath:
            path: /sys
            type: ""
          name: host-sys
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - emptyDir:
            medium: Memory
          name: socket-dir
        - name: ceph-csi-configs
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: csi-cluster-config-json
                  path: config.json
                name: rook-ceph-csi-config
            - configMap:
                items:
                - key: csi-mapping-config-json
                  path: cluster-mapping.json
                name: rook-ceph-csi-mapping-config
        - emptyDir:
            medium: Memory
          name: keys-tmp-dir
        - name: oidc-token
          projected:
            defaultMode: 420
            sources:
            - serviceAccountToken:
                audience: ceph-csi-kms
                expirationSeconds: 3600
                path: oidc-token
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2024-01-31T12:41:09Z"
      lastUpdateTime: "2024-01-31T12:41:12Z"
      message: ReplicaSet "csi-rbdplugin-provisioner-56f7bf6d4d" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-03-13T16:49:59Z"
      lastUpdateTime: "2024-03-13T16:49:59Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 7
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2024-03-13T16:45:38Z"
    generation: 2
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 17.2.6-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: worker-1
      node_name: worker-1
      rook-version: v1.13.1
      rook_cluster: rook-ceph
    name: rook-ceph-crashcollector-worker-1
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 02274ff6-97d3-4d09-b941-31edb9bdad99
    resourceVersion: "13085402"
    uid: 3b7b01c7-3c71-44a7-b57c-b355efbfe57e
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-crashcollector
        kubernetes.io/hostname: worker-1
        node_name: worker-1
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-crashcollector
          ceph-version: 17.2.6-0
          ceph_daemon_id: crash
          crashcollector: crash
          kubernetes.io/hostname: worker-1
          node_name: worker-1
          rook_cluster: rook-ceph
      spec:
        containers:
        - command:
          - ceph-crash
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: ceph-crash
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
            runAsGroup: 167
            runAsUser: 167
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /etc/ceph/crash-collector-keyring-store/
            name: rook-ceph-crash-collector-keyring
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - /var/lib/ceph/crash/posted
          command:
          - mkdir
          - -p
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: make-container-crash-dir
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        nodeSelector:
          kubernetes.io/hostname: worker-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - name: rook-ceph-crash-collector-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-crash-collector-keyring
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-03-13T16:50:04Z"
      lastUpdateTime: "2024-03-13T16:50:04Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-03-13T16:45:38Z"
      lastUpdateTime: "2024-03-13T16:50:04Z"
      message: ReplicaSet "rook-ceph-crashcollector-worker-1-78679dd88c" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "7"
    creationTimestamp: "2024-02-01T09:39:41Z"
    generation: 7
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 17.2.6-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: worker-2
      node_name: worker-2
      rook-version: v1.13.1
      rook_cluster: rook-ceph
    name: rook-ceph-crashcollector-worker-2
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 02274ff6-97d3-4d09-b941-31edb9bdad99
    resourceVersion: "13085572"
    uid: 34a796cf-b7dd-4fd4-935c-35ca648f60a2
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-crashcollector
        kubernetes.io/hostname: worker-2
        node_name: worker-2
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-crashcollector
          ceph-version: 17.2.6-0
          ceph_daemon_id: crash
          crashcollector: crash
          kubernetes.io/hostname: worker-2
          node_name: worker-2
          rook_cluster: rook-ceph
      spec:
        containers:
        - command:
          - ceph-crash
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: ceph-crash
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
            runAsGroup: 167
            runAsUser: 167
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /etc/ceph/crash-collector-keyring-store/
            name: rook-ceph-crash-collector-keyring
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - /var/lib/ceph/crash/posted
          command:
          - mkdir
          - -p
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: make-container-crash-dir
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        nodeSelector:
          kubernetes.io/hostname: worker-2
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 300
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - name: rook-ceph-crash-collector-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-crash-collector-keyring
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-03-13T16:49:58Z"
      lastUpdateTime: "2024-03-13T16:49:58Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-02-01T09:39:41Z"
      lastUpdateTime: "2024-03-13T16:50:15Z"
      message: ReplicaSet "rook-ceph-crashcollector-worker-2-686d8549dd" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 7
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "7"
    creationTimestamp: "2024-01-31T12:42:17Z"
    generation: 7
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 17.2.6-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: worker-3
      node_name: worker-3
      rook-version: v1.13.1
      rook_cluster: rook-ceph
    name: rook-ceph-crashcollector-worker-3
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 02274ff6-97d3-4d09-b941-31edb9bdad99
    resourceVersion: "13085414"
    uid: b03bf9eb-96b7-47fe-8e6b-bfa0a6eae32a
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-crashcollector
        kubernetes.io/hostname: worker-3
        node_name: worker-3
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-crashcollector
          ceph-version: 17.2.6-0
          ceph_daemon_id: crash
          crashcollector: crash
          kubernetes.io/hostname: worker-3
          node_name: worker-3
          rook_cluster: rook-ceph
      spec:
        containers:
        - command:
          - ceph-crash
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: ceph-crash
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
            runAsGroup: 167
            runAsUser: 167
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /etc/ceph/crash-collector-keyring-store/
            name: rook-ceph-crash-collector-keyring
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - /var/lib/ceph/crash/posted
          command:
          - mkdir
          - -p
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: make-container-crash-dir
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        nodeSelector:
          kubernetes.io/hostname: worker-3
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - name: rook-ceph-crash-collector-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-crash-collector-keyring
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-03-13T16:50:04Z"
      lastUpdateTime: "2024-03-13T16:50:04Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-03-13T10:40:56Z"
      lastUpdateTime: "2024-03-13T16:50:04Z"
      message: ReplicaSet "rook-ceph-crashcollector-worker-3-5845867477" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 7
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "6"
    creationTimestamp: "2024-02-28T07:51:18Z"
    generation: 6
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 17.2.6-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: worker-4
      node_name: worker-4
      rook-version: v1.13.1
      rook_cluster: rook-ceph
    name: rook-ceph-crashcollector-worker-4
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 02274ff6-97d3-4d09-b941-31edb9bdad99
    resourceVersion: "13085370"
    uid: 5e0a49dd-6c8f-4fa9-95cc-c2825c7a8bf1
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-crashcollector
        kubernetes.io/hostname: worker-4
        node_name: worker-4
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-crashcollector
          ceph-version: 17.2.6-0
          ceph_daemon_id: crash
          crashcollector: crash
          kubernetes.io/hostname: worker-4
          node_name: worker-4
          rook_cluster: rook-ceph
      spec:
        containers:
        - command:
          - ceph-crash
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: ceph-crash
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
            runAsGroup: 167
            runAsUser: 167
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /etc/ceph/crash-collector-keyring-store/
            name: rook-ceph-crash-collector-keyring
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - /var/lib/ceph/crash/posted
          command:
          - mkdir
          - -p
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: make-container-crash-dir
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        nodeSelector:
          kubernetes.io/hostname: worker-4
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - name: rook-ceph-crash-collector-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-crash-collector-keyring
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-03-13T16:49:58Z"
      lastUpdateTime: "2024-03-13T16:49:58Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-03-13T10:39:04Z"
      lastUpdateTime: "2024-03-13T16:50:03Z"
      message: ReplicaSet "rook-ceph-crashcollector-worker-4-66d7cd88d8" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 6
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-01-31T12:45:42Z"
    generation: 1
    labels:
      app: rook-ceph-mds
      app.kubernetes.io/component: cephfilesystems.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: ceph-filesystem-a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mds
      app.kubernetes.io/part-of: ceph-filesystem
      ceph-version: 17.2.6-0
      ceph_daemon_id: ceph-filesystem-a
      ceph_daemon_type: mds
      mds: ceph-filesystem-a
      rook-version: v1.13.1
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      rook_file_system: ceph-filesystem
    name: rook-ceph-mds-ceph-filesystem-a
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephFilesystem
      name: ceph-filesystem
      uid: 9b1420fc-8328-49c8-b3b9-05f41802f214
    resourceVersion: "13085143"
    uid: 0959fe1f-d474-409f-aebf-bbefbb7cdc60
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-mds
        ceph_daemon_id: ceph-filesystem-a
        mds: ceph-filesystem-a
        rook_cluster: rook-ceph
        rook_file_system: ceph-filesystem
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mds
          app.kubernetes.io/component: cephfilesystems.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: ceph-filesystem-a
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mds
          app.kubernetes.io/part-of: ceph-filesystem
          ceph_daemon_id: ceph-filesystem-a
          ceph_daemon_type: mds
          mds: ceph-filesystem-a
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          rook_file_system: ceph-filesystem
        name: rook-ceph-mds-ceph-filesystem-a
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=ceph-filesystem-a
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --public-addr=$(ROOK_POD_IP)
          command:
          - ceph-mds
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - timeout
              - "20"
              - sh
              - -c
              - |
                #!/usr/bin/env bash
                # do not use 'set -e -u' etc. because it is important to only fail this probe when failure is certain
                # spurious failures risk destabilizing ceph or the filesystem

                MDS_ID="ceph-filesystem-a"
                FILESYSTEM_NAME="ceph-filesystem"
                KEYRING="/etc/ceph/keyring-store/keyring"

                outp="$(ceph fs dump --mon-host="$ROOK_CEPH_MON_HOST" --mon-initial-members="$ROOK_CEPH_MON_INITIAL_MEMBERS" --keyring "$KEYRING" --format json)"
                rc=$?
                if [ $rc -ne 0 ]; then
                    echo "ceph MDS dump check failed with the following output:"
                    echo "$outp"
                    echo "passing probe to avoid restarting MDS. cannot determine if MDS is unhealthy. restarting MDS risks destabilizing ceph/filesystem, which is likely unreachable or in error state"
                    exit 0
                fi

                # get the active and standby MDS in the fs map
                standbyMds=$(echo "$outp" | jq ".standbys | map(.name) | any(.[]; . == \"$MDS_ID\")")
                activeMds=$(echo "$outp" | jq ".filesystems[] | select(.mdsmap.fs_name == \"$FILESYSTEM_NAME\") | .mdsmap.info | map(.name) | any(.[]; . == \"$MDS_ID\")")

                if [[ $standbyMds == true || $activeMds == true ]]; then
                    echo "MDS ID present in MDS map, no need to re-start the container"
                    exit 0
                fi

                echo "Error: MDS ID not present in MDS map"
                exit 1
            failureThreshold: 5
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 25
          name: mds
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 256Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mds-ceph-filesystem-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mds/ceph-ceph-filesystem-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        - command:
          - /bin/bash
          - -x
          - -e
          - -m
          - -c
          - "\nCEPH_CLIENT_ID=ceph-mds.ceph-filesystem-a\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
            edit the logrotate file to only rotate a specific daemon log\n# otherwise
            we will logrotate log files without reloading certain daemons\n# this
            might happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
            \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
            --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
            rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed
            --in-place \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif
            [ \"$LOG_MAX_SIZE\" != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE
            at the 4th line of the logrotate config file with 4 spaces to maintain
            indentation\n\tsed --in-place \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\"
            \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile true; do\n\t# we don't force the
            logrorate but we let the logrotate binary handle the rotation based on
            user's input for periodicity and size\n\tlogrotate --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep
            15m\ndone\n"
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: log-collector
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mds/ceph-ceph-filesystem-a
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 256Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mds-ceph-filesystem-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mds/ceph-ceph-filesystem-a
            name: ceph-daemon-data
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        shareProcessNamespace: true
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mds-ceph-filesystem-a-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mds-ceph-filesystem-a-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: ceph-daemon-data
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-01-31T12:45:42Z"
      lastUpdateTime: "2024-01-31T12:45:45Z"
      message: ReplicaSet "rook-ceph-mds-ceph-filesystem-a-6d784b75dd" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-03-13T16:49:57Z"
      lastUpdateTime: "2024-03-13T16:49:57Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-01-31T12:45:51Z"
    generation: 1
    labels:
      app: rook-ceph-mds
      app.kubernetes.io/component: cephfilesystems.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: ceph-filesystem-b
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mds
      app.kubernetes.io/part-of: ceph-filesystem
      ceph-version: 17.2.6-0
      ceph_daemon_id: ceph-filesystem-b
      ceph_daemon_type: mds
      mds: ceph-filesystem-b
      rook-version: v1.13.1
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      rook_file_system: ceph-filesystem
    name: rook-ceph-mds-ceph-filesystem-b
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephFilesystem
      name: ceph-filesystem
      uid: 9b1420fc-8328-49c8-b3b9-05f41802f214
    resourceVersion: "13084736"
    uid: a958ce97-63eb-495e-bd88-78f2b26dcd34
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-mds
        ceph_daemon_id: ceph-filesystem-b
        mds: ceph-filesystem-b
        rook_cluster: rook-ceph
        rook_file_system: ceph-filesystem
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mds
          app.kubernetes.io/component: cephfilesystems.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: ceph-filesystem-b
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mds
          app.kubernetes.io/part-of: ceph-filesystem
          ceph_daemon_id: ceph-filesystem-b
          ceph_daemon_type: mds
          mds: ceph-filesystem-b
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          rook_file_system: ceph-filesystem
        name: rook-ceph-mds-ceph-filesystem-b
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=ceph-filesystem-b
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --public-addr=$(ROOK_POD_IP)
          command:
          - ceph-mds
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - timeout
              - "20"
              - sh
              - -c
              - |
                #!/usr/bin/env bash
                # do not use 'set -e -u' etc. because it is important to only fail this probe when failure is certain
                # spurious failures risk destabilizing ceph or the filesystem

                MDS_ID="ceph-filesystem-b"
                FILESYSTEM_NAME="ceph-filesystem"
                KEYRING="/etc/ceph/keyring-store/keyring"

                outp="$(ceph fs dump --mon-host="$ROOK_CEPH_MON_HOST" --mon-initial-members="$ROOK_CEPH_MON_INITIAL_MEMBERS" --keyring "$KEYRING" --format json)"
                rc=$?
                if [ $rc -ne 0 ]; then
                    echo "ceph MDS dump check failed with the following output:"
                    echo "$outp"
                    echo "passing probe to avoid restarting MDS. cannot determine if MDS is unhealthy. restarting MDS risks destabilizing ceph/filesystem, which is likely unreachable or in error state"
                    exit 0
                fi

                # get the active and standby MDS in the fs map
                standbyMds=$(echo "$outp" | jq ".standbys | map(.name) | any(.[]; . == \"$MDS_ID\")")
                activeMds=$(echo "$outp" | jq ".filesystems[] | select(.mdsmap.fs_name == \"$FILESYSTEM_NAME\") | .mdsmap.info | map(.name) | any(.[]; . == \"$MDS_ID\")")

                if [[ $standbyMds == true || $activeMds == true ]]; then
                    echo "MDS ID present in MDS map, no need to re-start the container"
                    exit 0
                fi

                echo "Error: MDS ID not present in MDS map"
                exit 1
            failureThreshold: 5
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 25
          name: mds
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 256Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mds-ceph-filesystem-b-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mds/ceph-ceph-filesystem-b
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        - command:
          - /bin/bash
          - -x
          - -e
          - -m
          - -c
          - "\nCEPH_CLIENT_ID=ceph-mds.ceph-filesystem-b\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
            edit the logrotate file to only rotate a specific daemon log\n# otherwise
            we will logrotate log files without reloading certain daemons\n# this
            might happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
            \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
            --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
            rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed
            --in-place \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif
            [ \"$LOG_MAX_SIZE\" != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE
            at the 4th line of the logrotate config file with 4 spaces to maintain
            indentation\n\tsed --in-place \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\"
            \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile true; do\n\t# we don't force the
            logrorate but we let the logrotate binary handle the rotation based on
            user's input for periodicity and size\n\tlogrotate --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep
            15m\ndone\n"
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: log-collector
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mds/ceph-ceph-filesystem-b
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 256Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mds-ceph-filesystem-b-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mds/ceph-ceph-filesystem-b
            name: ceph-daemon-data
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        shareProcessNamespace: true
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mds-ceph-filesystem-b-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mds-ceph-filesystem-b-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: ceph-daemon-data
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-01-31T12:45:51Z"
      lastUpdateTime: "2024-01-31T12:45:54Z"
      message: ReplicaSet "rook-ceph-mds-ceph-filesystem-b-8459d7cc6" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-03-13T16:49:49Z"
      lastUpdateTime: "2024-03-13T16:49:49Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-01-31T12:42:17Z"
    generation: 1
    labels:
      app: rook-ceph-mgr
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mgr
      app.kubernetes.io/part-of: rook-ceph
      ceph-version: 17.2.6-0
      ceph_daemon_id: a
      ceph_daemon_type: mgr
      instance: a
      mgr: a
      mgr_role: active
      rook-version: v1.13.1
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mgr-a
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 02274ff6-97d3-4d09-b941-31edb9bdad99
    resourceVersion: "13085511"
    uid: 5b06e356-b81e-412e-9514-0e71294809fc
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-mgr
        ceph_daemon_id: a
        instance: a
        mgr: a
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        annotations:
          prometheus.io/port: "9283"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: rook-ceph-mgr
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: a
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mgr
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: a
          ceph_daemon_type: mgr
          instance: a
          mgr: a
          mgr_role: active
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mgr-a
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app: rook-ceph-mgr
                  rook_cluster: rook-ceph
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --client-mount-uid=0
          - --client-mount-gid=0
          - --foreground
          - --public-addr=$(ROOK_POD_IP)
          command:
          - ceph-mgr
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_OPERATOR_NAMESPACE
            value: rook-ceph
          - name: ROOK_CEPH_CLUSTER_CRD_VERSION
            value: v1
          - name: ROOK_CEPH_CLUSTER_CRD_NAME
            value: rook-ceph
          - name: CEPH_ARGS
            value: --mon-host $(ROOK_CEPH_MON_HOST) --keyring /etc/ceph/keyring-store/keyring
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: mgr
          ports:
          - containerPort: 6800
            name: mgr
            protocol: TCP
          - containerPort: 9283
            name: http-metrics
            protocol: TCP
          - containerPort: 8443
            name: dashboard
            protocol: TCP
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        - args:
          - ceph
          - mgr
          - watch-active
          env:
          - name: ROOK_CLUSTER_ID
            value: 02274ff6-97d3-4d09-b941-31edb9bdad99
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: ROOK_DASHBOARD_ENABLED
            value: "true"
          - name: ROOK_MONITORING_ENABLED
            value: "false"
          - name: ROOK_UPDATE_INTERVAL
            value: 15s
          - name: ROOK_DAEMON_NAME
            value: a
          - name: ROOK_CEPH_VERSION
            value: ceph version 17.2.6-0 quincy
          image: rook/ceph:v1.13.1
          imagePullPolicy: IfNotPresent
          name: watch-active
          resources:
            limits:
              cpu: 500m
              memory: 100Mi
            requests:
              cpu: 100m
              memory: 40Mi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook-ceph-mon
            name: ceph-admin-secret
            readOnly: true
        - command:
          - /bin/bash
          - -x
          - -e
          - -m
          - -c
          - "\nCEPH_CLIENT_ID=ceph-mgr.a\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
            edit the logrotate file to only rotate a specific daemon log\n# otherwise
            we will logrotate log files without reloading certain daemons\n# this
            might happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
            \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
            --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
            rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed
            --in-place \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif
            [ \"$LOG_MAX_SIZE\" != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE
            at the 4th line of the logrotate config file with 4 spaces to maintain
            indentation\n\tsed --in-place \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\"
            \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile true; do\n\t# we don't force the
            logrorate but we let the logrotate binary handle the rotation based on
            user's input for periodicity and size\n\tlogrotate --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep
            15m\ndone\n"
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: log-collector
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mgr/ceph-a
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-mgr
        serviceAccountName: rook-ceph-mgr
        shareProcessNamespace: true
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mgr-a-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mgr-a-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: ceph-daemon-data
        - name: ceph-admin-secret
          secret:
            defaultMode: 420
            items:
            - key: ceph-secret
              path: secret.keyring
            secretName: rook-ceph-mon
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-01-31T12:42:17Z"
      lastUpdateTime: "2024-01-31T12:42:37Z"
      message: ReplicaSet "rook-ceph-mgr-a-88759fcf8" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-03-13T16:50:12Z"
      lastUpdateTime: "2024-03-13T16:50:12Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-01-31T12:42:17Z"
    generation: 1
    labels:
      app: rook-ceph-mgr
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: b
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mgr
      app.kubernetes.io/part-of: rook-ceph
      ceph-version: 17.2.6-0
      ceph_daemon_id: b
      ceph_daemon_type: mgr
      instance: b
      mgr: b
      mgr_role: active
      rook-version: v1.13.1
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mgr-b
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 02274ff6-97d3-4d09-b941-31edb9bdad99
    resourceVersion: "13085522"
    uid: 2225d5d3-6b85-4b05-8de8-2584400f56aa
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-mgr
        ceph_daemon_id: b
        instance: b
        mgr: b
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        annotations:
          prometheus.io/port: "9283"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: rook-ceph-mgr
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: b
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mgr
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: b
          ceph_daemon_type: mgr
          instance: b
          mgr: b
          mgr_role: active
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mgr-b
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app: rook-ceph-mgr
                  rook_cluster: rook-ceph
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=b
          - --setuser=ceph
          - --setgroup=ceph
          - --client-mount-uid=0
          - --client-mount-gid=0
          - --foreground
          - --public-addr=$(ROOK_POD_IP)
          command:
          - ceph-mgr
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_OPERATOR_NAMESPACE
            value: rook-ceph
          - name: ROOK_CEPH_CLUSTER_CRD_VERSION
            value: v1
          - name: ROOK_CEPH_CLUSTER_CRD_NAME
            value: rook-ceph
          - name: CEPH_ARGS
            value: --mon-host $(ROOK_CEPH_MON_HOST) --keyring /etc/ceph/keyring-store/keyring
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.b.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: mgr
          ports:
          - containerPort: 6800
            name: mgr
            protocol: TCP
          - containerPort: 9283
            name: http-metrics
            protocol: TCP
          - containerPort: 8443
            name: dashboard
            protocol: TCP
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.b.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-b-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-b
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        - args:
          - ceph
          - mgr
          - watch-active
          env:
          - name: ROOK_CLUSTER_ID
            value: 02274ff6-97d3-4d09-b941-31edb9bdad99
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: ROOK_DASHBOARD_ENABLED
            value: "true"
          - name: ROOK_MONITORING_ENABLED
            value: "false"
          - name: ROOK_UPDATE_INTERVAL
            value: 15s
          - name: ROOK_DAEMON_NAME
            value: b
          - name: ROOK_CEPH_VERSION
            value: ceph version 17.2.6-0 quincy
          image: rook/ceph:v1.13.1
          imagePullPolicy: IfNotPresent
          name: watch-active
          resources:
            limits:
              cpu: 500m
              memory: 100Mi
            requests:
              cpu: 100m
              memory: 40Mi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook-ceph-mon
            name: ceph-admin-secret
            readOnly: true
        - command:
          - /bin/bash
          - -x
          - -e
          - -m
          - -c
          - "\nCEPH_CLIENT_ID=ceph-mgr.b\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
            edit the logrotate file to only rotate a specific daemon log\n# otherwise
            we will logrotate log files without reloading certain daemons\n# this
            might happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
            \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
            --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
            rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed
            --in-place \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif
            [ \"$LOG_MAX_SIZE\" != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE
            at the 4th line of the logrotate config file with 4 spaces to maintain
            indentation\n\tsed --in-place \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\"
            \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile true; do\n\t# we don't force the
            logrorate but we let the logrotate binary handle the rotation based on
            user's input for periodicity and size\n\tlogrotate --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep
            15m\ndone\n"
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: log-collector
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mgr/ceph-b
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-b-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-b
            name: ceph-daemon-data
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-mgr
        serviceAccountName: rook-ceph-mgr
        shareProcessNamespace: true
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mgr-b-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mgr-b-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: ceph-daemon-data
        - name: ceph-admin-secret
          secret:
            defaultMode: 420
            items:
            - key: ceph-secret
              path: secret.keyring
            secretName: rook-ceph-mon
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-01-31T12:42:18Z"
      lastUpdateTime: "2024-01-31T12:42:38Z"
      message: ReplicaSet "rook-ceph-mgr-b-684bb98899" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-03-13T16:50:12Z"
      lastUpdateTime: "2024-03-13T16:50:12Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-01-31T12:41:16Z"
    generation: 1
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph-version: 17.2.6-0
      ceph_daemon_id: a
      ceph_daemon_type: mon
      mon: a
      mon_cluster: rook-ceph
      rook-version: v1.13.1
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-a
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 02274ff6-97d3-4d09-b941-31edb9bdad99
    resourceVersion: "13085452"
    uid: f6b4f854-7cf9-488c-966b-ab8c121abe41
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: a
        mon: a
        mon_cluster: rook-ceph
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: a
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mon
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: a
          ceph_daemon_type: mon
          mon: a
          mon_cluster: rook-ceph
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mon-a
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --public-addr=10.43.110.147
          - --setuser-match-path=/var/lib/ceph/mon/ceph-a/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: mon
          ports:
          - containerPort: 3300
            name: tcp-msgr2
            protocol: TCP
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources:
            limits:
              cpu: 1500m
              memory: 1536Mi
            requests:
              cpu: 350m
              memory: 512Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        - command:
          - /bin/bash
          - -x
          - -e
          - -m
          - -c
          - "\nCEPH_CLIENT_ID=ceph-mon.a\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
            edit the logrotate file to only rotate a specific daemon log\n# otherwise
            we will logrotate log files without reloading certain daemons\n# this
            might happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
            \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
            --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
            rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed
            --in-place \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif
            [ \"$LOG_MAX_SIZE\" != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE
            at the 4th line of the logrotate config file with 4 spaces to maintain
            indentation\n\tsed --in-place \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\"
            \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile true; do\n\t# we don't force the
            logrorate but we let the logrotate binary handle the rotation based on
            user's input for periodicity and size\n\tlogrotate --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep
            15m\ndone\n"
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: log-collector
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mon/ceph-a
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 1500m
              memory: 1536Mi
            requests:
              cpu: 350m
              memory: 512Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        - args:
          - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --public-addr=10.43.110.147
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources:
            limits:
              cpu: 1500m
              memory: 1536Mi
            requests:
              cpu: 350m
              memory: 512Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: worker-3
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        shareProcessNamespace: true
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-a/data
            type: ""
          name: ceph-daemon-data
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-01-31T12:41:16Z"
      lastUpdateTime: "2024-01-31T12:41:38Z"
      message: ReplicaSet "rook-ceph-mon-a-5c7dcd9676" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-03-13T16:50:10Z"
      lastUpdateTime: "2024-03-13T16:50:10Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-01-31T12:41:56Z"
    generation: 1
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: c
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph-version: 17.2.6-0
      ceph_daemon_id: c
      ceph_daemon_type: mon
      mon: c
      mon_cluster: rook-ceph
      rook-version: v1.13.1
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-c
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 02274ff6-97d3-4d09-b941-31edb9bdad99
    resourceVersion: "13085380"
    uid: a0997286-60f7-4343-9fd6-2497b161761d
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: c
        mon: c
        mon_cluster: rook-ceph
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: c
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mon
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: c
          ceph_daemon_type: mon
          mon: c
          mon_cluster: rook-ceph
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mon-c
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=c
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --public-addr=10.43.221.137
          - --setuser-match-path=/var/lib/ceph/mon/ceph-c/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.c.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: mon
          ports:
          - containerPort: 3300
            name: tcp-msgr2
            protocol: TCP
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources:
            limits:
              cpu: 1500m
              memory: 1536Mi
            requests:
              cpu: 350m
              memory: 512Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.c.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-c
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        - command:
          - /bin/bash
          - -x
          - -e
          - -m
          - -c
          - "\nCEPH_CLIENT_ID=ceph-mon.c\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
            edit the logrotate file to only rotate a specific daemon log\n# otherwise
            we will logrotate log files without reloading certain daemons\n# this
            might happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
            \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
            --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
            rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed
            --in-place \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif
            [ \"$LOG_MAX_SIZE\" != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE
            at the 4th line of the logrotate config file with 4 spaces to maintain
            indentation\n\tsed --in-place \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\"
            \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile true; do\n\t# we don't force the
            logrorate but we let the logrotate binary handle the rotation based on
            user's input for periodicity and size\n\tlogrotate --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep
            15m\ndone\n"
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: log-collector
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mon/ceph-c
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 1500m
              memory: 1536Mi
            requests:
              cpu: 350m
              memory: 512Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-c
            name: ceph-daemon-data
        - args:
          - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=c
          - --setuser=ceph
          - --setgroup=ceph
          - --public-addr=10.43.221.137
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources:
            limits:
              cpu: 1500m
              memory: 1536Mi
            requests:
              cpu: 350m
              memory: 512Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-c
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: worker-1
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        shareProcessNamespace: true
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-c/data
            type: ""
          name: ceph-daemon-data
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-01-31T12:41:56Z"
      lastUpdateTime: "2024-01-31T12:42:17Z"
      message: ReplicaSet "rook-ceph-mon-c-59cdc6599b" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-03-13T16:50:03Z"
      lastUpdateTime: "2024-03-13T16:50:03Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: UEsDBBQACAAIAAAAAAAAAAAAAAAAAAAAAAAIAAAAb3JpZ2luYWzsWgtvI7cR/issY+CSQtTDr4sVCIUrKxchp0dtp2iadQVqd3aXNZfckFzZ6sX/vRhy9bAt23fJtUhRA4awIudFzszHb2l9oAU4nnDHafcDlXwO0uITL0vapUbraxZDmbNCK9rA0eZ1NQejwIFtCt2KdVFqBcrRLkXBWFbWgbFN/NJE/abQuzUNcAcJmy/vOdIlGO602akjlHVcxUC7FHYKFFzx7BONKl5AHf2Tyyy5cUyn20ZpI6gswFihFe3SztvmfvOYteuZWcKh0Gomkjrc7UG3LNFp8Fd4dQhPs3oLH/jyzxtfi06zc9Ds1BMY4mqNDNdjS+536aGFncbvGrTeg3v5ZhjQU8b0jQJzDikYUDFY2v0Ja0b8dR3gdv5bCwx0LnV8PUG9M5DgvJgzFTRorJUzWkqMK4xcC4W71ocy79cRPw6SNmjlN7e9v//2ME2P2cnb5IAdJu0TNj857LCDDiTzk3nCk5MTend116C2hBjL20ApRcwt7XYa1IKEGKuj+4EW3MX5+xfaYGd2PzaJu1OAsTnDHWRL9FuXxzmELsEUOShKic/d1579jT37+Tvzv92AW6WMzcOFAlM3ocnwgTKWWpH0jtOD44NOzFnKTxJ2OD8CNudHMTs5PoL4+Gi/cxgf0gZl7BqWRqis1wIXt9BHqx5h1mkDq29eNoGUV9IxqTPmNLMuAWN62Ln3psGY56ZxhfVuvGQJp8McKw2k4raXwLzKyK5oUiGhl3JpX/T2QBIlcm1db+/L88nk+1l/MP1uNpqMZ99NLi6/WosIJZzgkhVQzMHYR9LD8fByePp+NhqM/jw4vwiKIukFJxZcZcH06pLwA5nRVbkZSbUBHFKJ/1pWcylixpPE9Drt5uFBs3PS/Ppo2xrzqMVK7vJea8FNS4p5yGGhlX9g0PJpbCbzbaNzoZJguV7FdHI2G06/olcIykXBEYV/2rTZVYOCWvg6qwu2Pxlfng7Hg/PZcHT6bkAbdMFlhTM/V3zpMQPjwI/uIhyQ9K6xVkd/49PRWu9bowss6lSATM4hXT9PucuxK2vga3r9u7sdpi6mp/1fZy+02T2j48nZ4JMCxK5sKp3AeGeAo8Focv7j7P1wNLx8aNKA1ZWJ4dst04lYCIsnE0VWsZKgXSpFIZxtFlBos3zSz/ngLz8MLn6bJwM/V2Cf8dWf/vBrFtTZuaC4rHZ7+KxLeeTFd+8PF4PZ+en4bDKajSfj/nYxe0TakvfNMrp4d76/JVTYzOzPPKDMQMVmWSLJqQfw0DVg7Xrkkbl7ePNwnRaZgPselvUar2EZDqkZQtYOcsRirVKR3V/ms0j18S5rCJzVEPhJ3gPCfHQ7Oe4q2yx1MpyisasGFQXPnseXWmZaSTnVUsQY9jAdazc1YJH2NKgUC1Bg7dTouWdUcLs6Tdeoh0jXoEzQBrUemWPaoJHSlSt7Ed37En0SxnhSCMUCeSAtU6lNRIiZTWhyq68J7ltYDdmPqnb7APBz/7jzVUQjZeLe3p8iJVLyE9kzMWEKSJtcfUNcDipSkYM41yTySExqXzlw6XIS5xBfk5QLCQm5ES5HHZJqKfWNUBnBeCvXRS9rM3s4GFHyC7GQEAbkjW39oxXCIq3sjRe9FQ5jiVQqIkWRPdd5PwPJlxcQa5UggW43qBMF6Mqtx442jCbwqFIbF/jJmq5MtXG0e3DQbq9lXVwy30SoYbTTsZa0Sy/7U98sD1SP33598ki1s0P1agMCnicHqPHZLitEoqN2u0CKFwAOBw6ORwJJ1goxNsIHR/dljzr7KIqEDOLKCLfsa+Xg1qFKacRCSMggoV3f9J7mc+Oq8rXyPqXy0Ell4DI3YHMtE9o9fqYcSzBCJy9W6ELLqoCRrlRdmwU+1siz5sEP0c0DG9MLMEYk4A8YnkyUXIZ3VyzUe3ZWednY8QkKW2mZ1fE1S4ShjxSfIOK70NZb2jD0FyLyDFFnO1eH1qTOHkdzj1bGhtudqmHieeUNJ925Jcy/2GLT3mhzLVR2JsyjqAMcrBumNReqNQ8xsVv88Gy72PSOP/T674eD8eVseNbb9EikpoPz4eRs2B9e/thLuJDLSL2fvJudTy5PLwfhtPx2+H4Q3oykzox23EEz8ZEE2dHp32YXw78Pekft9ihSQbX3NlKR+oJAIpzvjLUuwTcP4jTRSi5JPcYJckeRinjVZlJnqK9dDuZGWCA3QG6ElFuGpM68Mev7T1eOGJCaJ9h+MRiEy9qaRVMuF5YUIssdyXlZgiI3OShSVNKJUsJKlJhKEa180JYXQAoe50JBpHzTChJR+8sfm1Jnv+zd31g/FlFs9F176KFAfUEMlJLHQOqXM+K3PUBIhiczwbcaIlRZudonE4oFnYjalpdv7W1lrpV9vNd672Ks0MY6hrck1YZwKckW1Nmw+8TlXBEzT1ghjNFmZ1C12berh70Qx8uReeytJVaVFFHyhx6JaDui22D8BeGJT27Bb634F5B7SoSHQjt0OZFCAdHpg8IL6BXqz2/3IfFvPRarseDCH7BEqASU48he0emjtR4KEkWbv52xPL9oBPZI3eS+D0wF35BEh/XdAEm0euMwGzFswjcY/rxyKCDhYUPNheJmSXKuEhmU/ITQisw5xq9DTb2xoap8rsMxIWLhloSrhOAiMIiNWcYWYObawrMJdFYClKRzVEQq0dgm9HPx1Boepc5YrGV9TfkCnXnEZt49QWU6DwXb7U+jMs4tVxe2v/Oz9D9/5F35dxMlXP+p+7i6lPz9i8ENtmKxunzs1oE9DHR3DNv78OTZ+uAKJ9c3n70svVG2puX+5Pb7/7si3L/z0vw/p3lo5PXO+n/9zvq5S+riOrWvF8q10dcL5dcLZXi9UP4tF8qfmcSgbw+SqX3lLa+85aN5y1UDs6Z9IiW3dhxE7dI6KBgiNouNcCLmMtSV48ata/FU3vCl9Re83MDU6BiCifp//dtvVXb7fHwiy6XR/4TYYf18oOsSDvftqchGvMQJ4aAIw6H36qZq0EInQLv7J/sNWoZNwSU3cd5fxT3n/O7uahsSnkx7aP8NEIyflfcmEYpCkj6s4lonC/VacFtqE34dVP9u40wY/5a8nJj++rczL1b2i47WUbZC+T1dlp9galWOTxbq1V34YRB3FSLL3d2/AwAA//9QSwcI0WGZBjwJAACvJwAAUEsBAhQAFAAIAAgAAAAAANFhmQY8CQAArycAAAgAAAAAAAAAAAAAAAAAAAAAAG9yaWdpbmFsUEsFBgAAAAABAAEANgAAAHIJAAAAAA==
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-02-28T07:57:57Z"
    generation: 1
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: e
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph-version: 17.2.6-0
      ceph_daemon_id: e
      ceph_daemon_type: mon
      mon: e
      mon_cluster: rook-ceph
      rook-version: v1.13.1
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-e
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 02274ff6-97d3-4d09-b941-31edb9bdad99
    resourceVersion: "13085478"
    uid: 97915027-b2f3-46a7-b38a-e1d0ecc7d148
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: e
        mon: e
        mon_cluster: rook-ceph
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: e
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mon
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: e
          ceph_daemon_type: mon
          mon: e
          mon_cluster: rook-ceph
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mon-e
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=e
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --public-addr=10.43.19.85
          - --setuser-match-path=/var/lib/ceph/mon/ceph-e/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.e.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: mon
          ports:
          - containerPort: 3300
            name: tcp-msgr2
            protocol: TCP
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources:
            limits:
              cpu: 1500m
              memory: 1536Mi
            requests:
              cpu: 350m
              memory: 512Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.e.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-e
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        - command:
          - /bin/bash
          - -x
          - -e
          - -m
          - -c
          - "\nCEPH_CLIENT_ID=ceph-mon.e\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
            edit the logrotate file to only rotate a specific daemon log\n# otherwise
            we will logrotate log files without reloading certain daemons\n# this
            might happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
            \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
            --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
            rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed
            --in-place \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif
            [ \"$LOG_MAX_SIZE\" != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE
            at the 4th line of the logrotate config file with 4 spaces to maintain
            indentation\n\tsed --in-place \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\"
            \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile true; do\n\t# we don't force the
            logrorate but we let the logrotate binary handle the rotation based on
            user's input for periodicity and size\n\tlogrotate --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep
            15m\ndone\n"
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: log-collector
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mon/ceph-e
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 1500m
              memory: 1536Mi
            requests:
              cpu: 350m
              memory: 512Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-e
            name: ceph-daemon-data
        - args:
          - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=e
          - --setuser=ceph
          - --setgroup=ceph
          - --public-addr=10.43.19.85
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources:
            limits:
              cpu: 1500m
              memory: 1536Mi
            requests:
              cpu: 350m
              memory: 512Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-e
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: worker-4
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        shareProcessNamespace: true
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-e/data
            type: ""
          name: ceph-daemon-data
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-02-28T07:57:57Z"
      lastUpdateTime: "2024-02-28T07:59:48Z"
      message: ReplicaSet "rook-ceph-mon-e-76cdf858cf" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-03-13T16:50:11Z"
      lastUpdateTime: "2024-03-13T16:50:11Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: rook-ceph
      meta.helm.sh/release-namespace: rook-ceph
    creationTimestamp: "2024-01-31T12:38:04Z"
    generation: 1
    labels:
      app.kubernetes.io/created-by: helm
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: rook-ceph-operator
      helm.sh/chart: rook-ceph-v1.13.1
      operator: rook
      storage-backend: ceph
    name: rook-ceph-operator
    namespace: rook-ceph
    resourceVersion: "13085215"
    uid: 7a3cf0ff-4a2c-4d49-8d00-fbf6db090b18
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-operator
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-operator
          helm.sh/chart: rook-ceph-v1.13.1
      spec:
        containers:
        - args:
          - ceph
          - operator
          env:
          - name: ROOK_CURRENT_NAMESPACE_ONLY
            value: "false"
          - name: ROOK_HOSTPATH_REQUIRES_PRIVILEGED
            value: "false"
          - name: ROOK_DISABLE_DEVICE_HOTPLUG
            value: "false"
          - name: ROOK_DISCOVER_DEVICES_INTERVAL
            value: 60m
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rook/ceph:v1.13.1
          imagePullPolicy: IfNotPresent
          name: rook-ceph-operator
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext:
            capabilities:
              drop:
              - ALL
            runAsGroup: 2016
            runAsNonRoot: true
            runAsUser: 2016
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-config
          - mountPath: /etc/ceph
            name: default-config-dir
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-system
        serviceAccountName: rook-ceph-system
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - emptyDir: {}
          name: rook-config
        - emptyDir: {}
          name: default-config-dir
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-01-31T12:38:04Z"
      lastUpdateTime: "2024-01-31T12:38:06Z"
      message: ReplicaSet "rook-ceph-operator-54bbffd5dd" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-03-13T16:49:59Z"
      lastUpdateTime: "2024-03-13T16:49:59Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-02-01T09:38:50Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "0"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "0"
      ceph-version: 17.2.6-0
      ceph_daemon_id: "0"
      ceph_daemon_type: osd
      device-class: ssd
      failure-domain: worker-3
      osd: "0"
      osd-store: bluestore
      portable: "false"
      rook-version: v1.13.1
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: worker-3
      topology-location-root: default
    name: rook-ceph-osd-0
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 02274ff6-97d3-4d09-b941-31edb9bdad99
    resourceVersion: "13085850"
    uid: 420a6c3c-fd4c-4123-b244-9fe7e3e54002
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "0"
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "0"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "0"
          ceph_daemon_id: "0"
          ceph_daemon_type: osd
          device-class: ssd
          failure-domain: worker-3
          osd: "0"
          osd-store: bluestore
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: worker-3
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "0"
          - --fsid
          - 6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=worker-3
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: worker-3
          - name: ROOK_CLUSTER_ID
            value: 02274ff6-97d3-4d09-b941-31edb9bdad99
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: worker-3
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: 9b721017-bd88-40c6-84cb-3ef9a79e3a0c
          - name: ROOK_OSD_ID
            value: "0"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdb
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: ssd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.0.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: osd
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.0.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          workingDir: /var/log/ceph
        - command:
          - /bin/bash
          - -x
          - -e
          - -m
          - -c
          - "\nCEPH_CLIENT_ID=ceph-osd.0\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
            edit the logrotate file to only rotate a specific daemon log\n# otherwise
            we will logrotate log files without reloading certain daemons\n# this
            might happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
            \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
            --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
            rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed
            --in-place \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif
            [ \"$LOG_MAX_SIZE\" != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE
            at the 4th line of the logrotate config file with 4 spaces to maintain
            indentation\n\tsed --in-place \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\"
            \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile true; do\n\t# we don't force the
            logrorate but we let the logrotate binary handle the rotation based on
            user's input for periodicity and size\n\tlogrotate --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep
            15m\ndone\n"
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: log-collector
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4\nOSD_UUID=9b721017-bd88-40c6-84cb-3ef9a79e3a0c\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tceph-volume raw list \"$DEVICE\" >
            \"$OSD_LIST\"\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device < \"$OSD_LIST\";
            then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device
            < \"$OSD_LIST\")\"\n\tfi\n\t[[ -z \"$DEVICE\" ]] && { echo \"no device\"
            ; exit 1 ; }\n\n\t# If a kernel device name change happens and a block
            device file\n\t# in the OSD directory becomes missing, this OSD fails
            to start\n\t# continuously. This problem can be resolved by confirming\n\t#
            the validity of the device file and recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdb
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "0"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: activate
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-0
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: worker-3
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        shareProcessNamespace: true
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/6f3631ca-fa9d-4b5e-ba5c-965ec65214c4_9b721017-bd88-40c6-84cb-3ef9a79e3a0c
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-02-01T09:38:50Z"
      lastUpdateTime: "2024-02-01T09:39:11Z"
      message: ReplicaSet "rook-ceph-osd-0-7b44966dc6" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-03-13T16:50:50Z"
      lastUpdateTime: "2024-03-13T16:50:50Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-02-01T09:38:53Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "1"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "1"
      ceph-version: 17.2.6-0
      ceph_daemon_id: "1"
      ceph_daemon_type: osd
      device-class: ssd
      failure-domain: worker-1
      osd: "1"
      osd-store: bluestore
      portable: "false"
      rook-version: v1.13.1
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: worker-1
      topology-location-root: default
    name: rook-ceph-osd-1
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 02274ff6-97d3-4d09-b941-31edb9bdad99
    resourceVersion: "13085854"
    uid: e12e6541-dcd1-48ab-a657-6a6ab6ccd0a1
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "1"
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "1"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "1"
          ceph_daemon_id: "1"
          ceph_daemon_type: osd
          device-class: ssd
          failure-domain: worker-1
          osd: "1"
          osd-store: bluestore
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: worker-1
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "1"
          - --fsid
          - 6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=worker-1
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: worker-1
          - name: ROOK_CLUSTER_ID
            value: 02274ff6-97d3-4d09-b941-31edb9bdad99
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: worker-1
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: 098de1f8-d466-4e51-aef3-874dee214609
          - name: ROOK_OSD_ID
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdb
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: ssd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.1.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: osd
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.1.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          workingDir: /var/log/ceph
        - command:
          - /bin/bash
          - -x
          - -e
          - -m
          - -c
          - "\nCEPH_CLIENT_ID=ceph-osd.1\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
            edit the logrotate file to only rotate a specific daemon log\n# otherwise
            we will logrotate log files without reloading certain daemons\n# this
            might happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
            \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
            --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
            rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed
            --in-place \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif
            [ \"$LOG_MAX_SIZE\" != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE
            at the 4th line of the logrotate config file with 4 spaces to maintain
            indentation\n\tsed --in-place \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\"
            \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile true; do\n\t# we don't force the
            logrorate but we let the logrotate binary handle the rotation based on
            user's input for periodicity and size\n\tlogrotate --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep
            15m\ndone\n"
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: log-collector
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4\nOSD_UUID=098de1f8-d466-4e51-aef3-874dee214609\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tceph-volume raw list \"$DEVICE\" >
            \"$OSD_LIST\"\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device < \"$OSD_LIST\";
            then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device
            < \"$OSD_LIST\")\"\n\tfi\n\t[[ -z \"$DEVICE\" ]] && { echo \"no device\"
            ; exit 1 ; }\n\n\t# If a kernel device name change happens and a block
            device file\n\t# in the OSD directory becomes missing, this OSD fails
            to start\n\t# continuously. This problem can be resolved by confirming\n\t#
            the validity of the device file and recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdb
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "1"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: activate
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-1
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: worker-1
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        shareProcessNamespace: true
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/6f3631ca-fa9d-4b5e-ba5c-965ec65214c4_098de1f8-d466-4e51-aef3-874dee214609
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-02-01T09:38:53Z"
      lastUpdateTime: "2024-02-01T09:39:13Z"
      message: ReplicaSet "rook-ceph-osd-1-78f8d45768" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-03-13T16:50:51Z"
      lastUpdateTime: "2024-03-13T16:50:51Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-02-01T09:38:52Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "2"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "2"
      ceph-version: 17.2.6-0
      ceph_daemon_id: "2"
      ceph_daemon_type: osd
      device-class: ssd
      failure-domain: worker-4
      osd: "2"
      osd-store: bluestore
      portable: "false"
      rook-version: v1.13.1
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: worker-4
      topology-location-root: default
    name: rook-ceph-osd-2
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 02274ff6-97d3-4d09-b941-31edb9bdad99
    resourceVersion: "13085791"
    uid: 8853349f-b64d-40c2-aeea-46c79d5784ba
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "2"
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "2"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "2"
          ceph_daemon_id: "2"
          ceph_daemon_type: osd
          device-class: ssd
          failure-domain: worker-4
          osd: "2"
          osd-store: bluestore
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: worker-4
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "2"
          - --fsid
          - 6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=worker-4
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: worker-4
          - name: ROOK_CLUSTER_ID
            value: 02274ff6-97d3-4d09-b941-31edb9bdad99
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: worker-4
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: 4dfb6650-7784-48ca-8101-ef967416f326
          - name: ROOK_OSD_ID
            value: "2"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdb
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: ssd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.2.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: osd
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.2.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          workingDir: /var/log/ceph
        - command:
          - /bin/bash
          - -x
          - -e
          - -m
          - -c
          - "\nCEPH_CLIENT_ID=ceph-osd.2\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
            edit the logrotate file to only rotate a specific daemon log\n# otherwise
            we will logrotate log files without reloading certain daemons\n# this
            might happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
            \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
            --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
            rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed
            --in-place \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif
            [ \"$LOG_MAX_SIZE\" != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE
            at the 4th line of the logrotate config file with 4 spaces to maintain
            indentation\n\tsed --in-place \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\"
            \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile true; do\n\t# we don't force the
            logrorate but we let the logrotate binary handle the rotation based on
            user's input for periodicity and size\n\tlogrotate --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep
            15m\ndone\n"
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: log-collector
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4\nOSD_UUID=4dfb6650-7784-48ca-8101-ef967416f326\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tceph-volume raw list \"$DEVICE\" >
            \"$OSD_LIST\"\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device < \"$OSD_LIST\";
            then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device
            < \"$OSD_LIST\")\"\n\tfi\n\t[[ -z \"$DEVICE\" ]] && { echo \"no device\"
            ; exit 1 ; }\n\n\t# If a kernel device name change happens and a block
            device file\n\t# in the OSD directory becomes missing, this OSD fails
            to start\n\t# continuously. This problem can be resolved by confirming\n\t#
            the validity of the device file and recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdb
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "2"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: activate
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-2
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: worker-4
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        shareProcessNamespace: true
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/6f3631ca-fa9d-4b5e-ba5c-965ec65214c4_4dfb6650-7784-48ca-8101-ef967416f326
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-02-01T09:38:52Z"
      lastUpdateTime: "2024-02-01T09:39:13Z"
      message: ReplicaSet "rook-ceph-osd-2-77f4cc765" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-03-13T16:50:42Z"
      lastUpdateTime: "2024-03-13T16:50:42Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-02-01T09:39:41Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "3"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "3"
      ceph-version: 17.2.6-0
      ceph_daemon_id: "3"
      ceph_daemon_type: osd
      device-class: ssd
      failure-domain: worker-2
      osd: "3"
      osd-store: bluestore
      portable: "false"
      rook-version: v1.13.1
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: worker-2
      topology-location-root: default
    name: rook-ceph-osd-3
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 02274ff6-97d3-4d09-b941-31edb9bdad99
    resourceVersion: "13085663"
    uid: 3f639ac3-9697-44d7-b967-6b2a1adbbe24
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "3"
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "3"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "3"
          ceph_daemon_id: "3"
          ceph_daemon_type: osd
          device-class: ssd
          failure-domain: worker-2
          osd: "3"
          osd-store: bluestore
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: worker-2
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "3"
          - --fsid
          - 6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=worker-2
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: worker-2
          - name: ROOK_CLUSTER_ID
            value: 02274ff6-97d3-4d09-b941-31edb9bdad99
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: worker-2
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: 3e4616df-9857-4b48-8919-f1f41a479006
          - name: ROOK_OSD_ID
            value: "3"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdb
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: ssd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.3.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: osd
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.3.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-3
            name: activate-osd
          workingDir: /var/log/ceph
        - command:
          - /bin/bash
          - -x
          - -e
          - -m
          - -c
          - "\nCEPH_CLIENT_ID=ceph-osd.3\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
            edit the logrotate file to only rotate a specific daemon log\n# otherwise
            we will logrotate log files without reloading certain daemons\n# this
            might happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
            \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
            --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
            rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed
            --in-place \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif
            [ \"$LOG_MAX_SIZE\" != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE
            at the 4th line of the logrotate config file with 4 spaces to maintain
            indentation\n\tsed --in-place \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\"
            \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile true; do\n\t# we don't force the
            logrorate but we let the logrotate binary handle the rotation based on
            user's input for periodicity and size\n\tlogrotate --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep
            15m\ndone\n"
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: log-collector
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4\nOSD_UUID=3e4616df-9857-4b48-8919-f1f41a479006\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tceph-volume raw list \"$DEVICE\" >
            \"$OSD_LIST\"\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device < \"$OSD_LIST\";
            then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device
            < \"$OSD_LIST\")\"\n\tfi\n\t[[ -z \"$DEVICE\" ]] && { echo \"no device\"
            ; exit 1 ; }\n\n\t# If a kernel device name change happens and a block
            device file\n\t# in the OSD directory becomes missing, this OSD fails
            to start\n\t# continuously. This problem can be resolved by confirming\n\t#
            the validity of the device file and recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdb
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "3"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: activate
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-3
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-3
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-3
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-3
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: worker-2
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        shareProcessNamespace: true
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/6f3631ca-fa9d-4b5e-ba5c-965ec65214c4_3e4616df-9857-4b48-8919-f1f41a479006
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-02-01T09:39:41Z"
      lastUpdateTime: "2024-02-01T09:40:02Z"
      message: ReplicaSet "rook-ceph-osd-3-665577cf9d" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-03-13T16:50:27Z"
      lastUpdateTime: "2024-03-13T16:50:27Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-02-01T09:40:48Z"
    generation: 1
    labels:
      app: rook-ceph-rgw
      app.kubernetes.io/component: cephobjectstores.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: ceph-objectstore
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-rgw
      app.kubernetes.io/part-of: ceph-objectstore
      ceph-version: 17.2.6-0
      ceph_daemon_id: ceph-objectstore
      ceph_daemon_type: rgw
      rgw: ceph-objectstore
      rook-version: v1.13.1
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      rook_object_store: ceph-objectstore
    name: rook-ceph-rgw-ceph-objectstore-a
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephObjectStore
      name: ceph-objectstore
      uid: 6f4d07cd-379c-4e9f-8d20-2795daa08703
    resourceVersion: "13086172"
    uid: a4edfa15-76e5-4947-a89a-9250fb0d82d6
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-rgw
        ceph_daemon_id: ceph-objectstore
        rgw: ceph-objectstore
        rook_cluster: rook-ceph
        rook_object_store: ceph-objectstore
    strategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-rgw
          app.kubernetes.io/component: cephobjectstores.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: ceph-objectstore
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-rgw
          app.kubernetes.io/part-of: ceph-objectstore
          ceph_daemon_id: ceph-objectstore
          ceph_daemon_type: rgw
          rgw: ceph-objectstore
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          rook_object_store: ceph-objectstore
        name: rook-ceph-rgw-ceph-objectstore-a
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: rook-ceph-rgw
                    ceph_daemon_id: ceph-objectstore
                    rgw: ceph-objectstore
                    rook_cluster: rook-ceph
                    rook_object_store: ceph-objectstore
                topologyKey: kubernetes.io/hostname
              weight: 50
        containers:
        - args:
          - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=rgw.ceph.objectstore.a
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --rgw-frontends=beast port=8080
          - --host=$(POD_NAME)
          - --rgw-mime-types-file=/etc/ceph/rgw/mime.types
          - --rgw-realm=ceph-objectstore
          - --rgw-zonegroup=ceph-objectstore
          - --rgw-zone=ceph-objectstore
          command:
          - radosgw
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: rgw
          readinessProbe:
            exec:
              command:
              - bash
              - -c
              - |
                #!/usr/bin/env bash

                PROBE_TYPE="readiness"
                PROBE_PORT="8080"
                PROBE_PROTOCOL="HTTP"

                # standard bash codes start at 126 and progress upward. pick error codes from 125 downward for
                # script as to allow curl to output new error codes and still return a distinctive number.
                USAGE_ERR_CODE=125
                PROBE_ERR_CODE=124
                # curl error codes: 1-123

                STARTUP_TYPE='startup'
                READINESS_TYPE='readiness'

                RGW_URL="$PROBE_PROTOCOL://0.0.0.0:$PROBE_PORT"

                function check() {
                  local URL="$1"
                  # --insecure - don't validate ssl if using secure port only
                  # --silent - don't output progress info
                  # --output /dev/stderr - output HTML header to stdout (good for debugging)
                  # --write-out '%{response_code}' - print the HTTP response code to stdout
                  curl --insecure --silent --output /dev/stderr --write-out '%{response_code}' "$URL"
                }

                http_response="$(check "$RGW_URL")"
                retcode=$?

                if [[ $retcode -ne 0 ]]; then
                  # if this is the startup probe, always returning failure. if startup probe passes, all subsequent
                  # probes can rely on the assumption that the health check was once succeeding without errors.
                  # if this is the readiness probe, we know that curl was previously working correctly in the
                  # startup probe, so curl error most likely means some new error with the RGW.
                  echo "RGW health check failed with error code: $retcode. the RGW likely cannot be reached by clients" >/dev/stderr
                  exit $retcode
                fi

                RGW_RATE_LIMITING_RESPONSE=503
                RGW_MISCONFIGURATION_RESPONSE=500

                if [[ $http_response -ge 200 ]] && [[ $http_response -lt 400 ]]; then
                  # 200-399 are successful responses. same behavior as Kubernetes' HTTP probe
                  exit 0

                elif [[ $http_response -eq $RGW_RATE_LIMITING_RESPONSE ]]; then
                  # S3's '503: slow down' code is not an error but an indication that RGW is throttling client
                  # traffic. failing the readiness check here would only cause an increase in client connections on
                  # other RGWs and likely cause those to fail also in a cascade. i.e., a special healthy response.
                  echo "INFO: RGW is rate limiting" 2>/dev/stderr
                  exit 0

                elif [[ $http_response -eq $RGW_MISCONFIGURATION_RESPONSE ]]; then
                  # can't specifically determine if the RGW is running or not. most likely a misconfiguration.
                  case "$PROBE_TYPE" in
                  "$STARTUP_TYPE")
                    # fail until we can accurately get a valid healthy response when runtime starts.
                    echo 'FAIL: HTTP code 500 suggests an RGW misconfiguration.' >/dev/stderr
                    exit $PROBE_ERR_CODE
                    ;;
                  "$READINESS_TYPE")
                    # config likely modified at runtime which could result in all RGWs failing this check.
                    # occasional client failures are still better than total failure, so ignore this
                    echo 'WARN: HTTP code 500 suggests an RGW misconfiguration' >/dev/stderr
                    exit 0
                    ;;
                  *)
                    # prior arg validation means this path should never be activated, but keep to be safe
                    echo "ERROR: probe type is unknown: $PROBE_TYPE" >/dev/stderr
                    exit $USAGE_ERR_CODE
                    ;;
                  esac

                else
                  # anything else is a failing response. same behavior as Kubernetes' HTTP probe
                  echo "FAIL: received an HTTP error code: $http_response"
                  exit $PROBE_ERR_CODE

                fi
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 3
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 256Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - bash
              - -c
              - |
                #!/usr/bin/env bash

                PROBE_TYPE="startup"
                PROBE_PORT="8080"
                PROBE_PROTOCOL="HTTP"

                # standard bash codes start at 126 and progress upward. pick error codes from 125 downward for
                # script as to allow curl to output new error codes and still return a distinctive number.
                USAGE_ERR_CODE=125
                PROBE_ERR_CODE=124
                # curl error codes: 1-123

                STARTUP_TYPE='startup'
                READINESS_TYPE='readiness'

                RGW_URL="$PROBE_PROTOCOL://0.0.0.0:$PROBE_PORT"

                function check() {
                  local URL="$1"
                  # --insecure - don't validate ssl if using secure port only
                  # --silent - don't output progress info
                  # --output /dev/stderr - output HTML header to stdout (good for debugging)
                  # --write-out '%{response_code}' - print the HTTP response code to stdout
                  curl --insecure --silent --output /dev/stderr --write-out '%{response_code}' "$URL"
                }

                http_response="$(check "$RGW_URL")"
                retcode=$?

                if [[ $retcode -ne 0 ]]; then
                  # if this is the startup probe, always returning failure. if startup probe passes, all subsequent
                  # probes can rely on the assumption that the health check was once succeeding without errors.
                  # if this is the readiness probe, we know that curl was previously working correctly in the
                  # startup probe, so curl error most likely means some new error with the RGW.
                  echo "RGW health check failed with error code: $retcode. the RGW likely cannot be reached by clients" >/dev/stderr
                  exit $retcode
                fi

                RGW_RATE_LIMITING_RESPONSE=503
                RGW_MISCONFIGURATION_RESPONSE=500

                if [[ $http_response -ge 200 ]] && [[ $http_response -lt 400 ]]; then
                  # 200-399 are successful responses. same behavior as Kubernetes' HTTP probe
                  exit 0

                elif [[ $http_response -eq $RGW_RATE_LIMITING_RESPONSE ]]; then
                  # S3's '503: slow down' code is not an error but an indication that RGW is throttling client
                  # traffic. failing the readiness check here would only cause an increase in client connections on
                  # other RGWs and likely cause those to fail also in a cascade. i.e., a special healthy response.
                  echo "INFO: RGW is rate limiting" 2>/dev/stderr
                  exit 0

                elif [[ $http_response -eq $RGW_MISCONFIGURATION_RESPONSE ]]; then
                  # can't specifically determine if the RGW is running or not. most likely a misconfiguration.
                  case "$PROBE_TYPE" in
                  "$STARTUP_TYPE")
                    # fail until we can accurately get a valid healthy response when runtime starts.
                    echo 'FAIL: HTTP code 500 suggests an RGW misconfiguration.' >/dev/stderr
                    exit $PROBE_ERR_CODE
                    ;;
                  "$READINESS_TYPE")
                    # config likely modified at runtime which could result in all RGWs failing this check.
                    # occasional client failures are still better than total failure, so ignore this
                    echo 'WARN: HTTP code 500 suggests an RGW misconfiguration' >/dev/stderr
                    exit 0
                    ;;
                  *)
                    # prior arg validation means this path should never be activated, but keep to be safe
                    echo "ERROR: probe type is unknown: $PROBE_TYPE" >/dev/stderr
                    exit $USAGE_ERR_CODE
                    ;;
                  esac

                else
                  # anything else is a failing response. same behavior as Kubernetes' HTTP probe
                  echo "FAIL: received an HTTP error code: $http_response"
                  exit $PROBE_ERR_CODE

                fi
            failureThreshold: 33
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-rgw-ceph-objectstore-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/rgw/ceph-ceph-objectstore
            name: ceph-daemon-data
          - mountPath: /etc/ceph/rgw
            name: rook-ceph-rgw-ceph-objectstore-mime-types
            readOnly: true
          workingDir: /var/log/ceph
        - command:
          - /bin/bash
          - -x
          - -e
          - -m
          - -c
          - "\nCEPH_CLIENT_ID=ceph-client.rgw.ceph.objectstore.a\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
            edit the logrotate file to only rotate a specific daemon log\n# otherwise
            we will logrotate log files without reloading certain daemons\n# this
            might happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
            \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
            --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
            rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed
            --in-place \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif
            [ \"$LOG_MAX_SIZE\" != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE
            at the 4th line of the logrotate config file with 4 spaces to maintain
            indentation\n\tsed --in-place \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\"
            \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile true; do\n\t# we don't force the
            logrorate but we let the logrotate binary handle the rotation based on
            user's input for periodicity and size\n\tlogrotate --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep
            15m\ndone\n"
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: log-collector
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/rgw/ceph-ceph-objectstore
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 256Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-rgw-ceph-objectstore-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/rgw/ceph-ceph-objectstore
            name: ceph-daemon-data
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-rgw
        serviceAccountName: rook-ceph-rgw
        shareProcessNamespace: true
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-rgw-ceph-objectstore-a-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-rgw-ceph-objectstore-a-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: ceph-daemon-data
        - configMap:
            defaultMode: 420
            name: rook-ceph-rgw-ceph-objectstore-mime-types
          name: rook-ceph-rgw-ceph-objectstore-mime-types
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-02-01T09:40:48Z"
      lastUpdateTime: "2024-02-01T09:40:48Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-02-01T09:40:48Z"
      lastUpdateTime: "2024-02-01T09:41:28Z"
      message: ReplicaSet "rook-ceph-rgw-ceph-objectstore-a-7cb6c484c4" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-02-01T03:54:45Z"
    generation: 1
    labels:
      app: rook-ceph-tools
    name: rook-ceph-tools
    namespace: rook-ceph
    resourceVersion: "13085045"
    uid: 220a73a0-4a54-47f2-ba1f-876f6fb99a4b
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-tools
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-tools
      spec:
        containers:
        - command:
          - /bin/bash
          - -c
          - |
            # Replicate the script from toolbox.sh inline so the ceph image
            # can be run directly, instead of requiring the rook toolbox
            CEPH_CONFIG="/etc/ceph/ceph.conf"
            MON_CONFIG="/etc/rook/mon-endpoints"
            KEYRING_FILE="/etc/ceph/keyring"

            # create a ceph config file in its default location so ceph/rados tools can be used
            # without specifying any arguments
            write_endpoints() {
              endpoints=$(cat ${MON_CONFIG})

              # filter out the mon names
              # external cluster can have numbers or hyphens in mon names, handling them in regex
              # shellcheck disable=SC2001
              mon_endpoints=$(echo "${endpoints}"| sed 's/[a-z0-9_-]\+=//g')

              DATE=$(date)
              echo "$DATE writing mon endpoints to ${CEPH_CONFIG}: ${endpoints}"
                cat <<EOF > ${CEPH_CONFIG}
            [global]
            mon_host = ${mon_endpoints}

            [client.admin]
            keyring = ${KEYRING_FILE}
            EOF
            }

            # watch the endpoints config file and update if the mon endpoints ever change
            watch_endpoints() {
              # get the timestamp for the target of the soft link
              real_path=$(realpath ${MON_CONFIG})
              initial_time=$(stat -c %Z "${real_path}")
              while true; do
                real_path=$(realpath ${MON_CONFIG})
                latest_time=$(stat -c %Z "${real_path}")

                if [[ "${latest_time}" != "${initial_time}" ]]; then
                  write_endpoints
                  initial_time=${latest_time}
                fi

                sleep 10
              done
            }

            # read the secret from an env var (for backward compatibility), or from the secret file
            ceph_secret=${ROOK_CEPH_SECRET}
            if [[ "$ceph_secret" == "" ]]; then
              ceph_secret=$(cat /var/lib/rook-ceph-mon/secret.keyring)
            fi

            # create the keyring file
            cat <<EOF > ${KEYRING_FILE}
            [${ROOK_CEPH_USERNAME}]
            key = ${ceph_secret}
            EOF

            # write the initial config file
            write_endpoints

            # continuously update the mon endpoints if they fail over
            watch_endpoints
          env:
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          image: quay.io/ceph/ceph:v18.2.1
          imagePullPolicy: IfNotPresent
          name: rook-ceph-tools
          resources: {}
          securityContext:
            capabilities:
              drop:
              - ALL
            runAsGroup: 2016
            runAsNonRoot: true
            runAsUser: 2016
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: ceph-config
          - mountPath: /etc/rook
            name: mon-endpoint-volume
          - mountPath: /var/lib/rook-ceph-mon
            name: ceph-admin-secret
            readOnly: true
        dnsPolicy: ClusterFirstWithHostNet
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: ceph-admin-secret
          secret:
            defaultMode: 420
            items:
            - key: ceph-secret
              path: secret.keyring
            optional: false
            secretName: rook-ceph-mon
        - configMap:
            defaultMode: 420
            items:
            - key: data
              path: mon-endpoints
            name: rook-ceph-mon-endpoints
          name: mon-endpoint-volume
        - emptyDir: {}
          name: ceph-config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-02-01T03:54:45Z"
      lastUpdateTime: "2024-02-01T03:54:47Z"
      message: ReplicaSet "rook-ceph-tools-6d6d694fb9" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-03-13T16:49:55Z"
      lastUpdateTime: "2024-03-13T16:49:55Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: tigera-operator
      meta.helm.sh/release-namespace: tigera-operator
    creationTimestamp: "2024-03-13T16:11:38Z"
    generation: 1
    labels:
      app.kubernetes.io/managed-by: Helm
      k8s-app: tigera-operator
    name: tigera-operator
    namespace: tigera-operator
    resourceVersion: "13072671"
    uid: f8bd4cac-b919-4c74-bfd5-c052a3e70e25
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        name: tigera-operator
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: tigera-operator
          name: tigera-operator
      spec:
        containers:
        - command:
          - operator
          env:
          - name: WATCH_NAMESPACE
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: OPERATOR_NAME
            value: tigera-operator
          - name: TIGERA_OPERATOR_INIT_IMAGE_VERSION
            value: master
          envFrom:
          - configMapRef:
              name: kubernetes-services-endpoint
              optional: true
          image: quay.io/tigera/operator:master
          imagePullPolicy: IfNotPresent
          name: tigera-operator
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/calico
            name: var-lib-calico
            readOnly: true
        dnsPolicy: ClusterFirstWithHostNet
        hostNetwork: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: tigera-operator
        serviceAccountName: tigera-operator
        terminationGracePeriodSeconds: 60
        tolerations:
        - effect: NoExecute
          operator: Exists
        - effect: NoSchedule
          operator: Exists
        volumes:
        - hostPath:
            path: /var/lib/calico
            type: ""
          name: var-lib-calico
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-03-13T16:11:49Z"
      lastUpdateTime: "2024-03-13T16:11:49Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-03-13T16:11:39Z"
      lastUpdateTime: "2024-03-13T16:11:49Z"
      message: ReplicaSet "tigera-operator-5cc9b4b697" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "20"
      deployment.kubernetes.io/revision-history: "18"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-07T11:04:51Z"
    generation: 4
    labels:
      app: airflow
      component: db-migrations
      pod-template-hash: 55ff7b548c
      release: airflow
    name: airflow-db-migrations-55ff7b548c
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-db-migrations
      uid: c39f7ef6-f9c4-4aea-a1b6-59e4ec4d8015
    resourceVersion: "12455903"
    uid: 34a2e138-6fff-47c7-90dd-ff1445950e0a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: db-migrations
        pod-template-hash: 55ff7b548c
        release: airflow
    template:
      metadata:
        annotations:
          checksum/db-migrations-script: 37898f38b90abd06081105d992362ec9e0d0015123b69e758e59031a9e6ddfc9
          checksum/secret-config-envs: 028d706c61a3c89e43897be1e60b3a0a619a3ebc5d272cd25b8ea2d8b7fe444b
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: db-migrations
          pod-template-hash: 55ff7b548c
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - python
          - -u
          - /mnt/scripts/db_migrations.py
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /mnt/scripts
            name: scripts
            readOnly: true
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: scripts
          secret:
            defaultMode: 420
            secretName: airflow-db-migrations
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "17"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-07T10:22:35Z"
    generation: 2
    labels:
      app: airflow
      component: db-migrations
      pod-template-hash: 594b5648d5
      release: airflow
    name: airflow-db-migrations-594b5648d5
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-db-migrations
      uid: c39f7ef6-f9c4-4aea-a1b6-59e4ec4d8015
    resourceVersion: "11331144"
    uid: d1975276-cb10-4af9-802c-dfe9fe728ee2
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: db-migrations
        pod-template-hash: 594b5648d5
        release: airflow
    template:
      metadata:
        annotations:
          checksum/db-migrations-script: 37898f38b90abd06081105d992362ec9e0d0015123b69e758e59031a9e6ddfc9
          checksum/secret-config-envs: 379fbef35e65b2d4b781e6b9b9675403a3757805cd7081c59e87fcfe69707a0a
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: db-migrations
          pod-template-hash: 594b5648d5
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - python
          - -u
          - /mnt/scripts/db_migrations.py
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /mnt/scripts
            name: scripts
            readOnly: true
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: scripts
          secret:
            defaultMode: 420
            secretName: airflow-db-migrations
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "15"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-06T14:18:22Z"
    generation: 2
    labels:
      app: airflow
      component: db-migrations
      pod-template-hash: 595c459d4f
      release: airflow
    name: airflow-db-migrations-595c459d4f
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-db-migrations
      uid: c39f7ef6-f9c4-4aea-a1b6-59e4ec4d8015
    resourceVersion: "11321232"
    uid: 9cf2c7f7-d133-4b88-9359-c4b7b66295b4
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: db-migrations
        pod-template-hash: 595c459d4f
        release: airflow
    template:
      metadata:
        annotations:
          checksum/db-migrations-script: 37898f38b90abd06081105d992362ec9e0d0015123b69e758e59031a9e6ddfc9
          checksum/secret-config-envs: 379fbef35e65b2d4b781e6b9b9675403a3757805cd7081c59e87fcfe69707a0a
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: db-migrations
          pod-template-hash: 595c459d4f
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - python
          - -u
          - /mnt/scripts/db_migrations.py
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
          - mountPath: /mnt/scripts
            name: scripts
            readOnly: true
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - bash
          - -c
          - |
            unset PYTHONUSERBASE && \
            pip freeze | grep -i -e "apache-airflow=="  > protected-packages.txt && \
            pip install --constraint ./protected-packages.txt --user "pymssql" "apache-airflow-providers-samba" "smbprotocol" "psycopg2-binary" "sqlalchemy" "pyodbc"  && \
            echo "copying '/home/airflow/.local/' to '/opt/home-airflow-local/.local/'..." && \
            rsync -ah --stats --delete /home/airflow/.local/ /opt/home-airflow-local/.local/
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: install-pip-packages
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/home-airflow-local
            name: home-airflow-local
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - emptyDir: {}
          name: home-airflow-local
        - name: scripts
          secret:
            defaultMode: 420
            secretName: airflow-db-migrations
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "21"
      deployment.kubernetes.io/revision-history: "19"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-11T11:22:14Z"
    generation: 4
    labels:
      app: airflow
      component: db-migrations
      pod-template-hash: 5b964cd58c
      release: airflow
    name: airflow-db-migrations-5b964cd58c
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-db-migrations
      uid: c39f7ef6-f9c4-4aea-a1b6-59e4ec4d8015
    resourceVersion: "12466678"
    uid: 65605b92-8875-46c4-8c29-b5faf1339e11
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: db-migrations
        pod-template-hash: 5b964cd58c
        release: airflow
    template:
      metadata:
        annotations:
          checksum/db-migrations-script: 37898f38b90abd06081105d992362ec9e0d0015123b69e758e59031a9e6ddfc9
          checksum/secret-config-envs: 028d706c61a3c89e43897be1e60b3a0a619a3ebc5d272cd25b8ea2d8b7fe444b
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: db-migrations
          pod-template-hash: 5b964cd58c
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - python
          - -u
          - /mnt/scripts/db_migrations.py
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /mnt/scripts
            name: scripts
            readOnly: true
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "true"
          - name: GIT_SSH_KNOWN_HOSTS_FILE
            value: /etc/git-secret/known_hosts
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
          - mountPath: /etc/git-secret/known_hosts
            name: git-known-hosts
            readOnly: true
            subPath: known_hosts
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "true"
          - name: GIT_SSH_KNOWN_HOSTS_FILE
            value: /etc/git-secret/known_hosts
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
          - mountPath: /etc/git-secret/known_hosts
            name: git-known-hosts
            readOnly: true
            subPath: known_hosts
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: git-known-hosts
          secret:
            defaultMode: 420
            secretName: airflow-known-hosts
        - name: scripts
          secret:
            defaultMode: 420
            secretName: airflow-db-migrations
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "13"
      deployment.kubernetes.io/revision-history: "11"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-04T11:40:42Z"
    generation: 4
    labels:
      app: airflow
      component: db-migrations
      pod-template-hash: "6695978489"
      release: airflow
    name: airflow-db-migrations-6695978489
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-db-migrations
      uid: c39f7ef6-f9c4-4aea-a1b6-59e4ec4d8015
    resourceVersion: "11075209"
    uid: b58bcf03-fdbd-4e2e-b16a-7ca43617bcf6
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: db-migrations
        pod-template-hash: "6695978489"
        release: airflow
    template:
      metadata:
        annotations:
          checksum/db-migrations-script: 37898f38b90abd06081105d992362ec9e0d0015123b69e758e59031a9e6ddfc9
          checksum/secret-config-envs: fe44ec2fcd31f346e7df5c06847f85e9f2fe78b8c87a596e234ca473034f72eb
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: db-migrations
          pod-template-hash: "6695978489"
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - python
          - -u
          - /mnt/scripts/db_migrations.py
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
          - mountPath: /mnt/scripts
            name: scripts
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - bash
          - -c
          - |
            unset PYTHONUSERBASE && \
            pip freeze | grep -i -e "apache-airflow=="  > protected-packages.txt && \
            pip install --constraint ./protected-packages.txt --user "pymssql" "apache-airflow-providers-samba" "smbprotocol" "psycopg2-binary" "sqlalchemy" "pyodbc"  && \
            echo "copying '/home/airflow/.local/' to '/opt/home-airflow-local/.local/'..." && \
            rsync -ah --stats --delete /home/airflow/.local/ /opt/home-airflow-local/.local/
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: install-pip-packages
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/home-airflow-local
            name: home-airflow-local
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - name: dags-data
          persistentVolumeClaim:
            claimName: airflow-dags
        - emptyDir: {}
          name: logs-data
        - emptyDir: {}
          name: home-airflow-local
        - name: scripts
          secret:
            defaultMode: 420
            secretName: airflow-db-migrations
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "16"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-07T10:17:05Z"
    generation: 2
    labels:
      app: airflow
      component: db-migrations
      pod-template-hash: "6869464597"
      release: airflow
    name: airflow-db-migrations-6869464597
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-db-migrations
      uid: c39f7ef6-f9c4-4aea-a1b6-59e4ec4d8015
    resourceVersion: "11322612"
    uid: d66e28f2-8783-42b6-8b7f-4b4fa476440a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: db-migrations
        pod-template-hash: "6869464597"
        release: airflow
    template:
      metadata:
        annotations:
          checksum/db-migrations-script: 37898f38b90abd06081105d992362ec9e0d0015123b69e758e59031a9e6ddfc9
          checksum/secret-config-envs: 379fbef35e65b2d4b781e6b9b9675403a3757805cd7081c59e87fcfe69707a0a
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: db-migrations
          pod-template-hash: "6869464597"
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - python
          - -u
          - /mnt/scripts/db_migrations.py
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /mnt/scripts
            name: scripts
            readOnly: true
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "true"
          - name: GIT_SSH_KNOWN_HOSTS_FILE
            value: /etc/git-secret/known_hosts
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
          - mountPath: /etc/git-secret/known_hosts
            name: git-known-hosts
            readOnly: true
            subPath: known_hosts
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "true"
          - name: GIT_SSH_KNOWN_HOSTS_FILE
            value: /etc/git-secret/known_hosts
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
          - mountPath: /etc/git-secret/known_hosts
            name: git-known-hosts
            readOnly: true
            subPath: known_hosts
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: git-known-hosts
          secret:
            defaultMode: 420
            secretName: airflow-known-hosts
        - name: scripts
          secret:
            defaultMode: 420
            secretName: airflow-db-migrations
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "24"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-14T04:56:26Z"
    generation: 2
    labels:
      app: airflow
      component: db-migrations
      pod-template-hash: 757f794c7
      release: airflow
    name: airflow-db-migrations-757f794c7
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-db-migrations
      uid: c39f7ef6-f9c4-4aea-a1b6-59e4ec4d8015
    resourceVersion: "13231220"
    uid: 48816531-d905-44a6-a1e9-7e8f82d3d1d7
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: db-migrations
        pod-template-hash: 757f794c7
        release: airflow
    template:
      metadata:
        annotations:
          checksum/db-migrations-script: 37898f38b90abd06081105d992362ec9e0d0015123b69e758e59031a9e6ddfc9
          checksum/secret-config-envs: 5928424b76843c48d9e503eecfd8541d5af836b104787689f14c26baf7dee423
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: db-migrations
          pod-template-hash: 757f794c7
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - python
          - -u
          - /mnt/scripts/db_migrations.py
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.10
          imagePullPolicy: IfNotPresent
          name: db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /mnt/scripts
            name: scripts
            readOnly: true
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.10
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-logs
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: scripts
          secret:
            defaultMode: 420
            secretName: airflow-db-migrations
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "25"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-14T05:12:14Z"
    generation: 1
    labels:
      app: airflow
      component: db-migrations
      pod-template-hash: 77b99788cf
      release: airflow
    name: airflow-db-migrations-77b99788cf
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-db-migrations
      uid: c39f7ef6-f9c4-4aea-a1b6-59e4ec4d8015
    resourceVersion: "13231686"
    uid: f80ef275-9f6f-402f-8664-6b360abb2498
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: airflow
        component: db-migrations
        pod-template-hash: 77b99788cf
        release: airflow
    template:
      metadata:
        annotations:
          checksum/db-migrations-script: 37898f38b90abd06081105d992362ec9e0d0015123b69e758e59031a9e6ddfc9
          checksum/secret-config-envs: 7b5b4b568e7c694c0f44651459436d8876c0d6f337d3876162036cffceec3b68
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: db-migrations
          pod-template-hash: 77b99788cf
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - python
          - -u
          - /mnt/scripts/db_migrations.py
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.11
          imagePullPolicy: IfNotPresent
          name: db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /mnt/scripts
            name: scripts
            readOnly: true
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.11
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-logs
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: scripts
          secret:
            defaultMode: 420
            secretName: airflow-db-migrations
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "23"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-14T04:28:36Z"
    generation: 2
    labels:
      app: airflow
      component: db-migrations
      pod-template-hash: 7cf6c55664
      release: airflow
    name: airflow-db-migrations-7cf6c55664
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-db-migrations
      uid: c39f7ef6-f9c4-4aea-a1b6-59e4ec4d8015
    resourceVersion: "13227754"
    uid: 6131fd8b-f9c9-4a80-a7df-7ad25e58e566
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: db-migrations
        pod-template-hash: 7cf6c55664
        release: airflow
    template:
      metadata:
        annotations:
          checksum/db-migrations-script: 37898f38b90abd06081105d992362ec9e0d0015123b69e758e59031a9e6ddfc9
          checksum/secret-config-envs: 379fbef35e65b2d4b781e6b9b9675403a3757805cd7081c59e87fcfe69707a0a
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: db-migrations
          pod-template-hash: 7cf6c55664
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - python
          - -u
          - /mnt/scripts/db_migrations.py
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /mnt/scripts
            name: scripts
            readOnly: true
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-logs
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: scripts
          secret:
            defaultMode: 420
            secretName: airflow-db-migrations
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "22"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-11T12:50:43Z"
    generation: 2
    labels:
      app: airflow
      component: db-migrations
      pod-template-hash: "854999494"
      release: airflow
    name: airflow-db-migrations-854999494
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-db-migrations
      uid: c39f7ef6-f9c4-4aea-a1b6-59e4ec4d8015
    resourceVersion: "13222018"
    uid: e5271e15-22bb-4147-bddf-f7864978a68e
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: db-migrations
        pod-template-hash: "854999494"
        release: airflow
    template:
      metadata:
        annotations:
          checksum/db-migrations-script: 37898f38b90abd06081105d992362ec9e0d0015123b69e758e59031a9e6ddfc9
          checksum/secret-config-envs: 028d706c61a3c89e43897be1e60b3a0a619a3ebc5d272cd25b8ea2d8b7fe444b
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: db-migrations
          pod-template-hash: "854999494"
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - python
          - -u
          - /mnt/scripts/db_migrations.py
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /mnt/scripts
            name: scripts
            readOnly: true
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-logs
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: scripts
          secret:
            defaultMode: 420
            secretName: airflow-db-migrations
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "14"
      deployment.kubernetes.io/revision-history: "12"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-06T12:59:53Z"
    generation: 4
    labels:
      app: airflow
      component: db-migrations
      pod-template-hash: b9b67f987
      release: airflow
    name: airflow-db-migrations-b9b67f987
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-db-migrations
      uid: c39f7ef6-f9c4-4aea-a1b6-59e4ec4d8015
    resourceVersion: "11088593"
    uid: e4cef822-e99f-42bf-8dbf-9653423357e8
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: db-migrations
        pod-template-hash: b9b67f987
        release: airflow
    template:
      metadata:
        annotations:
          checksum/db-migrations-script: 37898f38b90abd06081105d992362ec9e0d0015123b69e758e59031a9e6ddfc9
          checksum/secret-config-envs: 379fbef35e65b2d4b781e6b9b9675403a3757805cd7081c59e87fcfe69707a0a
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: db-migrations
          pod-template-hash: b9b67f987
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - python
          - -u
          - /mnt/scripts/db_migrations.py
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
          - mountPath: /mnt/scripts
            name: scripts
            readOnly: true
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: id_rsa_base64
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - bash
          - -c
          - |
            unset PYTHONUSERBASE && \
            pip freeze | grep -i -e "apache-airflow=="  > protected-packages.txt && \
            pip install --constraint ./protected-packages.txt --user "pymssql" "apache-airflow-providers-samba" "smbprotocol" "psycopg2-binary" "sqlalchemy" "pyodbc"  && \
            echo "copying '/home/airflow/.local/' to '/opt/home-airflow-local/.local/'..." && \
            rsync -ah --stats --delete /home/airflow/.local/ /opt/home-airflow-local/.local/
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: install-pip-packages
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/home-airflow-local
            name: home-airflow-local
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: id_rsa_base64
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - emptyDir: {}
          name: home-airflow-local
        - name: scripts
          secret:
            defaultMode: 420
            secretName: airflow-db-migrations
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "17"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-14T05:12:12Z"
    generation: 1
    labels:
      app: airflow
      component: pgbouncer
      pod-template-hash: 55f8877f8c
      release: airflow
    name: airflow-pgbouncer-55f8877f8c
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-pgbouncer
      uid: c1b2bef6-58f7-4144-b0ff-e1c0c351766d
    resourceVersion: "13231428"
    uid: b0e83978-4a5b-47ec-818a-345bc9dd8911
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: airflow
        component: pgbouncer
        pod-template-hash: 55f8877f8c
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 7b5b4b568e7c694c0f44651459436d8876c0d6f337d3876162036cffceec3b68
          checksum/secret-pgbouncer: cbb28505a66ba6d70bfda4f3495790f613816b1e9ac5761a955018cdb67ad44e
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: pgbouncer
          pod-template-hash: 55f8877f8c
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - /bin/sh
          - -c
          - |-
            /home/pgbouncer/config/gen_self_signed_cert.sh && \
            /home/pgbouncer/config/gen_auth_file.sh && \
            exec pgbouncer /home/pgbouncer/config/pgbouncer.ini
          command:
          - /usr/bin/dumb-init
          - --rewrite=15:2
          - --
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: ghcr.io/airflow-helm/pgbouncer:1.18.0-patch.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - psql $(eval $DATABASE_PSQL_CMD) --tuples-only --command="SELECT 1;"
                | grep -q "1"
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: pgbouncer
          ports:
          - containerPort: 6432
            name: pgbouncer
            protocol: TCP
          resources: {}
          securityContext:
            runAsGroup: 1001
            runAsUser: 1001
          startupProbe:
            failureThreshold: 30
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 6432
            timeoutSeconds: 15
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /home/pgbouncer/config
            name: pgbouncer-config
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 120
        volumes:
        - name: pgbouncer-config
          secret:
            defaultMode: 420
            items:
            - key: gen_auth_file.sh
              mode: 493
              path: gen_auth_file.sh
            - key: gen_self_signed_cert.sh
              mode: 493
              path: gen_self_signed_cert.sh
            - key: pgbouncer.ini
              path: pgbouncer.ini
            secretName: airflow-pgbouncer
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "16"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-14T04:56:23Z"
    generation: 2
    labels:
      app: airflow
      component: pgbouncer
      pod-template-hash: 56d98c948f
      release: airflow
    name: airflow-pgbouncer-56d98c948f
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-pgbouncer
      uid: c1b2bef6-58f7-4144-b0ff-e1c0c351766d
    resourceVersion: "13231438"
    uid: e2b8b198-b5b2-4665-b1a0-0c9a708d3666
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: pgbouncer
        pod-template-hash: 56d98c948f
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 5928424b76843c48d9e503eecfd8541d5af836b104787689f14c26baf7dee423
          checksum/secret-pgbouncer: cbb28505a66ba6d70bfda4f3495790f613816b1e9ac5761a955018cdb67ad44e
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: pgbouncer
          pod-template-hash: 56d98c948f
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - /bin/sh
          - -c
          - |-
            /home/pgbouncer/config/gen_self_signed_cert.sh && \
            /home/pgbouncer/config/gen_auth_file.sh && \
            exec pgbouncer /home/pgbouncer/config/pgbouncer.ini
          command:
          - /usr/bin/dumb-init
          - --rewrite=15:2
          - --
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: ghcr.io/airflow-helm/pgbouncer:1.18.0-patch.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - psql $(eval $DATABASE_PSQL_CMD) --tuples-only --command="SELECT 1;"
                | grep -q "1"
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: pgbouncer
          ports:
          - containerPort: 6432
            name: pgbouncer
            protocol: TCP
          resources: {}
          securityContext:
            runAsGroup: 1001
            runAsUser: 1001
          startupProbe:
            failureThreshold: 30
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 6432
            timeoutSeconds: 15
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /home/pgbouncer/config
            name: pgbouncer-config
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 120
        volumes:
        - name: pgbouncer-config
          secret:
            defaultMode: 420
            items:
            - key: gen_auth_file.sh
              mode: 493
              path: gen_auth_file.sh
            - key: gen_self_signed_cert.sh
              mode: 493
              path: gen_self_signed_cert.sh
            - key: pgbouncer.ini
              path: pgbouncer.ini
            secretName: airflow-pgbouncer
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "6"
      deployment.kubernetes.io/revision-history: "4"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-02-15T07:39:14Z"
    generation: 4
    labels:
      app: airflow
      component: pgbouncer
      pod-template-hash: 574bf69fb9
      release: airflow
    name: airflow-pgbouncer-574bf69fb9
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-pgbouncer
      uid: c1b2bef6-58f7-4144-b0ff-e1c0c351766d
    resourceVersion: "6533897"
    uid: 45f159c0-a347-468f-8b85-2bccd5a09998
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: pgbouncer
        pod-template-hash: 574bf69fb9
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: b89ce61779106b2a491d7f47d15661dc1ca879ca45aae85421bd5e222fd5ac45
          checksum/secret-pgbouncer: cbb28505a66ba6d70bfda4f3495790f613816b1e9ac5761a955018cdb67ad44e
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: pgbouncer
          pod-template-hash: 574bf69fb9
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - /bin/sh
          - -c
          - |-
            /home/pgbouncer/config/gen_self_signed_cert.sh && \
            /home/pgbouncer/config/gen_auth_file.sh && \
            exec pgbouncer /home/pgbouncer/config/pgbouncer.ini
          command:
          - /usr/bin/dumb-init
          - --rewrite=15:2
          - --
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: ghcr.io/airflow-helm/pgbouncer:1.18.0-patch.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - psql $(eval $DATABASE_PSQL_CMD) --tuples-only --command="SELECT 1;"
                | grep -q "1"
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: pgbouncer
          ports:
          - containerPort: 6432
            name: pgbouncer
            protocol: TCP
          resources: {}
          securityContext:
            runAsGroup: 1001
            runAsUser: 1001
          startupProbe:
            failureThreshold: 30
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 6432
            timeoutSeconds: 15
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /home/pgbouncer/config
            name: pgbouncer-config
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 120
        volumes:
        - name: pgbouncer-config
          secret:
            defaultMode: 420
            items:
            - key: gen_auth_file.sh
              mode: 493
              path: gen_auth_file.sh
            - key: gen_self_signed_cert.sh
              mode: 493
              path: gen_self_signed_cert.sh
            - key: pgbouncer.ini
              path: pgbouncer.ini
            secretName: airflow-pgbouncer
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-02-15T04:15:49Z"
    generation: 2
    labels:
      app: airflow
      component: pgbouncer
      pod-template-hash: 5cfc64c9bc
      release: airflow
    name: airflow-pgbouncer-5cfc64c9bc
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-pgbouncer
      uid: c1b2bef6-58f7-4144-b0ff-e1c0c351766d
    resourceVersion: "5099786"
    uid: 554163cd-65bc-4aa7-a5c7-636a65d62292
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: pgbouncer
        pod-template-hash: 5cfc64c9bc
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: b89ce61779106b2a491d7f47d15661dc1ca879ca45aae85421bd5e222fd5ac45
          checksum/secret-pgbouncer: cbb28505a66ba6d70bfda4f3495790f613816b1e9ac5761a955018cdb67ad44e
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: pgbouncer
          pod-template-hash: 5cfc64c9bc
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - /bin/sh
          - -c
          - |-
            /home/pgbouncer/config/gen_self_signed_cert.sh && \
            /home/pgbouncer/config/gen_auth_file.sh && \
            exec pgbouncer /home/pgbouncer/config/pgbouncer.ini
          command:
          - /usr/bin/dumb-init
          - --rewrite=15:2
          - --
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__CORE__STATSD_HOST
            value: 192.168.133.225
          - name: AIRFLOW__CORE__STATSD_PORT
            value: "9125"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: ghcr.io/airflow-helm/pgbouncer:1.18.0-patch.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - psql $(eval $DATABASE_PSQL_CMD) --tuples-only --command="SELECT 1;"
                | grep -q "1"
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: pgbouncer
          ports:
          - containerPort: 6432
            name: pgbouncer
            protocol: TCP
          resources: {}
          securityContext:
            runAsGroup: 1001
            runAsUser: 1001
          startupProbe:
            failureThreshold: 30
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 6432
            timeoutSeconds: 15
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /home/pgbouncer/config
            name: pgbouncer-config
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 120
        volumes:
        - name: pgbouncer-config
          secret:
            defaultMode: 420
            items:
            - key: gen_auth_file.sh
              mode: 493
              path: gen_auth_file.sh
            - key: gen_self_signed_cert.sh
              mode: 493
              path: gen_self_signed_cert.sh
            - key: pgbouncer.ini
              path: pgbouncer.ini
            secretName: airflow-pgbouncer
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "14"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-07T11:04:50Z"
    generation: 2
    labels:
      app: airflow
      component: pgbouncer
      pod-template-hash: 66dfc749cd
      release: airflow
    name: airflow-pgbouncer-66dfc749cd
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-pgbouncer
      uid: c1b2bef6-58f7-4144-b0ff-e1c0c351766d
    resourceVersion: "13222174"
    uid: 5973d6d5-2d07-46c6-8c98-ad9d3fb491d7
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: pgbouncer
        pod-template-hash: 66dfc749cd
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 028d706c61a3c89e43897be1e60b3a0a619a3ebc5d272cd25b8ea2d8b7fe444b
          checksum/secret-pgbouncer: cbb28505a66ba6d70bfda4f3495790f613816b1e9ac5761a955018cdb67ad44e
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: pgbouncer
          pod-template-hash: 66dfc749cd
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - /bin/sh
          - -c
          - |-
            /home/pgbouncer/config/gen_self_signed_cert.sh && \
            /home/pgbouncer/config/gen_auth_file.sh && \
            exec pgbouncer /home/pgbouncer/config/pgbouncer.ini
          command:
          - /usr/bin/dumb-init
          - --rewrite=15:2
          - --
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: ghcr.io/airflow-helm/pgbouncer:1.18.0-patch.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - psql $(eval $DATABASE_PSQL_CMD) --tuples-only --command="SELECT 1;"
                | grep -q "1"
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: pgbouncer
          ports:
          - containerPort: 6432
            name: pgbouncer
            protocol: TCP
          resources: {}
          securityContext:
            runAsGroup: 1001
            runAsUser: 1001
          startupProbe:
            failureThreshold: 30
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 6432
            timeoutSeconds: 15
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /home/pgbouncer/config
            name: pgbouncer-config
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 120
        volumes:
        - name: pgbouncer-config
          secret:
            defaultMode: 420
            items:
            - key: gen_auth_file.sh
              mode: 493
              path: gen_auth_file.sh
            - key: gen_self_signed_cert.sh
              mode: 493
              path: gen_self_signed_cert.sh
            - key: pgbouncer.ini
              path: pgbouncer.ini
            secretName: airflow-pgbouncer
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "15"
      deployment.kubernetes.io/revision-history: 11,13
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-06T12:59:46Z"
    generation: 6
    labels:
      app: airflow
      component: pgbouncer
      pod-template-hash: 697fdffb68
      release: airflow
    name: airflow-pgbouncer-697fdffb68
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-pgbouncer
      uid: c1b2bef6-58f7-4144-b0ff-e1c0c351766d
    resourceVersion: "13227964"
    uid: 73776569-3e2b-46bc-a7b9-4f3c67730afa
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: pgbouncer
        pod-template-hash: 697fdffb68
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 379fbef35e65b2d4b781e6b9b9675403a3757805cd7081c59e87fcfe69707a0a
          checksum/secret-pgbouncer: cbb28505a66ba6d70bfda4f3495790f613816b1e9ac5761a955018cdb67ad44e
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: pgbouncer
          pod-template-hash: 697fdffb68
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - /bin/sh
          - -c
          - |-
            /home/pgbouncer/config/gen_self_signed_cert.sh && \
            /home/pgbouncer/config/gen_auth_file.sh && \
            exec pgbouncer /home/pgbouncer/config/pgbouncer.ini
          command:
          - /usr/bin/dumb-init
          - --rewrite=15:2
          - --
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: ghcr.io/airflow-helm/pgbouncer:1.18.0-patch.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - psql $(eval $DATABASE_PSQL_CMD) --tuples-only --command="SELECT 1;"
                | grep -q "1"
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: pgbouncer
          ports:
          - containerPort: 6432
            name: pgbouncer
            protocol: TCP
          resources: {}
          securityContext:
            runAsGroup: 1001
            runAsUser: 1001
          startupProbe:
            failureThreshold: 30
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 6432
            timeoutSeconds: 15
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /home/pgbouncer/config
            name: pgbouncer-config
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 120
        volumes:
        - name: pgbouncer-config
          secret:
            defaultMode: 420
            items:
            - key: gen_auth_file.sh
              mode: 493
              path: gen_auth_file.sh
            - key: gen_self_signed_cert.sh
              mode: 493
              path: gen_self_signed_cert.sh
            - key: pgbouncer.ini
              path: pgbouncer.ini
            secretName: airflow-pgbouncer
  status:
    observedGeneration: 6
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-02-13T12:16:39Z"
    generation: 2
    labels:
      app: airflow
      component: pgbouncer
      pod-template-hash: 6c69967f84
      release: airflow
    name: airflow-pgbouncer-6c69967f84
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-pgbouncer
      uid: c1b2bef6-58f7-4144-b0ff-e1c0c351766d
    resourceVersion: "5061394"
    uid: 98111ce5-3510-48a8-aea5-0c8758de897a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: pgbouncer
        pod-template-hash: 6c69967f84
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: b89ce61779106b2a491d7f47d15661dc1ca879ca45aae85421bd5e222fd5ac45
          checksum/secret-pgbouncer: cbb28505a66ba6d70bfda4f3495790f613816b1e9ac5761a955018cdb67ad44e
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: pgbouncer
          pod-template-hash: 6c69967f84
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - /bin/sh
          - -c
          - |-
            /home/pgbouncer/config/gen_self_signed_cert.sh && \
            /home/pgbouncer/config/gen_auth_file.sh && \
            exec pgbouncer /home/pgbouncer/config/pgbouncer.ini
          command:
          - /usr/bin/dumb-init
          - --rewrite=15:2
          - --
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: ghcr.io/airflow-helm/pgbouncer:1.18.0-patch.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - psql $(eval $DATABASE_PSQL_CMD) --tuples-only --command="SELECT 1;"
                | grep -q "1"
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: pgbouncer
          ports:
          - containerPort: 6432
            name: pgbouncer
            protocol: TCP
          resources: {}
          securityContext:
            runAsGroup: 1001
            runAsUser: 1001
          startupProbe:
            failureThreshold: 30
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 6432
            timeoutSeconds: 15
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /home/pgbouncer/config
            name: pgbouncer-config
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 120
        volumes:
        - name: pgbouncer-config
          secret:
            defaultMode: 420
            items:
            - key: gen_auth_file.sh
              mode: 493
              path: gen_auth_file.sh
            - key: gen_self_signed_cert.sh
              mode: 493
              path: gen_self_signed_cert.sh
            - key: pgbouncer.ini
              path: pgbouncer.ini
            secretName: airflow-pgbouncer
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "9"
      deployment.kubernetes.io/revision-history: 5,7
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-02-20T05:52:51Z"
    generation: 6
    labels:
      app: airflow
      component: pgbouncer
      pod-template-hash: 74bf7d7bcc
      release: airflow
    name: airflow-pgbouncer-74bf7d7bcc
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-pgbouncer
      uid: c1b2bef6-58f7-4144-b0ff-e1c0c351766d
    resourceVersion: "10498921"
    uid: 91426378-99ad-4cf0-9369-89405eea6cf2
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: pgbouncer
        pod-template-hash: 74bf7d7bcc
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 6ef6fea622064a9de5494d4bafe82abc4b787abaabb53af0fde9baf57abc9be9
          checksum/secret-pgbouncer: cbb28505a66ba6d70bfda4f3495790f613816b1e9ac5761a955018cdb67ad44e
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: pgbouncer
          pod-template-hash: 74bf7d7bcc
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - /bin/sh
          - -c
          - |-
            /home/pgbouncer/config/gen_self_signed_cert.sh && \
            /home/pgbouncer/config/gen_auth_file.sh && \
            exec pgbouncer /home/pgbouncer/config/pgbouncer.ini
          command:
          - /usr/bin/dumb-init
          - --rewrite=15:2
          - --
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: ghcr.io/airflow-helm/pgbouncer:1.18.0-patch.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - psql $(eval $DATABASE_PSQL_CMD) --tuples-only --command="SELECT 1;"
                | grep -q "1"
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: pgbouncer
          ports:
          - containerPort: 6432
            name: pgbouncer
            protocol: TCP
          resources: {}
          securityContext:
            runAsGroup: 1001
            runAsUser: 1001
          startupProbe:
            failureThreshold: 30
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 6432
            timeoutSeconds: 15
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /home/pgbouncer/config
            name: pgbouncer-config
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 120
        volumes:
        - name: pgbouncer-config
          secret:
            defaultMode: 420
            items:
            - key: gen_auth_file.sh
              mode: 493
              path: gen_auth_file.sh
            - key: gen_self_signed_cert.sh
              mode: 493
              path: gen_self_signed_cert.sh
            - key: pgbouncer.ini
              path: pgbouncer.ini
            secretName: airflow-pgbouncer
  status:
    observedGeneration: 6
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "8"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-02-29T05:19:30Z"
    generation: 2
    labels:
      app: airflow
      component: pgbouncer
      pod-template-hash: 78879bc5ff
      release: airflow
    name: airflow-pgbouncer-78879bc5ff
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-pgbouncer
      uid: c1b2bef6-58f7-4144-b0ff-e1c0c351766d
    resourceVersion: "9332056"
    uid: 0effc1ab-384e-44eb-8c3e-f0ca926e1cbc
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: pgbouncer
        pod-template-hash: 78879bc5ff
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 60abb4088edbed3f0b95f51e1393e8de7aa5d285d12037e12171540ae91cf0aa
          checksum/secret-pgbouncer: cbb28505a66ba6d70bfda4f3495790f613816b1e9ac5761a955018cdb67ad44e
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: pgbouncer
          pod-template-hash: 78879bc5ff
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - /bin/sh
          - -c
          - |-
            /home/pgbouncer/config/gen_self_signed_cert.sh && \
            /home/pgbouncer/config/gen_auth_file.sh && \
            exec pgbouncer /home/pgbouncer/config/pgbouncer.ini
          command:
          - /usr/bin/dumb-init
          - --rewrite=15:2
          - --
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: ghcr.io/airflow-helm/pgbouncer:1.18.0-patch.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - psql $(eval $DATABASE_PSQL_CMD) --tuples-only --command="SELECT 1;"
                | grep -q "1"
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: pgbouncer
          ports:
          - containerPort: 6432
            name: pgbouncer
            protocol: TCP
          resources: {}
          securityContext:
            runAsGroup: 1001
            runAsUser: 1001
          startupProbe:
            failureThreshold: 30
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 6432
            timeoutSeconds: 15
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /home/pgbouncer/config
            name: pgbouncer-config
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 120
        volumes:
        - name: pgbouncer-config
          secret:
            defaultMode: 420
            items:
            - key: gen_auth_file.sh
              mode: 493
              path: gen_auth_file.sh
            - key: gen_self_signed_cert.sh
              mode: 493
              path: gen_self_signed_cert.sh
            - key: pgbouncer.ini
              path: pgbouncer.ini
            secretName: airflow-pgbouncer
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-02-13T10:42:14Z"
    generation: 2
    labels:
      app: airflow
      component: pgbouncer
      pod-template-hash: "966599578"
      release: airflow
    name: airflow-pgbouncer-966599578
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-pgbouncer
      uid: c1b2bef6-58f7-4144-b0ff-e1c0c351766d
    resourceVersion: "4581362"
    uid: 04a5799b-167b-46ab-b4d0-7ca3124b7446
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: pgbouncer
        pod-template-hash: "966599578"
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: bf104e80c3a1f8d88896f47964ab03b1bf6e987b01eb6250fc97b0d24e2f50cd
          checksum/secret-pgbouncer: 3cdbefd68c69d1eac6054e35bc92454f3df0e6c7619fb6e62f1f83a918d9da2b
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: pgbouncer
          pod-template-hash: "966599578"
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - /bin/sh
          - -c
          - |-
            /home/pgbouncer/config/gen_self_signed_cert.sh && \
            /home/pgbouncer/config/gen_auth_file.sh && \
            exec pgbouncer /home/pgbouncer/config/pgbouncer.ini
          command:
          - /usr/bin/dumb-init
          - --rewrite=15:2
          - --
          env:
          - name: DATABASE_USER
            valueFrom:
              secretKeyRef:
                key: username
                name: postgres-existing-cred
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgres-existing-cred
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: ghcr.io/airflow-helm/pgbouncer:1.18.0-patch.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - psql $(eval $DATABASE_PSQL_CMD) --tuples-only --command="SELECT 1;"
                | grep -q "1"
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: pgbouncer
          ports:
          - containerPort: 6432
            name: pgbouncer
            protocol: TCP
          resources: {}
          securityContext:
            runAsGroup: 1001
            runAsUser: 1001
          startupProbe:
            failureThreshold: 30
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 6432
            timeoutSeconds: 15
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /home/pgbouncer/config
            name: pgbouncer-config
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 120
        volumes:
        - name: pgbouncer-config
          secret:
            defaultMode: 420
            items:
            - key: gen_auth_file.sh
              mode: 493
              path: gen_auth_file.sh
            - key: gen_self_signed_cert.sh
              mode: 493
              path: gen_self_signed_cert.sh
            - key: pgbouncer.ini
              path: pgbouncer.ini
            secretName: airflow-pgbouncer
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "12"
      deployment.kubernetes.io/revision-history: "10"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-04T11:40:35Z"
    generation: 4
    labels:
      app: airflow
      component: pgbouncer
      pod-template-hash: fdf8b888d
      release: airflow
    name: airflow-pgbouncer-fdf8b888d
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-pgbouncer
      uid: c1b2bef6-58f7-4144-b0ff-e1c0c351766d
    resourceVersion: "11075383"
    uid: 997ea2a2-091b-4bb3-8d5e-445fb75282b6
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: pgbouncer
        pod-template-hash: fdf8b888d
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: fe44ec2fcd31f346e7df5c06847f85e9f2fe78b8c87a596e234ca473034f72eb
          checksum/secret-pgbouncer: cbb28505a66ba6d70bfda4f3495790f613816b1e9ac5761a955018cdb67ad44e
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: pgbouncer
          pod-template-hash: fdf8b888d
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - /bin/sh
          - -c
          - |-
            /home/pgbouncer/config/gen_self_signed_cert.sh && \
            /home/pgbouncer/config/gen_auth_file.sh && \
            exec pgbouncer /home/pgbouncer/config/pgbouncer.ini
          command:
          - /usr/bin/dumb-init
          - --rewrite=15:2
          - --
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: ghcr.io/airflow-helm/pgbouncer:1.18.0-patch.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - psql $(eval $DATABASE_PSQL_CMD) --tuples-only --command="SELECT 1;"
                | grep -q "1"
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: pgbouncer
          ports:
          - containerPort: 6432
            name: pgbouncer
            protocol: TCP
          resources: {}
          securityContext:
            runAsGroup: 1001
            runAsUser: 1001
          startupProbe:
            failureThreshold: 30
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 6432
            timeoutSeconds: 15
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /home/pgbouncer/config
            name: pgbouncer-config
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 120
        volumes:
        - name: pgbouncer-config
          secret:
            defaultMode: 420
            items:
            - key: gen_auth_file.sh
              mode: 493
              path: gen_auth_file.sh
            - key: gen_self_signed_cert.sh
              mode: 493
              path: gen_self_signed_cert.sh
            - key: pgbouncer.ini
              path: pgbouncer.ini
            secretName: airflow-pgbouncer
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "25"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-14T05:12:12Z"
    generation: 1
    labels:
      app: airflow
      component: scheduler
      pod-template-hash: 54bf954c6d
      release: airflow
    name: airflow-scheduler-54bf954c6d
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-scheduler
      uid: e92ea920-8ca8-4806-ad4e-ff5a9af671c1
    resourceVersion: "13231812"
    uid: 203f54e9-1bc1-4a69-b996-4fb12840c8dd
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: airflow
        component: scheduler
        pod-template-hash: 54bf954c6d
        release: airflow
    template:
      metadata:
        annotations:
          checksum/config-pod-template: 1a8683c073a2d13ce4ef577bf3256def776cd1e7666f47cc1d08d00f446fe1c9
          checksum/secret-config-envs: 7b5b4b568e7c694c0f44651459436d8876c0d6f337d3876162036cffceec3b68
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: scheduler
          pod-template-hash: 54bf954c6d
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow scheduler -n -1
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/dumb-init
              - --
              - /entrypoint
              - python
              - -Wignore
              - -c
              - |
                import os
                import sys

                # suppress logs triggered from importing airflow packages
                os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                # shared imports
                try:
                    from airflow.jobs.job import Job
                except ImportError:
                    # `BaseJob` was renamed to `Job` in airflow 2.6.0
                    from airflow.jobs.base_job import BaseJob as Job
                from airflow.utils.db import create_session
                from airflow.utils.net import get_hostname

                # heartbeat check imports
                try:
                    from airflow.jobs.scheduler_job_runner import SchedulerJobRunner
                except ImportError:
                    # `SchedulerJob` is wrapped by `SchedulerJobRunner` since airflow 2.6.0
                    from airflow.jobs.scheduler_job import SchedulerJob as SchedulerJobRunner

                with create_session() as session:
                    ########################
                    # heartbeat check
                    ########################
                    # ensure the SchedulerJob with most recent heartbeat for this `hostname` is alive
                    hostname = get_hostname()
                    scheduler_job = session \
                        .query(Job) \
                        .filter_by(job_type=SchedulerJobRunner.job_type) \
                        .filter_by(hostname=hostname) \
                        .order_by(Job.latest_heartbeat.desc()) \
                        .limit(1) \
                        .first()
                    if (scheduler_job is not None) and scheduler_job.is_alive():
                        pass
                    else:
                        sys.exit(f"The SchedulerJob (id={scheduler_job.id}) for hostname '{hostname}' is not alive")
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: airflow-scheduler
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /opt/airflow/pod_templates/pod_template.yaml
            name: pod-template
            readOnly: true
            subPath: pod_template.yaml
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.11
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.11
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-logs
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - configMap:
            defaultMode: 420
            name: airflow-pod-template
          name: pod-template
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "23"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-14T04:28:34Z"
    generation: 2
    labels:
      app: airflow
      component: scheduler
      pod-template-hash: 5597b946c9
      release: airflow
    name: airflow-scheduler-5597b946c9
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-scheduler
      uid: e92ea920-8ca8-4806-ad4e-ff5a9af671c1
    resourceVersion: "13228215"
    uid: c717c203-d7f2-4ee8-9171-903d5f42cbd9
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: scheduler
        pod-template-hash: 5597b946c9
        release: airflow
    template:
      metadata:
        annotations:
          checksum/config-pod-template: 54db7f79e737f41dbbc38ce8443b63636ac3fd378b5ce6f8c474ff131af5f082
          checksum/secret-config-envs: 379fbef35e65b2d4b781e6b9b9675403a3757805cd7081c59e87fcfe69707a0a
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: scheduler
          pod-template-hash: 5597b946c9
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow scheduler -n -1
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/dumb-init
              - --
              - /entrypoint
              - python
              - -Wignore
              - -c
              - |
                import os
                import sys

                # suppress logs triggered from importing airflow packages
                os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                # shared imports
                try:
                    from airflow.jobs.job import Job
                except ImportError:
                    # `BaseJob` was renamed to `Job` in airflow 2.6.0
                    from airflow.jobs.base_job import BaseJob as Job
                from airflow.utils.db import create_session
                from airflow.utils.net import get_hostname

                # heartbeat check imports
                try:
                    from airflow.jobs.scheduler_job_runner import SchedulerJobRunner
                except ImportError:
                    # `SchedulerJob` is wrapped by `SchedulerJobRunner` since airflow 2.6.0
                    from airflow.jobs.scheduler_job import SchedulerJob as SchedulerJobRunner

                with create_session() as session:
                    ########################
                    # heartbeat check
                    ########################
                    # ensure the SchedulerJob with most recent heartbeat for this `hostname` is alive
                    hostname = get_hostname()
                    scheduler_job = session \
                        .query(Job) \
                        .filter_by(job_type=SchedulerJobRunner.job_type) \
                        .filter_by(hostname=hostname) \
                        .order_by(Job.latest_heartbeat.desc()) \
                        .limit(1) \
                        .first()
                    if (scheduler_job is not None) and scheduler_job.is_alive():
                        pass
                    else:
                        sys.exit(f"The SchedulerJob (id={scheduler_job.id}) for hostname '{hostname}' is not alive")
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: airflow-scheduler
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /opt/airflow/pod_templates/pod_template.yaml
            name: pod-template
            readOnly: true
            subPath: pod_template.yaml
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-logs
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - configMap:
            defaultMode: 420
            name: airflow-pod-template
          name: pod-template
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "16"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-07T10:16:58Z"
    generation: 2
    labels:
      app: airflow
      component: scheduler
      pod-template-hash: 5648cb9fcc
      release: airflow
    name: airflow-scheduler-5648cb9fcc
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-scheduler
      uid: e92ea920-8ca8-4806-ad4e-ff5a9af671c1
    resourceVersion: "11322641"
    uid: bb66e95e-b799-49da-b879-cac52221303e
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: scheduler
        pod-template-hash: 5648cb9fcc
        release: airflow
    template:
      metadata:
        annotations:
          checksum/config-pod-template: c2157000b315917362907eb7354ec1c16a6044c2af4c26eeb7d376b9b170a282
          checksum/secret-config-envs: 379fbef35e65b2d4b781e6b9b9675403a3757805cd7081c59e87fcfe69707a0a
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: scheduler
          pod-template-hash: 5648cb9fcc
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow scheduler -n -1
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/dumb-init
              - --
              - /entrypoint
              - python
              - -Wignore
              - -c
              - |
                import os
                import sys

                # suppress logs triggered from importing airflow packages
                os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                # shared imports
                try:
                    from airflow.jobs.job import Job
                except ImportError:
                    # `BaseJob` was renamed to `Job` in airflow 2.6.0
                    from airflow.jobs.base_job import BaseJob as Job
                from airflow.utils.db import create_session
                from airflow.utils.net import get_hostname

                # heartbeat check imports
                try:
                    from airflow.jobs.scheduler_job_runner import SchedulerJobRunner
                except ImportError:
                    # `SchedulerJob` is wrapped by `SchedulerJobRunner` since airflow 2.6.0
                    from airflow.jobs.scheduler_job import SchedulerJob as SchedulerJobRunner

                with create_session() as session:
                    ########################
                    # heartbeat check
                    ########################
                    # ensure the SchedulerJob with most recent heartbeat for this `hostname` is alive
                    hostname = get_hostname()
                    scheduler_job = session \
                        .query(Job) \
                        .filter_by(job_type=SchedulerJobRunner.job_type) \
                        .filter_by(hostname=hostname) \
                        .order_by(Job.latest_heartbeat.desc()) \
                        .limit(1) \
                        .first()
                    if (scheduler_job is not None) and scheduler_job.is_alive():
                        pass
                    else:
                        sys.exit(f"The SchedulerJob (id={scheduler_job.id}) for hostname '{hostname}' is not alive")
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: airflow-scheduler
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /opt/airflow/pod_templates/pod_template.yaml
            name: pod-template
            readOnly: true
            subPath: pod_template.yaml
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "true"
          - name: GIT_SSH_KNOWN_HOSTS_FILE
            value: /etc/git-secret/known_hosts
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
          - mountPath: /etc/git-secret/known_hosts
            name: git-known-hosts
            readOnly: true
            subPath: known_hosts
        - args:
          - bash
          - -c
          - |
            set -euo pipefail

            # break the infinite loop when we receive SIGINT or SIGTERM
            trap "exit 0" SIGINT SIGTERM

            while true; do
              START_EPOCH=$(date --utc +%s)
              echo "[$(date --utc +%FT%T.%3N)] deleting log files older than $RETENTION_MINUTES minutes..."

              # delete all writable files ending in ".log" with modified-time older than $RETENTION_MINUTES
              # NOTE: `-printf "."` prints a "." for each deleted file, which we count the bytes of with `wc -c`
              DELETED_COUNT=$(
                find "$LOG_PATH" \
                  -type f \
                  -name "*.log" \
                  -mmin +"$RETENTION_MINUTES" \
                  -writable \
                  -delete \
                  -printf "." \
                | wc -c
              )

              END_EPOCH=$(date --utc +%s)
              LOOP_DURATION=$((END_EPOCH - START_EPOCH))
              echo "[$(date --utc +%FT%T.%3N)] deleted $DELETED_COUNT files in $LOOP_DURATION seconds"

              SECONDS_TO_SLEEP=$((INTERVAL_SECONDS - LOOP_DURATION))
              if (( SECONDS_TO_SLEEP > 0 )); then
                echo "[$(date --utc +%FT%T.%3N)] waiting $SECONDS_TO_SLEEP seconds..."
                sleep $SECONDS_TO_SLEEP
              fi
            done
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: LOG_PATH
            value: /opt/airflow/logs
          - name: RETENTION_MINUTES
            value: "21600"
          - name: INTERVAL_SECONDS
            value: "900"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: log-cleanup
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/logs
            name: logs-data
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "true"
          - name: GIT_SSH_KNOWN_HOSTS_FILE
            value: /etc/git-secret/known_hosts
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
          - mountPath: /etc/git-secret/known_hosts
            name: git-known-hosts
            readOnly: true
            subPath: known_hosts
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: git-known-hosts
          secret:
            defaultMode: 420
            secretName: airflow-known-hosts
        - configMap:
            defaultMode: 420
            name: airflow-pod-template
          name: pod-template
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "20"
      deployment.kubernetes.io/revision-history: "18"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-07T11:04:50Z"
    generation: 2
    labels:
      app: airflow
      component: scheduler
      pod-template-hash: 5676d58586
      release: airflow
    name: airflow-scheduler-5676d58586
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-scheduler
      uid: e92ea920-8ca8-4806-ad4e-ff5a9af671c1
    resourceVersion: "12456212"
    uid: db2e24d1-c04c-41dd-950f-f157520d27ff
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: scheduler
        pod-template-hash: 5676d58586
        release: airflow
    template:
      metadata:
        annotations:
          checksum/config-pod-template: 3c11936224e495246626bca9b57de7c3ef05fd3eff3c9485be2bdc2162d4963e
          checksum/secret-config-envs: 028d706c61a3c89e43897be1e60b3a0a619a3ebc5d272cd25b8ea2d8b7fe444b
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: scheduler
          pod-template-hash: 5676d58586
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow scheduler -n -1
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/dumb-init
              - --
              - /entrypoint
              - python
              - -Wignore
              - -c
              - |
                import os
                import sys

                # suppress logs triggered from importing airflow packages
                os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                # shared imports
                try:
                    from airflow.jobs.job import Job
                except ImportError:
                    # `BaseJob` was renamed to `Job` in airflow 2.6.0
                    from airflow.jobs.base_job import BaseJob as Job
                from airflow.utils.db import create_session
                from airflow.utils.net import get_hostname

                # heartbeat check imports
                try:
                    from airflow.jobs.scheduler_job_runner import SchedulerJobRunner
                except ImportError:
                    # `SchedulerJob` is wrapped by `SchedulerJobRunner` since airflow 2.6.0
                    from airflow.jobs.scheduler_job import SchedulerJob as SchedulerJobRunner

                with create_session() as session:
                    ########################
                    # heartbeat check
                    ########################
                    # ensure the SchedulerJob with most recent heartbeat for this `hostname` is alive
                    hostname = get_hostname()
                    scheduler_job = session \
                        .query(Job) \
                        .filter_by(job_type=SchedulerJobRunner.job_type) \
                        .filter_by(hostname=hostname) \
                        .order_by(Job.latest_heartbeat.desc()) \
                        .limit(1) \
                        .first()
                    if (scheduler_job is not None) and scheduler_job.is_alive():
                        pass
                    else:
                        sys.exit(f"The SchedulerJob (id={scheduler_job.id}) for hostname '{hostname}' is not alive")
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: airflow-scheduler
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /opt/airflow/pod_templates/pod_template.yaml
            name: pod-template
            readOnly: true
            subPath: pod_template.yaml
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - |
            set -euo pipefail

            # break the infinite loop when we receive SIGINT or SIGTERM
            trap "exit 0" SIGINT SIGTERM

            while true; do
              START_EPOCH=$(date --utc +%s)
              echo "[$(date --utc +%FT%T.%3N)] deleting log files older than $RETENTION_MINUTES minutes..."

              # delete all writable files ending in ".log" with modified-time older than $RETENTION_MINUTES
              # NOTE: `-printf "."` prints a "." for each deleted file, which we count the bytes of with `wc -c`
              DELETED_COUNT=$(
                find "$LOG_PATH" \
                  -type f \
                  -name "*.log" \
                  -mmin +"$RETENTION_MINUTES" \
                  -writable \
                  -delete \
                  -printf "." \
                | wc -c
              )

              END_EPOCH=$(date --utc +%s)
              LOOP_DURATION=$((END_EPOCH - START_EPOCH))
              echo "[$(date --utc +%FT%T.%3N)] deleted $DELETED_COUNT files in $LOOP_DURATION seconds"

              SECONDS_TO_SLEEP=$((INTERVAL_SECONDS - LOOP_DURATION))
              if (( SECONDS_TO_SLEEP > 0 )); then
                echo "[$(date --utc +%FT%T.%3N)] waiting $SECONDS_TO_SLEEP seconds..."
                sleep $SECONDS_TO_SLEEP
              fi
            done
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: LOG_PATH
            value: /opt/airflow/logs
          - name: RETENTION_MINUTES
            value: "21600"
          - name: INTERVAL_SECONDS
            value: "900"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: log-cleanup
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/logs
            name: logs-data
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - configMap:
            defaultMode: 420
            name: airflow-pod-template
          name: pod-template
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "14"
      deployment.kubernetes.io/revision-history: "12"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-06T12:59:46Z"
    generation: 4
    labels:
      app: airflow
      component: scheduler
      pod-template-hash: 5bf6c5cb59
      release: airflow
    name: airflow-scheduler-5bf6c5cb59
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-scheduler
      uid: e92ea920-8ca8-4806-ad4e-ff5a9af671c1
    resourceVersion: "11088616"
    uid: 13a667ef-5cc5-4602-89d2-1370cc9ddf04
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: scheduler
        pod-template-hash: 5bf6c5cb59
        release: airflow
    template:
      metadata:
        annotations:
          checksum/config-pod-template: b418939b0adcca55f2c33d839ba08d26924bdaea2a137d1a34b1fa07a2323e5b
          checksum/secret-config-envs: 379fbef35e65b2d4b781e6b9b9675403a3757805cd7081c59e87fcfe69707a0a
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: scheduler
          pod-template-hash: 5bf6c5cb59
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow scheduler -n -1
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/dumb-init
              - --
              - /entrypoint
              - python
              - -Wignore
              - -c
              - |
                import os
                import sys

                # suppress logs triggered from importing airflow packages
                os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                # shared imports
                try:
                    from airflow.jobs.job import Job
                except ImportError:
                    # `BaseJob` was renamed to `Job` in airflow 2.6.0
                    from airflow.jobs.base_job import BaseJob as Job
                from airflow.utils.db import create_session
                from airflow.utils.net import get_hostname

                # heartbeat check imports
                try:
                    from airflow.jobs.scheduler_job_runner import SchedulerJobRunner
                except ImportError:
                    # `SchedulerJob` is wrapped by `SchedulerJobRunner` since airflow 2.6.0
                    from airflow.jobs.scheduler_job import SchedulerJob as SchedulerJobRunner

                with create_session() as session:
                    ########################
                    # heartbeat check
                    ########################
                    # ensure the SchedulerJob with most recent heartbeat for this `hostname` is alive
                    hostname = get_hostname()
                    scheduler_job = session \
                        .query(Job) \
                        .filter_by(job_type=SchedulerJobRunner.job_type) \
                        .filter_by(hostname=hostname) \
                        .order_by(Job.latest_heartbeat.desc()) \
                        .limit(1) \
                        .first()
                    if (scheduler_job is not None) and scheduler_job.is_alive():
                        pass
                    else:
                        sys.exit(f"The SchedulerJob (id={scheduler_job.id}) for hostname '{hostname}' is not alive")
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: airflow-scheduler
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
          - mountPath: /opt/airflow/pod_templates/pod_template.yaml
            name: pod-template
            readOnly: true
            subPath: pod_template.yaml
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: id_rsa_base64
        - args:
          - bash
          - -c
          - |
            set -euo pipefail

            # break the infinite loop when we receive SIGINT or SIGTERM
            trap "exit 0" SIGINT SIGTERM

            while true; do
              START_EPOCH=$(date --utc +%s)
              echo "[$(date --utc +%FT%T.%3N)] deleting log files older than $RETENTION_MINUTES minutes..."

              # delete all writable files ending in ".log" with modified-time older than $RETENTION_MINUTES
              # NOTE: `-printf "."` prints a "." for each deleted file, which we count the bytes of with `wc -c`
              DELETED_COUNT=$(
                find "$LOG_PATH" \
                  -type f \
                  -name "*.log" \
                  -mmin +"$RETENTION_MINUTES" \
                  -writable \
                  -delete \
                  -printf "." \
                | wc -c
              )

              END_EPOCH=$(date --utc +%s)
              LOOP_DURATION=$((END_EPOCH - START_EPOCH))
              echo "[$(date --utc +%FT%T.%3N)] deleted $DELETED_COUNT files in $LOOP_DURATION seconds"

              SECONDS_TO_SLEEP=$((INTERVAL_SECONDS - LOOP_DURATION))
              if (( SECONDS_TO_SLEEP > 0 )); then
                echo "[$(date --utc +%FT%T.%3N)] waiting $SECONDS_TO_SLEEP seconds..."
                sleep $SECONDS_TO_SLEEP
              fi
            done
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: LOG_PATH
            value: /opt/airflow/logs
          - name: RETENTION_MINUTES
            value: "21600"
          - name: INTERVAL_SECONDS
            value: "900"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: log-cleanup
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/logs
            name: logs-data
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - bash
          - -c
          - |
            unset PYTHONUSERBASE && \
            pip freeze | grep -i -e "apache-airflow=="  > protected-packages.txt && \
            pip install --constraint ./protected-packages.txt --user "pymssql" "apache-airflow-providers-samba" "smbprotocol" "psycopg2-binary" "sqlalchemy" "pyodbc"  && \
            echo "copying '/home/airflow/.local/' to '/opt/home-airflow-local/.local/'..." && \
            rsync -ah --stats --delete /home/airflow/.local/ /opt/home-airflow-local/.local/
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: install-pip-packages
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/home-airflow-local
            name: home-airflow-local
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: id_rsa_base64
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - emptyDir: {}
          name: home-airflow-local
        - configMap:
            defaultMode: 420
            name: airflow-pod-template
          name: pod-template
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "21"
      deployment.kubernetes.io/revision-history: "19"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-11T11:22:13Z"
    generation: 4
    labels:
      app: airflow
      component: scheduler
      pod-template-hash: 6748b7c6bb
      release: airflow
    name: airflow-scheduler-6748b7c6bb
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-scheduler
      uid: e92ea920-8ca8-4806-ad4e-ff5a9af671c1
    resourceVersion: "12467553"
    uid: 79de0006-7f27-4a4e-8509-e720714fa6fd
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: scheduler
        pod-template-hash: 6748b7c6bb
        release: airflow
    template:
      metadata:
        annotations:
          checksum/config-pod-template: 5111299b260726a87d5c3807c46233316b89f0cf1ea71394850f33e6d18b58eb
          checksum/secret-config-envs: 028d706c61a3c89e43897be1e60b3a0a619a3ebc5d272cd25b8ea2d8b7fe444b
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: scheduler
          pod-template-hash: 6748b7c6bb
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow scheduler -n -1
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/dumb-init
              - --
              - /entrypoint
              - python
              - -Wignore
              - -c
              - |
                import os
                import sys

                # suppress logs triggered from importing airflow packages
                os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                # shared imports
                try:
                    from airflow.jobs.job import Job
                except ImportError:
                    # `BaseJob` was renamed to `Job` in airflow 2.6.0
                    from airflow.jobs.base_job import BaseJob as Job
                from airflow.utils.db import create_session
                from airflow.utils.net import get_hostname

                # heartbeat check imports
                try:
                    from airflow.jobs.scheduler_job_runner import SchedulerJobRunner
                except ImportError:
                    # `SchedulerJob` is wrapped by `SchedulerJobRunner` since airflow 2.6.0
                    from airflow.jobs.scheduler_job import SchedulerJob as SchedulerJobRunner

                with create_session() as session:
                    ########################
                    # heartbeat check
                    ########################
                    # ensure the SchedulerJob with most recent heartbeat for this `hostname` is alive
                    hostname = get_hostname()
                    scheduler_job = session \
                        .query(Job) \
                        .filter_by(job_type=SchedulerJobRunner.job_type) \
                        .filter_by(hostname=hostname) \
                        .order_by(Job.latest_heartbeat.desc()) \
                        .limit(1) \
                        .first()
                    if (scheduler_job is not None) and scheduler_job.is_alive():
                        pass
                    else:
                        sys.exit(f"The SchedulerJob (id={scheduler_job.id}) for hostname '{hostname}' is not alive")
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: airflow-scheduler
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /opt/airflow/pod_templates/pod_template.yaml
            name: pod-template
            readOnly: true
            subPath: pod_template.yaml
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "true"
          - name: GIT_SSH_KNOWN_HOSTS_FILE
            value: /etc/git-secret/known_hosts
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
          - mountPath: /etc/git-secret/known_hosts
            name: git-known-hosts
            readOnly: true
            subPath: known_hosts
        - args:
          - bash
          - -c
          - |
            set -euo pipefail

            # break the infinite loop when we receive SIGINT or SIGTERM
            trap "exit 0" SIGINT SIGTERM

            while true; do
              START_EPOCH=$(date --utc +%s)
              echo "[$(date --utc +%FT%T.%3N)] deleting log files older than $RETENTION_MINUTES minutes..."

              # delete all writable files ending in ".log" with modified-time older than $RETENTION_MINUTES
              # NOTE: `-printf "."` prints a "." for each deleted file, which we count the bytes of with `wc -c`
              DELETED_COUNT=$(
                find "$LOG_PATH" \
                  -type f \
                  -name "*.log" \
                  -mmin +"$RETENTION_MINUTES" \
                  -writable \
                  -delete \
                  -printf "." \
                | wc -c
              )

              END_EPOCH=$(date --utc +%s)
              LOOP_DURATION=$((END_EPOCH - START_EPOCH))
              echo "[$(date --utc +%FT%T.%3N)] deleted $DELETED_COUNT files in $LOOP_DURATION seconds"

              SECONDS_TO_SLEEP=$((INTERVAL_SECONDS - LOOP_DURATION))
              if (( SECONDS_TO_SLEEP > 0 )); then
                echo "[$(date --utc +%FT%T.%3N)] waiting $SECONDS_TO_SLEEP seconds..."
                sleep $SECONDS_TO_SLEEP
              fi
            done
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: LOG_PATH
            value: /opt/airflow/logs
          - name: RETENTION_MINUTES
            value: "21600"
          - name: INTERVAL_SECONDS
            value: "900"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: log-cleanup
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/logs
            name: logs-data
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "true"
          - name: GIT_SSH_KNOWN_HOSTS_FILE
            value: /etc/git-secret/known_hosts
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
          - mountPath: /etc/git-secret/known_hosts
            name: git-known-hosts
            readOnly: true
            subPath: known_hosts
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: git-known-hosts
          secret:
            defaultMode: 420
            secretName: airflow-known-hosts
        - configMap:
            defaultMode: 420
            name: airflow-pod-template
          name: pod-template
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "17"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-07T10:22:33Z"
    generation: 3
    labels:
      app: airflow
      component: scheduler
      pod-template-hash: 77ff9469fd
      release: airflow
    name: airflow-scheduler-77ff9469fd
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-scheduler
      uid: e92ea920-8ca8-4806-ad4e-ff5a9af671c1
    resourceVersion: "11331922"
    uid: d1afe989-0757-4c86-b620-1839513de5bb
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: scheduler
        pod-template-hash: 77ff9469fd
        release: airflow
    template:
      metadata:
        annotations:
          checksum/config-pod-template: 62a2cad8d77df5b01386e39b715bcacc7655b9e5912b157bcb41831070aa762a
          checksum/secret-config-envs: 379fbef35e65b2d4b781e6b9b9675403a3757805cd7081c59e87fcfe69707a0a
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: scheduler
          pod-template-hash: 77ff9469fd
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow scheduler -n -1
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/dumb-init
              - --
              - /entrypoint
              - python
              - -Wignore
              - -c
              - |
                import os
                import sys

                # suppress logs triggered from importing airflow packages
                os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                # shared imports
                try:
                    from airflow.jobs.job import Job
                except ImportError:
                    # `BaseJob` was renamed to `Job` in airflow 2.6.0
                    from airflow.jobs.base_job import BaseJob as Job
                from airflow.utils.db import create_session
                from airflow.utils.net import get_hostname

                # heartbeat check imports
                try:
                    from airflow.jobs.scheduler_job_runner import SchedulerJobRunner
                except ImportError:
                    # `SchedulerJob` is wrapped by `SchedulerJobRunner` since airflow 2.6.0
                    from airflow.jobs.scheduler_job import SchedulerJob as SchedulerJobRunner

                with create_session() as session:
                    ########################
                    # heartbeat check
                    ########################
                    # ensure the SchedulerJob with most recent heartbeat for this `hostname` is alive
                    hostname = get_hostname()
                    scheduler_job = session \
                        .query(Job) \
                        .filter_by(job_type=SchedulerJobRunner.job_type) \
                        .filter_by(hostname=hostname) \
                        .order_by(Job.latest_heartbeat.desc()) \
                        .limit(1) \
                        .first()
                    if (scheduler_job is not None) and scheduler_job.is_alive():
                        pass
                    else:
                        sys.exit(f"The SchedulerJob (id={scheduler_job.id}) for hostname '{hostname}' is not alive")
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: airflow-scheduler
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /opt/airflow/pod_templates/pod_template.yaml
            name: pod-template
            readOnly: true
            subPath: pod_template.yaml
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - |
            set -euo pipefail

            # break the infinite loop when we receive SIGINT or SIGTERM
            trap "exit 0" SIGINT SIGTERM

            while true; do
              START_EPOCH=$(date --utc +%s)
              echo "[$(date --utc +%FT%T.%3N)] deleting log files older than $RETENTION_MINUTES minutes..."

              # delete all writable files ending in ".log" with modified-time older than $RETENTION_MINUTES
              # NOTE: `-printf "."` prints a "." for each deleted file, which we count the bytes of with `wc -c`
              DELETED_COUNT=$(
                find "$LOG_PATH" \
                  -type f \
                  -name "*.log" \
                  -mmin +"$RETENTION_MINUTES" \
                  -writable \
                  -delete \
                  -printf "." \
                | wc -c
              )

              END_EPOCH=$(date --utc +%s)
              LOOP_DURATION=$((END_EPOCH - START_EPOCH))
              echo "[$(date --utc +%FT%T.%3N)] deleted $DELETED_COUNT files in $LOOP_DURATION seconds"

              SECONDS_TO_SLEEP=$((INTERVAL_SECONDS - LOOP_DURATION))
              if (( SECONDS_TO_SLEEP > 0 )); then
                echo "[$(date --utc +%FT%T.%3N)] waiting $SECONDS_TO_SLEEP seconds..."
                sleep $SECONDS_TO_SLEEP
              fi
            done
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: LOG_PATH
            value: /opt/airflow/logs
          - name: RETENTION_MINUTES
            value: "21600"
          - name: INTERVAL_SECONDS
            value: "900"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: log-cleanup
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/logs
            name: logs-data
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - configMap:
            defaultMode: 420
            name: airflow-pod-template
          name: pod-template
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "15"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-06T14:18:10Z"
    generation: 3
    labels:
      app: airflow
      component: scheduler
      pod-template-hash: 7cbfb49789
      release: airflow
    name: airflow-scheduler-7cbfb49789
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-scheduler
      uid: e92ea920-8ca8-4806-ad4e-ff5a9af671c1
    resourceVersion: "11323062"
    uid: 1be929a9-d3b1-48fa-9643-16d0c1a18270
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: scheduler
        pod-template-hash: 7cbfb49789
        release: airflow
    template:
      metadata:
        annotations:
          checksum/config-pod-template: ede78dfa41aff4e9de96734cef6d715221f4d0efdf383f8411bb6c1b3063a33a
          checksum/secret-config-envs: 379fbef35e65b2d4b781e6b9b9675403a3757805cd7081c59e87fcfe69707a0a
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: scheduler
          pod-template-hash: 7cbfb49789
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow scheduler -n -1
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/dumb-init
              - --
              - /entrypoint
              - python
              - -Wignore
              - -c
              - |
                import os
                import sys

                # suppress logs triggered from importing airflow packages
                os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                # shared imports
                try:
                    from airflow.jobs.job import Job
                except ImportError:
                    # `BaseJob` was renamed to `Job` in airflow 2.6.0
                    from airflow.jobs.base_job import BaseJob as Job
                from airflow.utils.db import create_session
                from airflow.utils.net import get_hostname

                # heartbeat check imports
                try:
                    from airflow.jobs.scheduler_job_runner import SchedulerJobRunner
                except ImportError:
                    # `SchedulerJob` is wrapped by `SchedulerJobRunner` since airflow 2.6.0
                    from airflow.jobs.scheduler_job import SchedulerJob as SchedulerJobRunner

                with create_session() as session:
                    ########################
                    # heartbeat check
                    ########################
                    # ensure the SchedulerJob with most recent heartbeat for this `hostname` is alive
                    hostname = get_hostname()
                    scheduler_job = session \
                        .query(Job) \
                        .filter_by(job_type=SchedulerJobRunner.job_type) \
                        .filter_by(hostname=hostname) \
                        .order_by(Job.latest_heartbeat.desc()) \
                        .limit(1) \
                        .first()
                    if (scheduler_job is not None) and scheduler_job.is_alive():
                        pass
                    else:
                        sys.exit(f"The SchedulerJob (id={scheduler_job.id}) for hostname '{hostname}' is not alive")
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: airflow-scheduler
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
          - mountPath: /opt/airflow/pod_templates/pod_template.yaml
            name: pod-template
            readOnly: true
            subPath: pod_template.yaml
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - |
            set -euo pipefail

            # break the infinite loop when we receive SIGINT or SIGTERM
            trap "exit 0" SIGINT SIGTERM

            while true; do
              START_EPOCH=$(date --utc +%s)
              echo "[$(date --utc +%FT%T.%3N)] deleting log files older than $RETENTION_MINUTES minutes..."

              # delete all writable files ending in ".log" with modified-time older than $RETENTION_MINUTES
              # NOTE: `-printf "."` prints a "." for each deleted file, which we count the bytes of with `wc -c`
              DELETED_COUNT=$(
                find "$LOG_PATH" \
                  -type f \
                  -name "*.log" \
                  -mmin +"$RETENTION_MINUTES" \
                  -writable \
                  -delete \
                  -printf "." \
                | wc -c
              )

              END_EPOCH=$(date --utc +%s)
              LOOP_DURATION=$((END_EPOCH - START_EPOCH))
              echo "[$(date --utc +%FT%T.%3N)] deleted $DELETED_COUNT files in $LOOP_DURATION seconds"

              SECONDS_TO_SLEEP=$((INTERVAL_SECONDS - LOOP_DURATION))
              if (( SECONDS_TO_SLEEP > 0 )); then
                echo "[$(date --utc +%FT%T.%3N)] waiting $SECONDS_TO_SLEEP seconds..."
                sleep $SECONDS_TO_SLEEP
              fi
            done
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: LOG_PATH
            value: /opt/airflow/logs
          - name: RETENTION_MINUTES
            value: "21600"
          - name: INTERVAL_SECONDS
            value: "900"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: log-cleanup
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/logs
            name: logs-data
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - bash
          - -c
          - |
            unset PYTHONUSERBASE && \
            pip freeze | grep -i -e "apache-airflow=="  > protected-packages.txt && \
            pip install --constraint ./protected-packages.txt --user "pymssql" "apache-airflow-providers-samba" "smbprotocol" "psycopg2-binary" "sqlalchemy" "pyodbc"  && \
            echo "copying '/home/airflow/.local/' to '/opt/home-airflow-local/.local/'..." && \
            rsync -ah --stats --delete /home/airflow/.local/ /opt/home-airflow-local/.local/
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: install-pip-packages
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/home-airflow-local
            name: home-airflow-local
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - emptyDir: {}
          name: home-airflow-local
        - configMap:
            defaultMode: 420
            name: airflow-pod-template
          name: pod-template
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "24"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-14T04:56:23Z"
    generation: 2
    labels:
      app: airflow
      component: scheduler
      pod-template-hash: 84f5cbd8cb
      release: airflow
    name: airflow-scheduler-84f5cbd8cb
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-scheduler
      uid: e92ea920-8ca8-4806-ad4e-ff5a9af671c1
    resourceVersion: "13231822"
    uid: ed5e55aa-2045-4e71-9e80-f03d61f8313d
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: scheduler
        pod-template-hash: 84f5cbd8cb
        release: airflow
    template:
      metadata:
        annotations:
          checksum/config-pod-template: cc67c555903001d4e27159f065cfab4e252e871dd8fe78a7ceddc4793beb5a03
          checksum/secret-config-envs: 5928424b76843c48d9e503eecfd8541d5af836b104787689f14c26baf7dee423
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: scheduler
          pod-template-hash: 84f5cbd8cb
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow scheduler -n -1
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.10
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/dumb-init
              - --
              - /entrypoint
              - python
              - -Wignore
              - -c
              - |
                import os
                import sys

                # suppress logs triggered from importing airflow packages
                os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                # shared imports
                try:
                    from airflow.jobs.job import Job
                except ImportError:
                    # `BaseJob` was renamed to `Job` in airflow 2.6.0
                    from airflow.jobs.base_job import BaseJob as Job
                from airflow.utils.db import create_session
                from airflow.utils.net import get_hostname

                # heartbeat check imports
                try:
                    from airflow.jobs.scheduler_job_runner import SchedulerJobRunner
                except ImportError:
                    # `SchedulerJob` is wrapped by `SchedulerJobRunner` since airflow 2.6.0
                    from airflow.jobs.scheduler_job import SchedulerJob as SchedulerJobRunner

                with create_session() as session:
                    ########################
                    # heartbeat check
                    ########################
                    # ensure the SchedulerJob with most recent heartbeat for this `hostname` is alive
                    hostname = get_hostname()
                    scheduler_job = session \
                        .query(Job) \
                        .filter_by(job_type=SchedulerJobRunner.job_type) \
                        .filter_by(hostname=hostname) \
                        .order_by(Job.latest_heartbeat.desc()) \
                        .limit(1) \
                        .first()
                    if (scheduler_job is not None) and scheduler_job.is_alive():
                        pass
                    else:
                        sys.exit(f"The SchedulerJob (id={scheduler_job.id}) for hostname '{hostname}' is not alive")
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: airflow-scheduler
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /opt/airflow/pod_templates/pod_template.yaml
            name: pod-template
            readOnly: true
            subPath: pod_template.yaml
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.10
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.10
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-logs
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - configMap:
            defaultMode: 420
            name: airflow-pod-template
          name: pod-template
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "13"
      deployment.kubernetes.io/revision-history: "11"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-04T11:40:35Z"
    generation: 2
    labels:
      app: airflow
      component: scheduler
      pod-template-hash: 85cd55bdbb
      release: airflow
    name: airflow-scheduler-85cd55bdbb
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-scheduler
      uid: e92ea920-8ca8-4806-ad4e-ff5a9af671c1
    resourceVersion: "11089350"
    uid: f1a4c5ef-cded-41a6-931f-e9911f053b4b
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: scheduler
        pod-template-hash: 85cd55bdbb
        release: airflow
    template:
      metadata:
        annotations:
          checksum/config-pod-template: d24401fc27674235a5de643b1fe46ccbbd42cf5969126d93749c4a504bc04616
          checksum/secret-config-envs: fe44ec2fcd31f346e7df5c06847f85e9f2fe78b8c87a596e234ca473034f72eb
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: scheduler
          pod-template-hash: 85cd55bdbb
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow scheduler -n -1
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/dumb-init
              - --
              - /entrypoint
              - python
              - -Wignore
              - -c
              - |
                import os
                import sys

                # suppress logs triggered from importing airflow packages
                os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                # shared imports
                try:
                    from airflow.jobs.job import Job
                except ImportError:
                    # `BaseJob` was renamed to `Job` in airflow 2.6.0
                    from airflow.jobs.base_job import BaseJob as Job
                from airflow.utils.db import create_session
                from airflow.utils.net import get_hostname

                # heartbeat check imports
                try:
                    from airflow.jobs.scheduler_job_runner import SchedulerJobRunner
                except ImportError:
                    # `SchedulerJob` is wrapped by `SchedulerJobRunner` since airflow 2.6.0
                    from airflow.jobs.scheduler_job import SchedulerJob as SchedulerJobRunner

                with create_session() as session:
                    ########################
                    # heartbeat check
                    ########################
                    # ensure the SchedulerJob with most recent heartbeat for this `hostname` is alive
                    hostname = get_hostname()
                    scheduler_job = session \
                        .query(Job) \
                        .filter_by(job_type=SchedulerJobRunner.job_type) \
                        .filter_by(hostname=hostname) \
                        .order_by(Job.latest_heartbeat.desc()) \
                        .limit(1) \
                        .first()
                    if (scheduler_job is not None) and scheduler_job.is_alive():
                        pass
                    else:
                        sys.exit(f"The SchedulerJob (id={scheduler_job.id}) for hostname '{hostname}' is not alive")
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: airflow-scheduler
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
          - mountPath: /opt/airflow/pod_templates/pod_template.yaml
            name: pod-template
            readOnly: true
            subPath: pod_template.yaml
        - args:
          - bash
          - -c
          - |
            set -euo pipefail

            # break the infinite loop when we receive SIGINT or SIGTERM
            trap "exit 0" SIGINT SIGTERM

            while true; do
              START_EPOCH=$(date --utc +%s)
              echo "[$(date --utc +%FT%T.%3N)] deleting log files older than $RETENTION_MINUTES minutes..."

              # delete all writable files ending in ".log" with modified-time older than $RETENTION_MINUTES
              # NOTE: `-printf "."` prints a "." for each deleted file, which we count the bytes of with `wc -c`
              DELETED_COUNT=$(
                find "$LOG_PATH" \
                  -type f \
                  -name "*.log" \
                  -mmin +"$RETENTION_MINUTES" \
                  -writable \
                  -delete \
                  -printf "." \
                | wc -c
              )

              END_EPOCH=$(date --utc +%s)
              LOOP_DURATION=$((END_EPOCH - START_EPOCH))
              echo "[$(date --utc +%FT%T.%3N)] deleted $DELETED_COUNT files in $LOOP_DURATION seconds"

              SECONDS_TO_SLEEP=$((INTERVAL_SECONDS - LOOP_DURATION))
              if (( SECONDS_TO_SLEEP > 0 )); then
                echo "[$(date --utc +%FT%T.%3N)] waiting $SECONDS_TO_SLEEP seconds..."
                sleep $SECONDS_TO_SLEEP
              fi
            done
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: LOG_PATH
            value: /opt/airflow/logs
          - name: RETENTION_MINUTES
            value: "21600"
          - name: INTERVAL_SECONDS
            value: "900"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: log-cleanup
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/logs
            name: logs-data
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - bash
          - -c
          - |
            unset PYTHONUSERBASE && \
            pip freeze | grep -i -e "apache-airflow=="  > protected-packages.txt && \
            pip install --constraint ./protected-packages.txt --user "pymssql" "apache-airflow-providers-samba" "smbprotocol" "psycopg2-binary" "sqlalchemy" "pyodbc"  && \
            echo "copying '/home/airflow/.local/' to '/opt/home-airflow-local/.local/'..." && \
            rsync -ah --stats --delete /home/airflow/.local/ /opt/home-airflow-local/.local/
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: install-pip-packages
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/home-airflow-local
            name: home-airflow-local
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - name: dags-data
          persistentVolumeClaim:
            claimName: airflow-dags
        - emptyDir: {}
          name: logs-data
        - emptyDir: {}
          name: home-airflow-local
        - configMap:
            defaultMode: 420
            name: airflow-pod-template
          name: pod-template
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "22"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-11T12:50:40Z"
    generation: 2
    labels:
      app: airflow
      component: scheduler
      pod-template-hash: c8d5b85c7
      release: airflow
    name: airflow-scheduler-c8d5b85c7
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-scheduler
      uid: e92ea920-8ca8-4806-ad4e-ff5a9af671c1
    resourceVersion: "13222446"
    uid: 9ce941c5-d60e-4abe-9c27-d58b4044a903
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: scheduler
        pod-template-hash: c8d5b85c7
        release: airflow
    template:
      metadata:
        annotations:
          checksum/config-pod-template: 266f6b9a6da9cb5f81ec6cc254aea0e88bdc8dea6e6277aba1673077793aaab6
          checksum/secret-config-envs: 028d706c61a3c89e43897be1e60b3a0a619a3ebc5d272cd25b8ea2d8b7fe444b
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: scheduler
          pod-template-hash: c8d5b85c7
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow scheduler -n -1
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/dumb-init
              - --
              - /entrypoint
              - python
              - -Wignore
              - -c
              - |
                import os
                import sys

                # suppress logs triggered from importing airflow packages
                os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                # shared imports
                try:
                    from airflow.jobs.job import Job
                except ImportError:
                    # `BaseJob` was renamed to `Job` in airflow 2.6.0
                    from airflow.jobs.base_job import BaseJob as Job
                from airflow.utils.db import create_session
                from airflow.utils.net import get_hostname

                # heartbeat check imports
                try:
                    from airflow.jobs.scheduler_job_runner import SchedulerJobRunner
                except ImportError:
                    # `SchedulerJob` is wrapped by `SchedulerJobRunner` since airflow 2.6.0
                    from airflow.jobs.scheduler_job import SchedulerJob as SchedulerJobRunner

                with create_session() as session:
                    ########################
                    # heartbeat check
                    ########################
                    # ensure the SchedulerJob with most recent heartbeat for this `hostname` is alive
                    hostname = get_hostname()
                    scheduler_job = session \
                        .query(Job) \
                        .filter_by(job_type=SchedulerJobRunner.job_type) \
                        .filter_by(hostname=hostname) \
                        .order_by(Job.latest_heartbeat.desc()) \
                        .limit(1) \
                        .first()
                    if (scheduler_job is not None) and scheduler_job.is_alive():
                        pass
                    else:
                        sys.exit(f"The SchedulerJob (id={scheduler_job.id}) for hostname '{hostname}' is not alive")
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: airflow-scheduler
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /opt/airflow/pod_templates/pod_template.yaml
            name: pod-template
            readOnly: true
            subPath: pod_template.yaml
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-logs
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - configMap:
            defaultMode: 420
            name: airflow-pod-template
          name: pod-template
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "24"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-14T04:56:26Z"
    generation: 2
    labels:
      app: airflow
      component: sync-users
      pod-template-hash: 5885bfb9d
      release: airflow
    name: airflow-sync-users-5885bfb9d
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-sync-users
      uid: 29b6ea14-c5af-46dd-9172-07088904cb02
    resourceVersion: "13231268"
    uid: c4d681ec-9939-40b7-814b-7a0684dfcfce
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: sync-users
        pod-template-hash: 5885bfb9d
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 5928424b76843c48d9e503eecfd8541d5af836b104787689f14c26baf7dee423
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          checksum/sync-users-script: 0e7bdc9596783274f1496d1dc37fa2eda2d72d60a2b56e3b61274b347f7e10ee
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: sync-users
          pod-template-hash: 5885bfb9d
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - python
          - -u
          - /mnt/scripts/sync_users.py
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.10
          imagePullPolicy: IfNotPresent
          name: sync-airflow-users
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /mnt/scripts
            name: scripts
            readOnly: true
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.10
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.10
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-logs
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: scripts
          secret:
            defaultMode: 420
            secretName: airflow-sync-users
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "22"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-11T12:50:43Z"
    generation: 2
    labels:
      app: airflow
      component: sync-users
      pod-template-hash: 5f6bf58777
      release: airflow
    name: airflow-sync-users-5f6bf58777
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-sync-users
      uid: 29b6ea14-c5af-46dd-9172-07088904cb02
    resourceVersion: "13222046"
    uid: 306b9187-e82a-4280-820a-47e263d33a42
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: sync-users
        pod-template-hash: 5f6bf58777
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 028d706c61a3c89e43897be1e60b3a0a619a3ebc5d272cd25b8ea2d8b7fe444b
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          checksum/sync-users-script: 0e7bdc9596783274f1496d1dc37fa2eda2d72d60a2b56e3b61274b347f7e10ee
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: sync-users
          pod-template-hash: 5f6bf58777
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - python
          - -u
          - /mnt/scripts/sync_users.py
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: sync-airflow-users
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /mnt/scripts
            name: scripts
            readOnly: true
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-logs
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: scripts
          secret:
            defaultMode: 420
            secretName: airflow-sync-users
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "21"
      deployment.kubernetes.io/revision-history: "19"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-11T11:22:16Z"
    generation: 4
    labels:
      app: airflow
      component: sync-users
      pod-template-hash: 5f8bc594f8
      release: airflow
    name: airflow-sync-users-5f8bc594f8
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-sync-users
      uid: 29b6ea14-c5af-46dd-9172-07088904cb02
    resourceVersion: "12466718"
    uid: ed1d7b58-773a-4ba6-b031-2e772983755f
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: sync-users
        pod-template-hash: 5f8bc594f8
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 028d706c61a3c89e43897be1e60b3a0a619a3ebc5d272cd25b8ea2d8b7fe444b
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          checksum/sync-users-script: 0e7bdc9596783274f1496d1dc37fa2eda2d72d60a2b56e3b61274b347f7e10ee
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: sync-users
          pod-template-hash: 5f8bc594f8
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - python
          - -u
          - /mnt/scripts/sync_users.py
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: sync-airflow-users
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /mnt/scripts
            name: scripts
            readOnly: true
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "true"
          - name: GIT_SSH_KNOWN_HOSTS_FILE
            value: /etc/git-secret/known_hosts
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
          - mountPath: /etc/git-secret/known_hosts
            name: git-known-hosts
            readOnly: true
            subPath: known_hosts
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "true"
          - name: GIT_SSH_KNOWN_HOSTS_FILE
            value: /etc/git-secret/known_hosts
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
          - mountPath: /etc/git-secret/known_hosts
            name: git-known-hosts
            readOnly: true
            subPath: known_hosts
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: git-known-hosts
          secret:
            defaultMode: 420
            secretName: airflow-known-hosts
        - name: scripts
          secret:
            defaultMode: 420
            secretName: airflow-sync-users
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "23"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-14T04:28:36Z"
    generation: 2
    labels:
      app: airflow
      component: sync-users
      pod-template-hash: 6d68cf6f
      release: airflow
    name: airflow-sync-users-6d68cf6f
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-sync-users
      uid: 29b6ea14-c5af-46dd-9172-07088904cb02
    resourceVersion: "13227794"
    uid: 30e00f6d-4d6e-404a-9a38-8b55f8dc341a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: sync-users
        pod-template-hash: 6d68cf6f
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 379fbef35e65b2d4b781e6b9b9675403a3757805cd7081c59e87fcfe69707a0a
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          checksum/sync-users-script: 0e7bdc9596783274f1496d1dc37fa2eda2d72d60a2b56e3b61274b347f7e10ee
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: sync-users
          pod-template-hash: 6d68cf6f
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - python
          - -u
          - /mnt/scripts/sync_users.py
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: sync-airflow-users
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /mnt/scripts
            name: scripts
            readOnly: true
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-logs
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: scripts
          secret:
            defaultMode: 420
            secretName: airflow-sync-users
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "16"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-07T10:17:06Z"
    generation: 2
    labels:
      app: airflow
      component: sync-users
      pod-template-hash: 6d96b99446
      release: airflow
    name: airflow-sync-users-6d96b99446
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-sync-users
      uid: 29b6ea14-c5af-46dd-9172-07088904cb02
    resourceVersion: "11322650"
    uid: 423f5796-5c90-4cb0-aad6-c892a8180a5f
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: sync-users
        pod-template-hash: 6d96b99446
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 379fbef35e65b2d4b781e6b9b9675403a3757805cd7081c59e87fcfe69707a0a
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          checksum/sync-users-script: 0e7bdc9596783274f1496d1dc37fa2eda2d72d60a2b56e3b61274b347f7e10ee
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: sync-users
          pod-template-hash: 6d96b99446
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - python
          - -u
          - /mnt/scripts/sync_users.py
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: sync-airflow-users
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /mnt/scripts
            name: scripts
            readOnly: true
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "true"
          - name: GIT_SSH_KNOWN_HOSTS_FILE
            value: /etc/git-secret/known_hosts
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
          - mountPath: /etc/git-secret/known_hosts
            name: git-known-hosts
            readOnly: true
            subPath: known_hosts
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "true"
          - name: GIT_SSH_KNOWN_HOSTS_FILE
            value: /etc/git-secret/known_hosts
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
          - mountPath: /etc/git-secret/known_hosts
            name: git-known-hosts
            readOnly: true
            subPath: known_hosts
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: git-known-hosts
          secret:
            defaultMode: 420
            secretName: airflow-known-hosts
        - name: scripts
          secret:
            defaultMode: 420
            secretName: airflow-sync-users
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "14"
      deployment.kubernetes.io/revision-history: "12"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-06T12:59:53Z"
    generation: 4
    labels:
      app: airflow
      component: sync-users
      pod-template-hash: 6f4ffb59b8
      release: airflow
    name: airflow-sync-users-6f4ffb59b8
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-sync-users
      uid: 29b6ea14-c5af-46dd-9172-07088904cb02
    resourceVersion: "11088618"
    uid: 0f840938-babf-468e-974f-244346404473
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: sync-users
        pod-template-hash: 6f4ffb59b8
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 379fbef35e65b2d4b781e6b9b9675403a3757805cd7081c59e87fcfe69707a0a
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          checksum/sync-users-script: 0e7bdc9596783274f1496d1dc37fa2eda2d72d60a2b56e3b61274b347f7e10ee
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: sync-users
          pod-template-hash: 6f4ffb59b8
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - python
          - -u
          - /mnt/scripts/sync_users.py
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: sync-airflow-users
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
          - mountPath: /mnt/scripts
            name: scripts
            readOnly: true
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: id_rsa_base64
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - bash
          - -c
          - |
            unset PYTHONUSERBASE && \
            pip freeze | grep -i -e "apache-airflow=="  > protected-packages.txt && \
            pip install --constraint ./protected-packages.txt --user "pymssql" "apache-airflow-providers-samba" "smbprotocol" "psycopg2-binary" "sqlalchemy" "pyodbc"  && \
            echo "copying '/home/airflow/.local/' to '/opt/home-airflow-local/.local/'..." && \
            rsync -ah --stats --delete /home/airflow/.local/ /opt/home-airflow-local/.local/
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: install-pip-packages
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/home-airflow-local
            name: home-airflow-local
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: id_rsa_base64
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - emptyDir: {}
          name: home-airflow-local
        - name: scripts
          secret:
            defaultMode: 420
            secretName: airflow-sync-users
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "17"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-07T10:22:37Z"
    generation: 2
    labels:
      app: airflow
      component: sync-users
      pod-template-hash: 7778c4977d
      release: airflow
    name: airflow-sync-users-7778c4977d
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-sync-users
      uid: 29b6ea14-c5af-46dd-9172-07088904cb02
    resourceVersion: "11331173"
    uid: 7770c6f1-8d81-45c9-ba9c-5a2b50a9f50f
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: sync-users
        pod-template-hash: 7778c4977d
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 379fbef35e65b2d4b781e6b9b9675403a3757805cd7081c59e87fcfe69707a0a
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          checksum/sync-users-script: 0e7bdc9596783274f1496d1dc37fa2eda2d72d60a2b56e3b61274b347f7e10ee
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: sync-users
          pod-template-hash: 7778c4977d
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - python
          - -u
          - /mnt/scripts/sync_users.py
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: sync-airflow-users
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /mnt/scripts
            name: scripts
            readOnly: true
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: scripts
          secret:
            defaultMode: 420
            secretName: airflow-sync-users
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "15"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-06T14:18:22Z"
    generation: 2
    labels:
      app: airflow
      component: sync-users
      pod-template-hash: 796dbb594d
      release: airflow
    name: airflow-sync-users-796dbb594d
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-sync-users
      uid: 29b6ea14-c5af-46dd-9172-07088904cb02
    resourceVersion: "11321255"
    uid: 77ef80b7-b18f-41af-9cd3-f45cdeeff62d
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: sync-users
        pod-template-hash: 796dbb594d
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 379fbef35e65b2d4b781e6b9b9675403a3757805cd7081c59e87fcfe69707a0a
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          checksum/sync-users-script: 0e7bdc9596783274f1496d1dc37fa2eda2d72d60a2b56e3b61274b347f7e10ee
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: sync-users
          pod-template-hash: 796dbb594d
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - python
          - -u
          - /mnt/scripts/sync_users.py
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: sync-airflow-users
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
          - mountPath: /mnt/scripts
            name: scripts
            readOnly: true
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - bash
          - -c
          - |
            unset PYTHONUSERBASE && \
            pip freeze | grep -i -e "apache-airflow=="  > protected-packages.txt && \
            pip install --constraint ./protected-packages.txt --user "pymssql" "apache-airflow-providers-samba" "smbprotocol" "psycopg2-binary" "sqlalchemy" "pyodbc"  && \
            echo "copying '/home/airflow/.local/' to '/opt/home-airflow-local/.local/'..." && \
            rsync -ah --stats --delete /home/airflow/.local/ /opt/home-airflow-local/.local/
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: install-pip-packages
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/home-airflow-local
            name: home-airflow-local
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - emptyDir: {}
          name: home-airflow-local
        - name: scripts
          secret:
            defaultMode: 420
            secretName: airflow-sync-users
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "20"
      deployment.kubernetes.io/revision-history: "18"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-07T11:04:51Z"
    generation: 4
    labels:
      app: airflow
      component: sync-users
      pod-template-hash: 7c689cb78
      release: airflow
    name: airflow-sync-users-7c689cb78
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-sync-users
      uid: 29b6ea14-c5af-46dd-9172-07088904cb02
    resourceVersion: "12455935"
    uid: 5444d437-913e-4246-ac7e-c7faed2b6270
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: sync-users
        pod-template-hash: 7c689cb78
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 028d706c61a3c89e43897be1e60b3a0a619a3ebc5d272cd25b8ea2d8b7fe444b
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          checksum/sync-users-script: 0e7bdc9596783274f1496d1dc37fa2eda2d72d60a2b56e3b61274b347f7e10ee
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: sync-users
          pod-template-hash: 7c689cb78
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - python
          - -u
          - /mnt/scripts/sync_users.py
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: sync-airflow-users
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /mnt/scripts
            name: scripts
            readOnly: true
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: scripts
          secret:
            defaultMode: 420
            secretName: airflow-sync-users
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "25"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-14T05:12:13Z"
    generation: 1
    labels:
      app: airflow
      component: sync-users
      pod-template-hash: 7ffb469495
      release: airflow
    name: airflow-sync-users-7ffb469495
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-sync-users
      uid: 29b6ea14-c5af-46dd-9172-07088904cb02
    resourceVersion: "13231783"
    uid: 40b8488f-f22b-4808-b5d8-2766d956ba4c
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: airflow
        component: sync-users
        pod-template-hash: 7ffb469495
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 7b5b4b568e7c694c0f44651459436d8876c0d6f337d3876162036cffceec3b68
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          checksum/sync-users-script: 0e7bdc9596783274f1496d1dc37fa2eda2d72d60a2b56e3b61274b347f7e10ee
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: sync-users
          pod-template-hash: 7ffb469495
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - python
          - -u
          - /mnt/scripts/sync_users.py
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.11
          imagePullPolicy: IfNotPresent
          name: sync-airflow-users
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /mnt/scripts
            name: scripts
            readOnly: true
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.11
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.11
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-logs
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: scripts
          secret:
            defaultMode: 420
            secretName: airflow-sync-users
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "13"
      deployment.kubernetes.io/revision-history: "11"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-04T11:40:43Z"
    generation: 4
    labels:
      app: airflow
      component: sync-users
      pod-template-hash: d567f797b
      release: airflow
    name: airflow-sync-users-d567f797b
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-sync-users
      uid: 29b6ea14-c5af-46dd-9172-07088904cb02
    resourceVersion: "11075249"
    uid: 760e540c-a4a9-4127-bf72-61d949c95ea7
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: sync-users
        pod-template-hash: d567f797b
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: fe44ec2fcd31f346e7df5c06847f85e9f2fe78b8c87a596e234ca473034f72eb
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          checksum/sync-users-script: 0e7bdc9596783274f1496d1dc37fa2eda2d72d60a2b56e3b61274b347f7e10ee
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: sync-users
          pod-template-hash: d567f797b
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - python
          - -u
          - /mnt/scripts/sync_users.py
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: sync-airflow-users
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
          - mountPath: /mnt/scripts
            name: scripts
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - bash
          - -c
          - |
            unset PYTHONUSERBASE && \
            pip freeze | grep -i -e "apache-airflow=="  > protected-packages.txt && \
            pip install --constraint ./protected-packages.txt --user "pymssql" "apache-airflow-providers-samba" "smbprotocol" "psycopg2-binary" "sqlalchemy" "pyodbc"  && \
            echo "copying '/home/airflow/.local/' to '/opt/home-airflow-local/.local/'..." && \
            rsync -ah --stats --delete /home/airflow/.local/ /opt/home-airflow-local/.local/
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: install-pip-packages
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/home-airflow-local
            name: home-airflow-local
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - name: dags-data
          persistentVolumeClaim:
            claimName: airflow-dags
        - emptyDir: {}
          name: logs-data
        - emptyDir: {}
          name: home-airflow-local
        - name: scripts
          secret:
            defaultMode: 420
            secretName: airflow-sync-users
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "25"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-14T05:12:12Z"
    generation: 1
    labels:
      app: airflow
      component: triggerer
      pod-template-hash: 54b68d597f
      release: airflow
    name: airflow-triggerer-54b68d597f
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-triggerer
      uid: df1ec31d-c210-44a1-8a99-70bd09b513ca
    resourceVersion: "13231739"
    uid: c7ba2757-c2af-4a34-9b4b-f344c079726c
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: airflow
        component: triggerer
        pod-template-hash: 54b68d597f
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 7b5b4b568e7c694c0f44651459436d8876c0d6f337d3876162036cffceec3b68
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: triggerer
          pod-template-hash: 54b68d597f
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow triggerer
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/dumb-init
              - --
              - /entrypoint
              - python
              - -Wignore
              - -c
              - |
                import os
                import sys

                # suppress logs triggered from importing airflow packages
                os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                # shared imports
                try:
                    from airflow.jobs.job import Job
                except ImportError:
                    # `BaseJob` was renamed to `Job` in airflow 2.6.0
                    from airflow.jobs.base_job import BaseJob as Job
                from airflow.utils.db import create_session
                from airflow.utils.net import get_hostname

                # heartbeat check imports
                try:
                    from airflow.jobs.triggerer_job_runner import TriggererJobRunner
                except ImportError:
                    # `TriggererJob` is wrapped by `TriggererJobRunner` since airflow 2.6.0
                    from airflow.jobs.triggerer_job import TriggererJob as TriggererJobRunner

                with create_session() as session:
                    # ensure the TriggererJob with most recent heartbeat for this `hostname` is alive
                    hostname = get_hostname()
                    triggerer_job = session \
                        .query(Job) \
                        .filter_by(job_type=TriggererJobRunner.job_type) \
                        .filter_by(hostname=hostname) \
                        .order_by(Job.latest_heartbeat.desc()) \
                        .limit(1) \
                        .first()
                    if (triggerer_job is not None) and triggerer_job.is_alive():
                        pass
                    else:
                        sys.exit(f"The TriggererJob (id={triggerer_job.id}) for hostname '{hostname}' is not alive")
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: airflow-triggerer
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.11
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.11
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-logs
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "24"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-14T04:56:23Z"
    generation: 2
    labels:
      app: airflow
      component: triggerer
      pod-template-hash: 54f6d98786
      release: airflow
    name: airflow-triggerer-54f6d98786
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-triggerer
      uid: df1ec31d-c210-44a1-8a99-70bd09b513ca
    resourceVersion: "13231749"
    uid: 715f17e2-72c1-486b-b395-f9b155474965
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: triggerer
        pod-template-hash: 54f6d98786
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 5928424b76843c48d9e503eecfd8541d5af836b104787689f14c26baf7dee423
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: triggerer
          pod-template-hash: 54f6d98786
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow triggerer
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.10
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/dumb-init
              - --
              - /entrypoint
              - python
              - -Wignore
              - -c
              - |
                import os
                import sys

                # suppress logs triggered from importing airflow packages
                os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                # shared imports
                try:
                    from airflow.jobs.job import Job
                except ImportError:
                    # `BaseJob` was renamed to `Job` in airflow 2.6.0
                    from airflow.jobs.base_job import BaseJob as Job
                from airflow.utils.db import create_session
                from airflow.utils.net import get_hostname

                # heartbeat check imports
                try:
                    from airflow.jobs.triggerer_job_runner import TriggererJobRunner
                except ImportError:
                    # `TriggererJob` is wrapped by `TriggererJobRunner` since airflow 2.6.0
                    from airflow.jobs.triggerer_job import TriggererJob as TriggererJobRunner

                with create_session() as session:
                    # ensure the TriggererJob with most recent heartbeat for this `hostname` is alive
                    hostname = get_hostname()
                    triggerer_job = session \
                        .query(Job) \
                        .filter_by(job_type=TriggererJobRunner.job_type) \
                        .filter_by(hostname=hostname) \
                        .order_by(Job.latest_heartbeat.desc()) \
                        .limit(1) \
                        .first()
                    if (triggerer_job is not None) and triggerer_job.is_alive():
                        pass
                    else:
                        sys.exit(f"The TriggererJob (id={triggerer_job.id}) for hostname '{hostname}' is not alive")
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: airflow-triggerer
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.10
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.10
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-logs
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "17"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-07T10:22:33Z"
    generation: 3
    labels:
      app: airflow
      component: triggerer
      pod-template-hash: 598d4b84d8
      release: airflow
    name: airflow-triggerer-598d4b84d8
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-triggerer
      uid: df1ec31d-c210-44a1-8a99-70bd09b513ca
    resourceVersion: "11331911"
    uid: 55c3919f-5963-4b6d-a961-e181cbb71ea2
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: triggerer
        pod-template-hash: 598d4b84d8
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 379fbef35e65b2d4b781e6b9b9675403a3757805cd7081c59e87fcfe69707a0a
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: triggerer
          pod-template-hash: 598d4b84d8
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow triggerer
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/dumb-init
              - --
              - /entrypoint
              - python
              - -Wignore
              - -c
              - |
                import os
                import sys

                # suppress logs triggered from importing airflow packages
                os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                # shared imports
                try:
                    from airflow.jobs.job import Job
                except ImportError:
                    # `BaseJob` was renamed to `Job` in airflow 2.6.0
                    from airflow.jobs.base_job import BaseJob as Job
                from airflow.utils.db import create_session
                from airflow.utils.net import get_hostname

                # heartbeat check imports
                try:
                    from airflow.jobs.triggerer_job_runner import TriggererJobRunner
                except ImportError:
                    # `TriggererJob` is wrapped by `TriggererJobRunner` since airflow 2.6.0
                    from airflow.jobs.triggerer_job import TriggererJob as TriggererJobRunner

                with create_session() as session:
                    # ensure the TriggererJob with most recent heartbeat for this `hostname` is alive
                    hostname = get_hostname()
                    triggerer_job = session \
                        .query(Job) \
                        .filter_by(job_type=TriggererJobRunner.job_type) \
                        .filter_by(hostname=hostname) \
                        .order_by(Job.latest_heartbeat.desc()) \
                        .limit(1) \
                        .first()
                    if (triggerer_job is not None) and triggerer_job.is_alive():
                        pass
                    else:
                        sys.exit(f"The TriggererJob (id={triggerer_job.id}) for hostname '{hostname}' is not alive")
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: airflow-triggerer
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "21"
      deployment.kubernetes.io/revision-history: "19"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-11T11:22:13Z"
    generation: 4
    labels:
      app: airflow
      component: triggerer
      pod-template-hash: 68c47c7d64
      release: airflow
    name: airflow-triggerer-68c47c7d64
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-triggerer
      uid: df1ec31d-c210-44a1-8a99-70bd09b513ca
    resourceVersion: "12467604"
    uid: a5faf2a1-19d7-4ad8-920b-674254e57ed3
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: triggerer
        pod-template-hash: 68c47c7d64
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 028d706c61a3c89e43897be1e60b3a0a619a3ebc5d272cd25b8ea2d8b7fe444b
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: triggerer
          pod-template-hash: 68c47c7d64
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow triggerer
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/dumb-init
              - --
              - /entrypoint
              - python
              - -Wignore
              - -c
              - |
                import os
                import sys

                # suppress logs triggered from importing airflow packages
                os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                # shared imports
                try:
                    from airflow.jobs.job import Job
                except ImportError:
                    # `BaseJob` was renamed to `Job` in airflow 2.6.0
                    from airflow.jobs.base_job import BaseJob as Job
                from airflow.utils.db import create_session
                from airflow.utils.net import get_hostname

                # heartbeat check imports
                try:
                    from airflow.jobs.triggerer_job_runner import TriggererJobRunner
                except ImportError:
                    # `TriggererJob` is wrapped by `TriggererJobRunner` since airflow 2.6.0
                    from airflow.jobs.triggerer_job import TriggererJob as TriggererJobRunner

                with create_session() as session:
                    # ensure the TriggererJob with most recent heartbeat for this `hostname` is alive
                    hostname = get_hostname()
                    triggerer_job = session \
                        .query(Job) \
                        .filter_by(job_type=TriggererJobRunner.job_type) \
                        .filter_by(hostname=hostname) \
                        .order_by(Job.latest_heartbeat.desc()) \
                        .limit(1) \
                        .first()
                    if (triggerer_job is not None) and triggerer_job.is_alive():
                        pass
                    else:
                        sys.exit(f"The TriggererJob (id={triggerer_job.id}) for hostname '{hostname}' is not alive")
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: airflow-triggerer
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "true"
          - name: GIT_SSH_KNOWN_HOSTS_FILE
            value: /etc/git-secret/known_hosts
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
          - mountPath: /etc/git-secret/known_hosts
            name: git-known-hosts
            readOnly: true
            subPath: known_hosts
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "true"
          - name: GIT_SSH_KNOWN_HOSTS_FILE
            value: /etc/git-secret/known_hosts
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
          - mountPath: /etc/git-secret/known_hosts
            name: git-known-hosts
            readOnly: true
            subPath: known_hosts
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: git-known-hosts
          secret:
            defaultMode: 420
            secretName: airflow-known-hosts
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "16"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-07T10:16:58Z"
    generation: 2
    labels:
      app: airflow
      component: triggerer
      pod-template-hash: 68fcf8f74d
      release: airflow
    name: airflow-triggerer-68fcf8f74d
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-triggerer
      uid: df1ec31d-c210-44a1-8a99-70bd09b513ca
    resourceVersion: "11322684"
    uid: 7251c207-d835-4828-acd8-205d91d14b28
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: triggerer
        pod-template-hash: 68fcf8f74d
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 379fbef35e65b2d4b781e6b9b9675403a3757805cd7081c59e87fcfe69707a0a
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: triggerer
          pod-template-hash: 68fcf8f74d
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow triggerer
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/dumb-init
              - --
              - /entrypoint
              - python
              - -Wignore
              - -c
              - |
                import os
                import sys

                # suppress logs triggered from importing airflow packages
                os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                # shared imports
                try:
                    from airflow.jobs.job import Job
                except ImportError:
                    # `BaseJob` was renamed to `Job` in airflow 2.6.0
                    from airflow.jobs.base_job import BaseJob as Job
                from airflow.utils.db import create_session
                from airflow.utils.net import get_hostname

                # heartbeat check imports
                try:
                    from airflow.jobs.triggerer_job_runner import TriggererJobRunner
                except ImportError:
                    # `TriggererJob` is wrapped by `TriggererJobRunner` since airflow 2.6.0
                    from airflow.jobs.triggerer_job import TriggererJob as TriggererJobRunner

                with create_session() as session:
                    # ensure the TriggererJob with most recent heartbeat for this `hostname` is alive
                    hostname = get_hostname()
                    triggerer_job = session \
                        .query(Job) \
                        .filter_by(job_type=TriggererJobRunner.job_type) \
                        .filter_by(hostname=hostname) \
                        .order_by(Job.latest_heartbeat.desc()) \
                        .limit(1) \
                        .first()
                    if (triggerer_job is not None) and triggerer_job.is_alive():
                        pass
                    else:
                        sys.exit(f"The TriggererJob (id={triggerer_job.id}) for hostname '{hostname}' is not alive")
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: airflow-triggerer
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "true"
          - name: GIT_SSH_KNOWN_HOSTS_FILE
            value: /etc/git-secret/known_hosts
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
          - mountPath: /etc/git-secret/known_hosts
            name: git-known-hosts
            readOnly: true
            subPath: known_hosts
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "true"
          - name: GIT_SSH_KNOWN_HOSTS_FILE
            value: /etc/git-secret/known_hosts
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
          - mountPath: /etc/git-secret/known_hosts
            name: git-known-hosts
            readOnly: true
            subPath: known_hosts
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: git-known-hosts
          secret:
            defaultMode: 420
            secretName: airflow-known-hosts
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "20"
      deployment.kubernetes.io/revision-history: "18"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-07T11:04:50Z"
    generation: 2
    labels:
      app: airflow
      component: triggerer
      pod-template-hash: 6d5548f64d
      release: airflow
    name: airflow-triggerer-6d5548f64d
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-triggerer
      uid: df1ec31d-c210-44a1-8a99-70bd09b513ca
    resourceVersion: "12456228"
    uid: baf5cf3c-6a44-4de2-850c-bb0b1adcf551
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: triggerer
        pod-template-hash: 6d5548f64d
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 028d706c61a3c89e43897be1e60b3a0a619a3ebc5d272cd25b8ea2d8b7fe444b
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: triggerer
          pod-template-hash: 6d5548f64d
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow triggerer
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/dumb-init
              - --
              - /entrypoint
              - python
              - -Wignore
              - -c
              - |
                import os
                import sys

                # suppress logs triggered from importing airflow packages
                os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                # shared imports
                try:
                    from airflow.jobs.job import Job
                except ImportError:
                    # `BaseJob` was renamed to `Job` in airflow 2.6.0
                    from airflow.jobs.base_job import BaseJob as Job
                from airflow.utils.db import create_session
                from airflow.utils.net import get_hostname

                # heartbeat check imports
                try:
                    from airflow.jobs.triggerer_job_runner import TriggererJobRunner
                except ImportError:
                    # `TriggererJob` is wrapped by `TriggererJobRunner` since airflow 2.6.0
                    from airflow.jobs.triggerer_job import TriggererJob as TriggererJobRunner

                with create_session() as session:
                    # ensure the TriggererJob with most recent heartbeat for this `hostname` is alive
                    hostname = get_hostname()
                    triggerer_job = session \
                        .query(Job) \
                        .filter_by(job_type=TriggererJobRunner.job_type) \
                        .filter_by(hostname=hostname) \
                        .order_by(Job.latest_heartbeat.desc()) \
                        .limit(1) \
                        .first()
                    if (triggerer_job is not None) and triggerer_job.is_alive():
                        pass
                    else:
                        sys.exit(f"The TriggererJob (id={triggerer_job.id}) for hostname '{hostname}' is not alive")
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: airflow-triggerer
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "13"
      deployment.kubernetes.io/revision-history: "11"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-04T11:40:35Z"
    generation: 2
    labels:
      app: airflow
      component: triggerer
      pod-template-hash: 75bb78fd86
      release: airflow
    name: airflow-triggerer-75bb78fd86
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-triggerer
      uid: df1ec31d-c210-44a1-8a99-70bd09b513ca
    resourceVersion: "11089326"
    uid: a05de78a-2e95-4f74-ae99-5744f485265c
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: triggerer
        pod-template-hash: 75bb78fd86
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: fe44ec2fcd31f346e7df5c06847f85e9f2fe78b8c87a596e234ca473034f72eb
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: triggerer
          pod-template-hash: 75bb78fd86
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow triggerer
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/dumb-init
              - --
              - /entrypoint
              - python
              - -Wignore
              - -c
              - |
                import os
                import sys

                # suppress logs triggered from importing airflow packages
                os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                # shared imports
                try:
                    from airflow.jobs.job import Job
                except ImportError:
                    # `BaseJob` was renamed to `Job` in airflow 2.6.0
                    from airflow.jobs.base_job import BaseJob as Job
                from airflow.utils.db import create_session
                from airflow.utils.net import get_hostname

                # heartbeat check imports
                try:
                    from airflow.jobs.triggerer_job_runner import TriggererJobRunner
                except ImportError:
                    # `TriggererJob` is wrapped by `TriggererJobRunner` since airflow 2.6.0
                    from airflow.jobs.triggerer_job import TriggererJob as TriggererJobRunner

                with create_session() as session:
                    # ensure the TriggererJob with most recent heartbeat for this `hostname` is alive
                    hostname = get_hostname()
                    triggerer_job = session \
                        .query(Job) \
                        .filter_by(job_type=TriggererJobRunner.job_type) \
                        .filter_by(hostname=hostname) \
                        .order_by(Job.latest_heartbeat.desc()) \
                        .limit(1) \
                        .first()
                    if (triggerer_job is not None) and triggerer_job.is_alive():
                        pass
                    else:
                        sys.exit(f"The TriggererJob (id={triggerer_job.id}) for hostname '{hostname}' is not alive")
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: airflow-triggerer
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - bash
          - -c
          - |
            unset PYTHONUSERBASE && \
            pip freeze | grep -i -e "apache-airflow=="  > protected-packages.txt && \
            pip install --constraint ./protected-packages.txt --user "pymssql" "apache-airflow-providers-samba" "smbprotocol" "psycopg2-binary" "sqlalchemy" "pyodbc"  && \
            echo "copying '/home/airflow/.local/' to '/opt/home-airflow-local/.local/'..." && \
            rsync -ah --stats --delete /home/airflow/.local/ /opt/home-airflow-local/.local/
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: install-pip-packages
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/home-airflow-local
            name: home-airflow-local
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - name: dags-data
          persistentVolumeClaim:
            claimName: airflow-dags
        - emptyDir: {}
          name: logs-data
        - emptyDir: {}
          name: home-airflow-local
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "22"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-11T12:50:40Z"
    generation: 2
    labels:
      app: airflow
      component: triggerer
      pod-template-hash: 75f5f8567f
      release: airflow
    name: airflow-triggerer-75f5f8567f
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-triggerer
      uid: df1ec31d-c210-44a1-8a99-70bd09b513ca
    resourceVersion: "13222419"
    uid: 902994d5-5ced-4022-b9bb-6f44da15d2e3
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: triggerer
        pod-template-hash: 75f5f8567f
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 028d706c61a3c89e43897be1e60b3a0a619a3ebc5d272cd25b8ea2d8b7fe444b
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: triggerer
          pod-template-hash: 75f5f8567f
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow triggerer
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/dumb-init
              - --
              - /entrypoint
              - python
              - -Wignore
              - -c
              - |
                import os
                import sys

                # suppress logs triggered from importing airflow packages
                os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                # shared imports
                try:
                    from airflow.jobs.job import Job
                except ImportError:
                    # `BaseJob` was renamed to `Job` in airflow 2.6.0
                    from airflow.jobs.base_job import BaseJob as Job
                from airflow.utils.db import create_session
                from airflow.utils.net import get_hostname

                # heartbeat check imports
                try:
                    from airflow.jobs.triggerer_job_runner import TriggererJobRunner
                except ImportError:
                    # `TriggererJob` is wrapped by `TriggererJobRunner` since airflow 2.6.0
                    from airflow.jobs.triggerer_job import TriggererJob as TriggererJobRunner

                with create_session() as session:
                    # ensure the TriggererJob with most recent heartbeat for this `hostname` is alive
                    hostname = get_hostname()
                    triggerer_job = session \
                        .query(Job) \
                        .filter_by(job_type=TriggererJobRunner.job_type) \
                        .filter_by(hostname=hostname) \
                        .order_by(Job.latest_heartbeat.desc()) \
                        .limit(1) \
                        .first()
                    if (triggerer_job is not None) and triggerer_job.is_alive():
                        pass
                    else:
                        sys.exit(f"The TriggererJob (id={triggerer_job.id}) for hostname '{hostname}' is not alive")
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: airflow-triggerer
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-logs
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "23"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-14T04:28:34Z"
    generation: 2
    labels:
      app: airflow
      component: triggerer
      pod-template-hash: 79cf86f955
      release: airflow
    name: airflow-triggerer-79cf86f955
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-triggerer
      uid: df1ec31d-c210-44a1-8a99-70bd09b513ca
    resourceVersion: "13228273"
    uid: ca25325a-0d29-4abe-9ff6-ab783ce95dab
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: triggerer
        pod-template-hash: 79cf86f955
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 379fbef35e65b2d4b781e6b9b9675403a3757805cd7081c59e87fcfe69707a0a
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: triggerer
          pod-template-hash: 79cf86f955
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow triggerer
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/dumb-init
              - --
              - /entrypoint
              - python
              - -Wignore
              - -c
              - |
                import os
                import sys

                # suppress logs triggered from importing airflow packages
                os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                # shared imports
                try:
                    from airflow.jobs.job import Job
                except ImportError:
                    # `BaseJob` was renamed to `Job` in airflow 2.6.0
                    from airflow.jobs.base_job import BaseJob as Job
                from airflow.utils.db import create_session
                from airflow.utils.net import get_hostname

                # heartbeat check imports
                try:
                    from airflow.jobs.triggerer_job_runner import TriggererJobRunner
                except ImportError:
                    # `TriggererJob` is wrapped by `TriggererJobRunner` since airflow 2.6.0
                    from airflow.jobs.triggerer_job import TriggererJob as TriggererJobRunner

                with create_session() as session:
                    # ensure the TriggererJob with most recent heartbeat for this `hostname` is alive
                    hostname = get_hostname()
                    triggerer_job = session \
                        .query(Job) \
                        .filter_by(job_type=TriggererJobRunner.job_type) \
                        .filter_by(hostname=hostname) \
                        .order_by(Job.latest_heartbeat.desc()) \
                        .limit(1) \
                        .first()
                    if (triggerer_job is not None) and triggerer_job.is_alive():
                        pass
                    else:
                        sys.exit(f"The TriggererJob (id={triggerer_job.id}) for hostname '{hostname}' is not alive")
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: airflow-triggerer
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-logs
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "14"
      deployment.kubernetes.io/revision-history: "12"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-06T12:59:46Z"
    generation: 4
    labels:
      app: airflow
      component: triggerer
      pod-template-hash: 7d774b5668
      release: airflow
    name: airflow-triggerer-7d774b5668
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-triggerer
      uid: df1ec31d-c210-44a1-8a99-70bd09b513ca
    resourceVersion: "11088648"
    uid: a991e227-a684-4b65-bec2-377bb75b3070
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: triggerer
        pod-template-hash: 7d774b5668
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 379fbef35e65b2d4b781e6b9b9675403a3757805cd7081c59e87fcfe69707a0a
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: triggerer
          pod-template-hash: 7d774b5668
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow triggerer
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/dumb-init
              - --
              - /entrypoint
              - python
              - -Wignore
              - -c
              - |
                import os
                import sys

                # suppress logs triggered from importing airflow packages
                os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                # shared imports
                try:
                    from airflow.jobs.job import Job
                except ImportError:
                    # `BaseJob` was renamed to `Job` in airflow 2.6.0
                    from airflow.jobs.base_job import BaseJob as Job
                from airflow.utils.db import create_session
                from airflow.utils.net import get_hostname

                # heartbeat check imports
                try:
                    from airflow.jobs.triggerer_job_runner import TriggererJobRunner
                except ImportError:
                    # `TriggererJob` is wrapped by `TriggererJobRunner` since airflow 2.6.0
                    from airflow.jobs.triggerer_job import TriggererJob as TriggererJobRunner

                with create_session() as session:
                    # ensure the TriggererJob with most recent heartbeat for this `hostname` is alive
                    hostname = get_hostname()
                    triggerer_job = session \
                        .query(Job) \
                        .filter_by(job_type=TriggererJobRunner.job_type) \
                        .filter_by(hostname=hostname) \
                        .order_by(Job.latest_heartbeat.desc()) \
                        .limit(1) \
                        .first()
                    if (triggerer_job is not None) and triggerer_job.is_alive():
                        pass
                    else:
                        sys.exit(f"The TriggererJob (id={triggerer_job.id}) for hostname '{hostname}' is not alive")
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: airflow-triggerer
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: id_rsa_base64
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - bash
          - -c
          - |
            unset PYTHONUSERBASE && \
            pip freeze | grep -i -e "apache-airflow=="  > protected-packages.txt && \
            pip install --constraint ./protected-packages.txt --user "pymssql" "apache-airflow-providers-samba" "smbprotocol" "psycopg2-binary" "sqlalchemy" "pyodbc"  && \
            echo "copying '/home/airflow/.local/' to '/opt/home-airflow-local/.local/'..." && \
            rsync -ah --stats --delete /home/airflow/.local/ /opt/home-airflow-local/.local/
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: install-pip-packages
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/home-airflow-local
            name: home-airflow-local
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: id_rsa_base64
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - emptyDir: {}
          name: home-airflow-local
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "15"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-06T14:18:10Z"
    generation: 3
    labels:
      app: airflow
      component: triggerer
      pod-template-hash: 848d774cf8
      release: airflow
    name: airflow-triggerer-848d774cf8
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-triggerer
      uid: df1ec31d-c210-44a1-8a99-70bd09b513ca
    resourceVersion: "11323052"
    uid: 585bb279-754f-4a3c-8b71-507b2b45d4d1
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: triggerer
        pod-template-hash: 848d774cf8
        release: airflow
    template:
      metadata:
        annotations:
          checksum/secret-config-envs: 379fbef35e65b2d4b781e6b9b9675403a3757805cd7081c59e87fcfe69707a0a
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: triggerer
          pod-template-hash: 848d774cf8
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow triggerer
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/dumb-init
              - --
              - /entrypoint
              - python
              - -Wignore
              - -c
              - |
                import os
                import sys

                # suppress logs triggered from importing airflow packages
                os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                # shared imports
                try:
                    from airflow.jobs.job import Job
                except ImportError:
                    # `BaseJob` was renamed to `Job` in airflow 2.6.0
                    from airflow.jobs.base_job import BaseJob as Job
                from airflow.utils.db import create_session
                from airflow.utils.net import get_hostname

                # heartbeat check imports
                try:
                    from airflow.jobs.triggerer_job_runner import TriggererJobRunner
                except ImportError:
                    # `TriggererJob` is wrapped by `TriggererJobRunner` since airflow 2.6.0
                    from airflow.jobs.triggerer_job import TriggererJob as TriggererJobRunner

                with create_session() as session:
                    # ensure the TriggererJob with most recent heartbeat for this `hostname` is alive
                    hostname = get_hostname()
                    triggerer_job = session \
                        .query(Job) \
                        .filter_by(job_type=TriggererJobRunner.job_type) \
                        .filter_by(hostname=hostname) \
                        .order_by(Job.latest_heartbeat.desc()) \
                        .limit(1) \
                        .first()
                    if (triggerer_job is not None) and triggerer_job.is_alive():
                        pass
                    else:
                        sys.exit(f"The TriggererJob (id={triggerer_job.id}) for hostname '{hostname}' is not alive")
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 60
          name: airflow-triggerer
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - bash
          - -c
          - |
            unset PYTHONUSERBASE && \
            pip freeze | grep -i -e "apache-airflow=="  > protected-packages.txt && \
            pip install --constraint ./protected-packages.txt --user "pymssql" "apache-airflow-providers-samba" "smbprotocol" "psycopg2-binary" "sqlalchemy" "pyodbc"  && \
            echo "copying '/home/airflow/.local/' to '/opt/home-airflow-local/.local/'..." && \
            rsync -ah --stats --delete /home/airflow/.local/ /opt/home-airflow-local/.local/
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: install-pip-packages
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/home-airflow-local
            name: home-airflow-local
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - emptyDir: {}
          name: home-airflow-local
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "16"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-07T10:16:59Z"
    generation: 2
    labels:
      app: airflow
      component: web
      pod-template-hash: 54b5447499
      release: airflow
    name: airflow-web-54b5447499
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-web
      uid: 4522bcf9-665c-4869-9368-7c5ca6500338
    resourceVersion: "11322687"
    uid: 45f50dd6-b0a2-41c7-a5de-e760796d32ea
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: web
        pod-template-hash: 54b5447499
        release: airflow
    template:
      metadata:
        annotations:
          checksum/config-webserver-config: 08f3293f0aaa3978bcbcae61eee58760bcba270cf691c5ff9c788d678ba8d579
          checksum/secret-config-envs: 379fbef35e65b2d4b781e6b9b9675403a3757805cd7081c59e87fcfe69707a0a
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: web
          pod-template-hash: 54b5447499
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow webserver
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /airflow/health
              port: web
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: airflow-web
          ports:
          - containerPort: 8080
            name: web
            protocol: TCP
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /airflow/health
              port: web
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /opt/airflow/webserver_config.py
            name: webserver-config
            readOnly: true
            subPath: webserver_config.py
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "true"
          - name: GIT_SSH_KNOWN_HOSTS_FILE
            value: /etc/git-secret/known_hosts
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
          - mountPath: /etc/git-secret/known_hosts
            name: git-known-hosts
            readOnly: true
            subPath: known_hosts
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "true"
          - name: GIT_SSH_KNOWN_HOSTS_FILE
            value: /etc/git-secret/known_hosts
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
          - mountPath: /etc/git-secret/known_hosts
            name: git-known-hosts
            readOnly: true
            subPath: known_hosts
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: git-known-hosts
          secret:
            defaultMode: 420
            secretName: airflow-known-hosts
        - name: webserver-config
          secret:
            defaultMode: 420
            secretName: airflow-webserver-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "21"
      deployment.kubernetes.io/revision-history: "19"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-11T11:22:13Z"
    generation: 4
    labels:
      app: airflow
      component: web
      pod-template-hash: 57d8677787
      release: airflow
    name: airflow-web-57d8677787
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-web
      uid: 4522bcf9-665c-4869-9368-7c5ca6500338
    resourceVersion: "12467801"
    uid: 466f1a7e-a64b-4e58-b2e7-2a2784f3f9ab
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: web
        pod-template-hash: 57d8677787
        release: airflow
    template:
      metadata:
        annotations:
          checksum/config-webserver-config: 08f3293f0aaa3978bcbcae61eee58760bcba270cf691c5ff9c788d678ba8d579
          checksum/secret-config-envs: 028d706c61a3c89e43897be1e60b3a0a619a3ebc5d272cd25b8ea2d8b7fe444b
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: web
          pod-template-hash: 57d8677787
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow webserver
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /airflow/health
              port: web
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: airflow-web
          ports:
          - containerPort: 8080
            name: web
            protocol: TCP
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /airflow/health
              port: web
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /opt/airflow/webserver_config.py
            name: webserver-config
            readOnly: true
            subPath: webserver_config.py
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "true"
          - name: GIT_SSH_KNOWN_HOSTS_FILE
            value: /etc/git-secret/known_hosts
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
          - mountPath: /etc/git-secret/known_hosts
            name: git-known-hosts
            readOnly: true
            subPath: known_hosts
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "true"
          - name: GIT_SSH_KNOWN_HOSTS_FILE
            value: /etc/git-secret/known_hosts
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
          - mountPath: /etc/git-secret/known_hosts
            name: git-known-hosts
            readOnly: true
            subPath: known_hosts
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: git-known-hosts
          secret:
            defaultMode: 420
            secretName: airflow-known-hosts
        - name: webserver-config
          secret:
            defaultMode: 420
            secretName: airflow-webserver-config
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "17"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-07T10:22:33Z"
    generation: 3
    labels:
      app: airflow
      component: web
      pod-template-hash: 59966ccd67
      release: airflow
    name: airflow-web-59966ccd67
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-web
      uid: 4522bcf9-665c-4869-9368-7c5ca6500338
    resourceVersion: "11332065"
    uid: 8b158e06-f6f5-47a3-b891-392502add879
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: web
        pod-template-hash: 59966ccd67
        release: airflow
    template:
      metadata:
        annotations:
          checksum/config-webserver-config: 08f3293f0aaa3978bcbcae61eee58760bcba270cf691c5ff9c788d678ba8d579
          checksum/secret-config-envs: 379fbef35e65b2d4b781e6b9b9675403a3757805cd7081c59e87fcfe69707a0a
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: web
          pod-template-hash: 59966ccd67
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow webserver
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /airflow/health
              port: web
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: airflow-web
          ports:
          - containerPort: 8080
            name: web
            protocol: TCP
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /airflow/health
              port: web
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /opt/airflow/webserver_config.py
            name: webserver-config
            readOnly: true
            subPath: webserver_config.py
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: webserver-config
          secret:
            defaultMode: 420
            secretName: airflow-webserver-config
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "22"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-11T12:50:40Z"
    generation: 2
    labels:
      app: airflow
      component: web
      pod-template-hash: 5bd65db946
      release: airflow
    name: airflow-web-5bd65db946
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-web
      uid: 4522bcf9-665c-4869-9368-7c5ca6500338
    resourceVersion: "13222696"
    uid: 82fcb425-212e-4fd8-ba0a-18ca7c49028a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: web
        pod-template-hash: 5bd65db946
        release: airflow
    template:
      metadata:
        annotations:
          checksum/config-webserver-config: 08f3293f0aaa3978bcbcae61eee58760bcba270cf691c5ff9c788d678ba8d579
          checksum/secret-config-envs: 028d706c61a3c89e43897be1e60b3a0a619a3ebc5d272cd25b8ea2d8b7fe444b
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: web
          pod-template-hash: 5bd65db946
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow webserver
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /airflow/health
              port: web
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: airflow-web
          ports:
          - containerPort: 8080
            name: web
            protocol: TCP
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /airflow/health
              port: web
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /opt/airflow/webserver_config.py
            name: webserver-config
            readOnly: true
            subPath: webserver_config.py
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-logs
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: webserver-config
          secret:
            defaultMode: 420
            secretName: airflow-webserver-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "24"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-14T04:56:23Z"
    generation: 2
    labels:
      app: airflow
      component: web
      pod-template-hash: 5c5cb8dbf5
      release: airflow
    name: airflow-web-5c5cb8dbf5
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-web
      uid: 4522bcf9-665c-4869-9368-7c5ca6500338
    resourceVersion: "13231999"
    uid: 1b1173f1-4b02-42c1-9d85-5edfd374c465
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: web
        pod-template-hash: 5c5cb8dbf5
        release: airflow
    template:
      metadata:
        annotations:
          checksum/config-webserver-config: 08f3293f0aaa3978bcbcae61eee58760bcba270cf691c5ff9c788d678ba8d579
          checksum/secret-config-envs: 5928424b76843c48d9e503eecfd8541d5af836b104787689f14c26baf7dee423
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: web
          pod-template-hash: 5c5cb8dbf5
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow webserver
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.10
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /airflow/health
              port: web
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: airflow-web
          ports:
          - containerPort: 8080
            name: web
            protocol: TCP
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /airflow/health
              port: web
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /opt/airflow/webserver_config.py
            name: webserver-config
            readOnly: true
            subPath: webserver_config.py
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.10
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.10
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-logs
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: webserver-config
          secret:
            defaultMode: 420
            secretName: airflow-webserver-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "25"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-14T05:12:12Z"
    generation: 1
    labels:
      app: airflow
      component: web
      pod-template-hash: 5c757c7656
      release: airflow
    name: airflow-web-5c757c7656
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-web
      uid: 4522bcf9-665c-4869-9368-7c5ca6500338
    resourceVersion: "13231987"
    uid: 0f27fcb2-ba16-47b9-81bc-2141939535a1
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: airflow
        component: web
        pod-template-hash: 5c757c7656
        release: airflow
    template:
      metadata:
        annotations:
          checksum/config-webserver-config: 08f3293f0aaa3978bcbcae61eee58760bcba270cf691c5ff9c788d678ba8d579
          checksum/secret-config-envs: 7b5b4b568e7c694c0f44651459436d8876c0d6f337d3876162036cffceec3b68
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: web
          pod-template-hash: 5c757c7656
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow webserver
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /airflow/health
              port: web
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: airflow-web
          ports:
          - containerPort: 8080
            name: web
            protocol: TCP
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /airflow/health
              port: web
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /opt/airflow/webserver_config.py
            name: webserver-config
            readOnly: true
            subPath: webserver_config.py
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.11
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.11
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-logs
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: webserver-config
          secret:
            defaultMode: 420
            secretName: airflow-webserver-config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "13"
      deployment.kubernetes.io/revision-history: "11"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-04T11:40:35Z"
    generation: 2
    labels:
      app: airflow
      component: web
      pod-template-hash: 5ccd49fbbf
      release: airflow
    name: airflow-web-5ccd49fbbf
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-web
      uid: 4522bcf9-665c-4869-9368-7c5ca6500338
    resourceVersion: "11089590"
    uid: fc3a0949-7a21-4212-acef-02cad6a2d6a6
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: web
        pod-template-hash: 5ccd49fbbf
        release: airflow
    template:
      metadata:
        annotations:
          checksum/config-webserver-config: 08f3293f0aaa3978bcbcae61eee58760bcba270cf691c5ff9c788d678ba8d579
          checksum/secret-config-envs: fe44ec2fcd31f346e7df5c06847f85e9f2fe78b8c87a596e234ca473034f72eb
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: web
          pod-template-hash: 5ccd49fbbf
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow webserver
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /airflow/health
              port: web
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: airflow-web
          ports:
          - containerPort: 8080
            name: web
            protocol: TCP
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /airflow/health
              port: web
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
          - mountPath: /opt/airflow/webserver_config.py
            name: webserver-config
            readOnly: true
            subPath: webserver_config.py
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - bash
          - -c
          - |
            unset PYTHONUSERBASE && \
            pip freeze | grep -i -e "apache-airflow=="  > protected-packages.txt && \
            pip install --constraint ./protected-packages.txt --user "pymssql" "apache-airflow-providers-samba" "smbprotocol" "psycopg2-binary" "sqlalchemy" "pyodbc"  && \
            echo "copying '/home/airflow/.local/' to '/opt/home-airflow-local/.local/'..." && \
            rsync -ah --stats --delete /home/airflow/.local/ /opt/home-airflow-local/.local/
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: install-pip-packages
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/home-airflow-local
            name: home-airflow-local
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - name: dags-data
          persistentVolumeClaim:
            claimName: airflow-dags
        - emptyDir: {}
          name: logs-data
        - emptyDir: {}
          name: home-airflow-local
        - name: webserver-config
          secret:
            defaultMode: 420
            secretName: airflow-webserver-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "14"
      deployment.kubernetes.io/revision-history: "12"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-06T12:59:47Z"
    generation: 4
    labels:
      app: airflow
      component: web
      pod-template-hash: 66989c98f
      release: airflow
    name: airflow-web-66989c98f
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-web
      uid: 4522bcf9-665c-4869-9368-7c5ca6500338
    resourceVersion: "11088655"
    uid: 9cc1a9c4-68c2-46fb-9a02-97aae2cc76ca
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: web
        pod-template-hash: 66989c98f
        release: airflow
    template:
      metadata:
        annotations:
          checksum/config-webserver-config: 08f3293f0aaa3978bcbcae61eee58760bcba270cf691c5ff9c788d678ba8d579
          checksum/secret-config-envs: 379fbef35e65b2d4b781e6b9b9675403a3757805cd7081c59e87fcfe69707a0a
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: web
          pod-template-hash: 66989c98f
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow webserver
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /airflow/health
              port: web
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: airflow-web
          ports:
          - containerPort: 8080
            name: web
            protocol: TCP
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /airflow/health
              port: web
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
          - mountPath: /opt/airflow/webserver_config.py
            name: webserver-config
            readOnly: true
            subPath: webserver_config.py
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: id_rsa_base64
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - bash
          - -c
          - |
            unset PYTHONUSERBASE && \
            pip freeze | grep -i -e "apache-airflow=="  > protected-packages.txt && \
            pip install --constraint ./protected-packages.txt --user "pymssql" "apache-airflow-providers-samba" "smbprotocol" "psycopg2-binary" "sqlalchemy" "pyodbc"  && \
            echo "copying '/home/airflow/.local/' to '/opt/home-airflow-local/.local/'..." && \
            rsync -ah --stats --delete /home/airflow/.local/ /opt/home-airflow-local/.local/
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: install-pip-packages
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/home-airflow-local
            name: home-airflow-local
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: id_rsa_base64
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - emptyDir: {}
          name: home-airflow-local
        - name: webserver-config
          secret:
            defaultMode: 420
            secretName: airflow-webserver-config
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "23"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-14T04:28:34Z"
    generation: 2
    labels:
      app: airflow
      component: web
      pod-template-hash: 8459bf468b
      release: airflow
    name: airflow-web-8459bf468b
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-web
      uid: 4522bcf9-665c-4869-9368-7c5ca6500338
    resourceVersion: "13228820"
    uid: 6143766f-a6a8-456b-8699-eb28f401bbbc
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: web
        pod-template-hash: 8459bf468b
        release: airflow
    template:
      metadata:
        annotations:
          checksum/config-webserver-config: 08f3293f0aaa3978bcbcae61eee58760bcba270cf691c5ff9c788d678ba8d579
          checksum/secret-config-envs: 379fbef35e65b2d4b781e6b9b9675403a3757805cd7081c59e87fcfe69707a0a
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: web
          pod-template-hash: 8459bf468b
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow webserver
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /airflow/health
              port: web
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: airflow-web
          ports:
          - containerPort: 8080
            name: web
            protocol: TCP
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /airflow/health
              port: web
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /opt/airflow/webserver_config.py
            name: webserver-config
            readOnly: true
            subPath: webserver_config.py
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-logs
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: webserver-config
          secret:
            defaultMode: 420
            secretName: airflow-webserver-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "15"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-06T14:18:10Z"
    generation: 3
    labels:
      app: airflow
      component: web
      pod-template-hash: bdb6d84c7
      release: airflow
    name: airflow-web-bdb6d84c7
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-web
      uid: 4522bcf9-665c-4869-9368-7c5ca6500338
    resourceVersion: "11323465"
    uid: 45e587c7-4d8c-4615-a136-639294e6364c
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: web
        pod-template-hash: bdb6d84c7
        release: airflow
    template:
      metadata:
        annotations:
          checksum/config-webserver-config: 08f3293f0aaa3978bcbcae61eee58760bcba270cf691c5ff9c788d678ba8d579
          checksum/secret-config-envs: 379fbef35e65b2d4b781e6b9b9675403a3757805cd7081c59e87fcfe69707a0a
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: web
          pod-template-hash: bdb6d84c7
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow webserver
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /airflow/health
              port: web
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: airflow-web
          ports:
          - containerPort: 8080
            name: web
            protocol: TCP
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /airflow/health
              port: web
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
          - mountPath: /opt/airflow/webserver_config.py
            name: webserver-config
            readOnly: true
            subPath: webserver_config.py
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - bash
          - -c
          - |
            unset PYTHONUSERBASE && \
            pip freeze | grep -i -e "apache-airflow=="  > protected-packages.txt && \
            pip install --constraint ./protected-packages.txt --user "pymssql" "apache-airflow-providers-samba" "smbprotocol" "psycopg2-binary" "sqlalchemy" "pyodbc"  && \
            echo "copying '/home/airflow/.local/' to '/opt/home-airflow-local/.local/'..." && \
            rsync -ah --stats --delete /home/airflow/.local/ /opt/home-airflow-local/.local/
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: install-pip-packages
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/home-airflow-local
            name: home-airflow-local
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /home/airflow/.local
            name: home-airflow-local
            subPath: .local
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - emptyDir: {}
          name: home-airflow-local
        - name: webserver-config
          secret:
            defaultMode: 420
            secretName: airflow-webserver-config
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "20"
      deployment.kubernetes.io/revision-history: "18"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-03-07T11:04:50Z"
    generation: 2
    labels:
      app: airflow
      component: web
      pod-template-hash: cc5d8b495
      release: airflow
    name: airflow-web-cc5d8b495
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-web
      uid: 4522bcf9-665c-4869-9368-7c5ca6500338
    resourceVersion: "12456690"
    uid: 81c15f97-4ecb-45bf-96fd-4ea9e358b150
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: airflow
        component: web
        pod-template-hash: cc5d8b495
        release: airflow
    template:
      metadata:
        annotations:
          checksum/config-webserver-config: 08f3293f0aaa3978bcbcae61eee58760bcba270cf691c5ff9c788d678ba8d579
          checksum/secret-config-envs: 028d706c61a3c89e43897be1e60b3a0a619a3ebc5d272cd25b8ea2d8b7fe444b
          checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: airflow
          component: web
          pod-template-hash: cc5d8b495
          release: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - bash
          - -c
          - exec airflow webserver
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /airflow/health
              port: web
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: airflow-web
          ports:
          - containerPort: 8080
            name: web
            protocol: TCP
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /airflow/health
              port: web
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
          - mountPath: /opt/airflow/webserver_config.py
            name: webserver-config
            readOnly: true
            subPath: webserver_config.py
        - env:
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-sync
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        dnsPolicy: ClusterFirst
        initContainers:
        - env:
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GIT_SYNC_ROOT
            value: /dags
          - name: GIT_SYNC_DEST
            value: repo
          - name: GIT_SYNC_REPO
            value: git@gitlab.neinver.com:rpicadom/data_infrastructure.git
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_WAIT
            value: "60"
          - name: GIT_SYNC_TIMEOUT
            value: "120"
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GIT_SYNC_SUBMODULES
            value: recursive
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/id_rsa
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: registry.k8s.io/git-sync/git-sync:v3.6.5
          imagePullPolicy: IfNotPresent
          name: dags-git-clone
          resources: {}
          securityContext:
            runAsGroup: 65533
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dags
            name: dags-data
          - mountPath: /etc/git-secret/id_rsa
            name: git-secret
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - -c
          - exec timeout 60s airflow db check
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: check-db
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        - args:
          - bash
          - -c
          - exec airflow db check-migrations -t 60
          command:
          - /usr/bin/dumb-init
          - --
          - /entrypoint
          env:
          - name: DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: CONNECTION_CHECK_MAX_COUNT
            value: "0"
          - name: AIRFLOW__METRICS__STATSD_HOST
            value: prometheus-statsd-exporter
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "9125"
          - name: AIRFLOW__METRICS__STATSD_ON
            value: "true"
          envFrom:
          - secretRef:
              name: airflow-config-envs
          image: neinver2024/apacheairflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          name: wait-for-db-migrations
          resources: {}
          securityContext:
            runAsGroup: 0
            runAsUser: 50000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/dags
            name: dags-data
          - mountPath: /opt/airflow/logs
            name: logs-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
        serviceAccount: airflow
        serviceAccountName: airflow
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dags-data
        - emptyDir: {}
          name: logs-data
        - name: git-secret
          secret:
            defaultMode: 420
            secretName: airflow-ssh-secret
        - name: webserver-config
          secret:
            defaultMode: 420
            secretName: airflow-webserver-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: postgresql-ha
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-02-13T12:27:05Z"
    generation: 1
    labels:
      app.kubernetes.io/component: pgpool
      app.kubernetes.io/instance: postgresql-ha
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql-ha
      app.kubernetes.io/version: 4.5.0
      helm.sh/chart: postgresql-ha-13.2.3
      pod-template-hash: 76bcdbc66f
    name: postgresql-ha-pgpool-76bcdbc66f
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: postgresql-ha-pgpool
      uid: c051ad9a-2262-4220-9062-0ee86cd88699
    resourceVersion: "13087970"
    uid: ebef0d66-d9ee-4174-8113-e9c1141107b4
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: pgpool
        app.kubernetes.io/instance: postgresql-ha
        app.kubernetes.io/name: postgresql-ha
        pod-template-hash: 76bcdbc66f
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: pgpool
          app.kubernetes.io/instance: postgresql-ha
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: postgresql-ha
          app.kubernetes.io/version: 4.5.0
          helm.sh/chart: postgresql-ha-13.2.3
          pod-template-hash: 76bcdbc66f
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/component: pgpool
                  app.kubernetes.io/instance: postgresql-ha
                  app.kubernetes.io/name: postgresql-ha
              topologyKey: kubernetes.io/hostname
        automountServiceAccountToken: false
        containers:
        - env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: PGPOOL_BACKEND_NODES
            value: 0:postgresql-ha-postgresql-0.postgresql-ha-postgresql-headless:5432,1:postgresql-ha-postgresql-1.postgresql-ha-postgresql-headless:5432,2:postgresql-ha-postgresql-2.postgresql-ha-postgresql-headless:5432,
          - name: PGPOOL_SR_CHECK_USER
            value: repmgr
          - name: PGPOOL_SR_CHECK_PASSWORD
            valueFrom:
              secretKeyRef:
                key: repmgr-password
                name: postgresql-ha-postgresql
          - name: PGPOOL_SR_CHECK_DATABASE
            value: postgres
          - name: PGPOOL_ENABLE_LDAP
            value: "no"
          - name: PGPOOL_POSTGRES_USERNAME
            value: postgres
          - name: PGPOOL_POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: PGPOOL_ADMIN_USERNAME
            value: admin
          - name: PGPOOL_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: postgresql-ha-pgpool
          - name: PGPOOL_AUTHENTICATION_METHOD
            value: scram-sha-256
          - name: PGPOOL_ENABLE_LOAD_BALANCING
            value: "yes"
          - name: PGPOOL_DISABLE_LOAD_BALANCE_ON_WRITE
            value: transaction
          - name: PGPOOL_ENABLE_LOG_CONNECTIONS
            value: "no"
          - name: PGPOOL_ENABLE_LOG_HOSTNAME
            value: "yes"
          - name: PGPOOL_ENABLE_LOG_PER_NODE_STATEMENT
            value: "no"
          - name: PGPOOL_RESERVED_CONNECTIONS
            value: "1"
          - name: PGPOOL_CHILD_LIFE_TIME
          - name: PGPOOL_ENABLE_TLS
            value: "no"
          - name: PGPOOL_HEALTH_CHECK_PSQL_TIMEOUT
            value: "6"
          image: docker.io/bitnami/pgpool:4.5.0-debian-11-r4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /opt/bitnami/scripts/pgpool/healthcheck.sh
            failureThreshold: 5
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: pgpool
          ports:
          - containerPort: 5432
            name: postgresql
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - bash
              - -ec
              - PGPASSWORD=${PGPOOL_POSTGRES_PASSWORD} psql -U "postgres" -d "airflow"
                -h /opt/bitnami/pgpool/tmp -tA -c "SELECT 1" >/dev/null
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          fsGroupChangePolicy: Always
        serviceAccount: postgresql-ha
        serviceAccountName: postgresql-ha
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: prometheus-statsd-exporter
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-02-20T10:54:44Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: prometheus-statsd-exporter
      app.kubernetes.io/name: prometheus-statsd-exporter
      pod-template-hash: 555dd67764
    name: prometheus-statsd-exporter-555dd67764
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-statsd-exporter
      uid: e2847bef-392a-40fb-b221-24b1c54bbc6c
    resourceVersion: "6548338"
    uid: 13690475-fcbf-48e2-9351-fce30b719f4e
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus-statsd-exporter
        app.kubernetes.io/name: prometheus-statsd-exporter
        pod-template-hash: 555dd67764
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: prometheus-statsd-exporter
          app.kubernetes.io/name: prometheus-statsd-exporter
          pod-template-hash: 555dd67764
      spec:
        containers:
        - args:
          - --web.listen-address=:9102
          - --web.telemetry-path=/metrics
          - --statsd.listen-udp=:9125
          - --statsd.listen-tcp=:9125
          - --statsd.cache-size=1000
          - --statsd.event-queue-size=10000
          - --statsd.event-flush-threshold=1000
          - --statsd.event-flush-interval=200ms
          - --statsd.mapping-config=/etc/prometheus-statsd-exporter/statsd-mapping.conf
          image: prom/statsd-exporter:v0.14.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /metrics
              port: web
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus-statsd-exporter
          ports:
          - containerPort: 9102
            name: web
            protocol: TCP
          - containerPort: 9125
            name: statsd-tcp
            protocol: TCP
          - containerPort: 9125
            name: statsd-udp
            protocol: UDP
          resources: {}
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/prometheus-statsd-exporter
            name: statsd-mapping-config
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: prometheus-statsd-exporter
        serviceAccountName: prometheus-statsd-exporter
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: statsd.mappingConf
              path: statsd-mapping.conf
            name: prometheus-statsd-exporter
          name: statsd-mapping-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: prometheus-statsd-exporter
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-02-20T10:57:37Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: prometheus-statsd-exporter
      app.kubernetes.io/name: prometheus-statsd-exporter
      pod-template-hash: 56d8b89dfd
    name: prometheus-statsd-exporter-56d8b89dfd
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-statsd-exporter
      uid: e2847bef-392a-40fb-b221-24b1c54bbc6c
    resourceVersion: "13082249"
    uid: ab6afa04-eec8-44db-ae8d-f492f25a8629
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus-statsd-exporter
        app.kubernetes.io/name: prometheus-statsd-exporter
        pod-template-hash: 56d8b89dfd
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: prometheus-statsd-exporter
          app.kubernetes.io/name: prometheus-statsd-exporter
          pod-template-hash: 56d8b89dfd
      spec:
        containers:
        - args:
          - --web.listen-address=:9102
          - --web.telemetry-path=/metrics
          - --statsd.listen-udp=:9125
          - --statsd.listen-tcp=:9125
          - --statsd.cache-size=1000
          - --statsd.event-queue-size=10000
          - --statsd.event-flush-threshold=1000
          - --statsd.event-flush-interval=200ms
          - --statsd.mapping-config=/etc/prometheus-statsd-exporter/statsd-mapping.conf
          image: prom/statsd-exporter:v0.26.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /metrics
              port: web
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus-statsd-exporter
          ports:
          - containerPort: 9102
            name: web
            protocol: TCP
          - containerPort: 9125
            name: statsd-tcp
            protocol: TCP
          - containerPort: 9125
            name: statsd-udp
            protocol: UDP
          resources: {}
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/prometheus-statsd-exporter
            name: statsd-mapping-config
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: prometheus-statsd-exporter
        serviceAccountName: prometheus-statsd-exporter
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: statsd.mappingConf
              path: statsd-mapping.conf
            name: prometheus-statsd-exporter
          name: statsd-mapping-config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: prometheus-statsd-exporter
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-02-15T03:44:40Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: prometheus-statsd-exporter
      app.kubernetes.io/name: prometheus-statsd-exporter
      pod-template-hash: 79dbbf4845
    name: prometheus-statsd-exporter-79dbbf4845
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-statsd-exporter
      uid: e2847bef-392a-40fb-b221-24b1c54bbc6c
    resourceVersion: "6548398"
    uid: bf72f78f-c20d-4f38-9374-851032623c87
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus-statsd-exporter
        app.kubernetes.io/name: prometheus-statsd-exporter
        pod-template-hash: 79dbbf4845
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: prometheus-statsd-exporter
          app.kubernetes.io/name: prometheus-statsd-exporter
          pod-template-hash: 79dbbf4845
      spec:
        containers:
        - args:
          - --web.listen-address=:9102
          - --web.telemetry-path=/metrics
          - --statsd.listen-udp=:9125
          - --statsd.listen-tcp=:9125
          - --statsd.cache-size=1000
          - --statsd.event-queue-size=10000
          - --statsd.event-flush-threshold=1000
          - --statsd.event-flush-interval=200ms
          image: prom/statsd-exporter:v0.14.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /metrics
              port: web
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus-statsd-exporter
          ports:
          - containerPort: 9102
            name: web
            protocol: TCP
          - containerPort: 9125
            name: statsd-tcp
            protocol: TCP
          - containerPort: 9125
            name: statsd-udp
            protocol: UDP
          resources: {}
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: prometheus-statsd-exporter
        serviceAccountName: prometheus-statsd-exporter
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-03-13T16:45:36Z"
    generation: 1
    labels:
      apiserver: "true"
      app.kubernetes.io/name: calico-apiserver
      k8s-app: calico-apiserver
      pod-template-hash: fd8fdffff
    name: calico-apiserver-fd8fdffff
    namespace: calico-apiserver
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: calico-apiserver
      uid: ce5075a2-6544-4452-bcb1-f8b513eb02ac
    resourceVersion: "13082595"
    uid: b1650dc9-c891-4ae6-b6c0-0b0795c9f729
  spec:
    replicas: 2
    selector:
      matchLabels:
        apiserver: "true"
        pod-template-hash: fd8fdffff
    template:
      metadata:
        annotations:
          tigera-operator.hash.operator.tigera.io/calico-apiserver-certs: f486d634d991c86be36e698588a498a9965d8a2f
        creationTimestamp: null
        labels:
          apiserver: "true"
          app.kubernetes.io/name: calico-apiserver
          k8s-app: calico-apiserver
          pod-template-hash: fd8fdffff
        name: calico-apiserver
        namespace: calico-apiserver
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    k8s-app: calico-apiserver
                namespaces:
                - calico-apiserver
                topologyKey: kubernetes.io/hostname
              weight: 100
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    k8s-app: calico-apiserver
                namespaces:
                - calico-apiserver
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - args:
          - --secure-port=5443
          - --tls-private-key-file=/calico-apiserver-certs/tls.key
          - --tls-cert-file=/calico-apiserver-certs/tls.crt
          env:
          - name: DATASTORE_TYPE
            value: kubernetes
          - name: KUBERNETES_SERVICE_HOST
            value: 10.43.0.1
          - name: KUBERNETES_SERVICE_PORT
            value: "443"
          - name: MULTI_INTERFACE_MODE
            value: none
          image: docker.io/calico/apiserver:master
          imagePullPolicy: IfNotPresent
          name: calico-apiserver
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 5443
              scheme: HTTPS
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            runAsGroup: 0
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /calico-apiserver-certs
            name: calico-apiserver-certs
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-apiserver
        serviceAccountName: calico-apiserver
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
        volumes:
        - name: calico-apiserver-certs
          secret:
            defaultMode: 420
            secretName: calico-apiserver-certs
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-03-13T16:12:07Z"
    generation: 1
    labels:
      app.kubernetes.io/name: calico-kube-controllers
      k8s-app: calico-kube-controllers
      pod-template-hash: 55bd969cb
    name: calico-kube-controllers-55bd969cb
    namespace: calico-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: calico-kube-controllers
      uid: e72128af-ec6f-4b3a-8fc9-b0460369c249
    resourceVersion: "13075261"
    uid: b918bf76-f7dd-49a1-b41b-f01366d9d9c6
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: calico-kube-controllers
        pod-template-hash: 55bd969cb
    template:
      metadata:
        annotations:
          hash.operator.tigera.io/system: fdde45054a8ae4f629960ce37570929502e59449
          tigera-operator.hash.operator.tigera.io/tigera-ca-private: 4f5267a1bedc8badce3a9288fba115ecfdb158ed
        creationTimestamp: null
        labels:
          app.kubernetes.io/name: calico-kube-controllers
          k8s-app: calico-kube-controllers
          pod-template-hash: 55bd969cb
        name: calico-kube-controllers
        namespace: calico-system
      spec:
        containers:
        - env:
          - name: KUBE_CONTROLLERS_CONFIG_NAME
            value: default
          - name: DATASTORE_TYPE
            value: kubernetes
          - name: ENABLED_CONTROLLERS
            value: node
          - name: FIPS_MODE_ENABLED
            value: "false"
          - name: DISABLE_KUBE_CONTROLLERS_CONFIG_API
            value: "false"
          - name: KUBERNETES_SERVICE_HOST
            value: 10.43.0.1
          - name: KUBERNETES_SERVICE_PORT
            value: "443"
          - name: CA_CRT_PATH
            value: /etc/pki/tls/certs/tigera-ca-bundle.crt
          image: docker.io/calico/kube-controllers:master
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/check-status
              - -l
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 10
          name: calico-kube-controllers
          readinessProbe:
            exec:
              command:
              - /usr/bin/check-status
              - -r
            failureThreshold: 3
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            runAsGroup: 0
            runAsNonRoot: true
            runAsUser: 999
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/pki/tls/certs
            name: tigera-ca-bundle
            readOnly: true
          - mountPath: /etc/pki/tls/cert.pem
            name: tigera-ca-bundle
            readOnly: true
            subPath: ca-bundle.crt
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-kube-controllers
        serviceAccountName: calico-kube-controllers
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: tigera-ca-bundle
          name: tigera-ca-bundle
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "3"
      deployment.kubernetes.io/max-replicas: "6"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-03-13T16:12:07Z"
    generation: 2
    labels:
      app.kubernetes.io/name: calico-typha
      k8s-app: calico-typha
      pod-template-hash: 5d4bd748b9
    name: calico-typha-5d4bd748b9
    namespace: calico-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: calico-typha
      uid: d8a884f0-b9f4-4c67-8e8c-700b30f06e87
    resourceVersion: "13073189"
    uid: 85b27842-6d00-4aab-867a-24f6db7d7d98
  spec:
    replicas: 3
    selector:
      matchLabels:
        k8s-app: calico-typha
        pod-template-hash: 5d4bd748b9
    template:
      metadata:
        annotations:
          hash.operator.tigera.io/system: fdde45054a8ae4f629960ce37570929502e59449
          tigera-operator.hash.operator.tigera.io/tigera-ca-private: 4f5267a1bedc8badce3a9288fba115ecfdb158ed
          tigera-operator.hash.operator.tigera.io/typha-certs: c9d75be38dacbdb51cf4b46c6f5d1bc93a95e3db
        creationTimestamp: null
        labels:
          app.kubernetes.io/name: calico-typha
          k8s-app: calico-typha
          pod-template-hash: 5d4bd748b9
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: k8s-app
                    operator: In
                    values:
                    - calico-typha
                topologyKey: topology.kubernetes.io/zone
              weight: 1
        containers:
        - env:
          - name: TYPHA_LOGSEVERITYSCREEN
            value: info
          - name: TYPHA_LOGFILEPATH
            value: none
          - name: TYPHA_LOGSEVERITYSYS
            value: none
          - name: TYPHA_CONNECTIONREBALANCINGMODE
            value: kubernetes
          - name: TYPHA_DATASTORETYPE
            value: kubernetes
          - name: TYPHA_HEALTHENABLED
            value: "true"
          - name: TYPHA_HEALTHPORT
            value: "9098"
          - name: TYPHA_K8SNAMESPACE
            value: calico-system
          - name: TYPHA_CAFILE
            value: /etc/pki/tls/certs/tigera-ca-bundle.crt
          - name: TYPHA_SERVERCERTFILE
            value: /typha-certs/tls.crt
          - name: TYPHA_SERVERKEYFILE
            value: /typha-certs/tls.key
          - name: TYPHA_FIPSMODEENABLED
            value: "false"
          - name: TYPHA_SHUTDOWNTIMEOUTSECS
            value: "300"
          - name: TYPHA_CLIENTCN
            value: typha-client
          - name: KUBERNETES_SERVICE_HOST
            value: 10.43.0.1
          - name: KUBERNETES_SERVICE_PORT
            value: "443"
          image: docker.io/calico/typha:master
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              host: localhost
              path: /liveness
              port: 9098
              scheme: HTTP
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 10
          name: calico-typha
          ports:
          - containerPort: 5473
            hostPort: 5473
            name: calico-typha
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              host: localhost
              path: /readiness
              port: 9098
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            runAsGroup: 10001
            runAsNonRoot: true
            runAsUser: 10001
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/pki/tls/certs
            name: tigera-ca-bundle
            readOnly: true
          - mountPath: /etc/pki/tls/cert.pem
            name: tigera-ca-bundle
            readOnly: true
            subPath: ca-bundle.crt
          - mountPath: /typha-certs
            name: typha-certs
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-typha
        serviceAccountName: calico-typha
        terminationGracePeriodSeconds: 300
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          operator: Exists
        - effect: NoExecute
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: tigera-ca-bundle
          name: tigera-ca-bundle
        - name: typha-certs
          secret:
            defaultMode: 420
            secretName: typha-certs
  status:
    availableReplicas: 3
    fullyLabeledReplicas: 3
    observedGeneration: 2
    readyReplicas: 3
    replicas: 3
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: ingress-nginx
      meta.helm.sh/release-namespace: ingress-nginx
    creationTimestamp: "2024-02-21T05:23:25Z"
    generation: 1
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.9.6
      helm.sh/chart: ingress-nginx-4.9.1
      pod-template-hash: 699bbd7596
    name: ingress-nginx-controller-699bbd7596
    namespace: ingress-nginx
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: ingress-nginx-controller
      uid: ad4a9ef1-81d9-4bd6-b027-770e7124b256
    resourceVersion: "13085773"
    uid: 76e4dee8-22d9-4298-99f4-b5a14395636f
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
        pod-template-hash: 699bbd7596
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: ingress-nginx
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: ingress-nginx
          app.kubernetes.io/part-of: ingress-nginx
          app.kubernetes.io/version: 1.9.6
          helm.sh/chart: ingress-nginx-4.9.1
          pod-template-hash: 699bbd7596
      spec:
        containers:
        - args:
          - /nginx-ingress-controller
          - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
          - --election-id=ingress-nginx-leader
          - --controller-class=k8s.io/ingress-nginx
          - --ingress-class=nginx
          - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
          - --validating-webhook=:8443
          - --validating-webhook-certificate=/usr/local/certificates/cert
          - --validating-webhook-key=/usr/local/certificates/key
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: LD_PRELOAD
            value: /usr/local/lib/libmimalloc.so
          image: registry.k8s.io/ingress-nginx/controller:v1.9.6@sha256:1405cc613bd95b2c6edd8b2a152510ae91c7e62aea4698500d23b2145960ab9c
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /wait-shutdown
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: controller
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          - containerPort: 443
            name: https
            protocol: TCP
          - containerPort: 8443
            name: webhook
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 90Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 101
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /usr/local/certificates/
            name: webhook-cert
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: ingress-nginx
        serviceAccountName: ingress-nginx
        terminationGracePeriodSeconds: 300
        volumes:
        - name: webhook-cert
          secret:
            defaultMode: 420
            secretName: ingress-nginx-admission
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-01-26T07:18:49Z"
    generation: 4
    labels:
      k8s-app: kube-dns
      pod-template-hash: 66b64c55d4
    name: coredns-66b64c55d4
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: coredns
      uid: 3e7ffa92-455d-46b1-bfd1-4137b69ee321
    resourceVersion: "12984847"
    uid: 41ef22f7-277b-484d-97f7-460f87eb9baa
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: kube-dns
        pod-template-hash: 66b64c55d4
    template:
      metadata:
        annotations:
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
          pod-template-hash: 66b64c55d4
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: node-role.kubernetes.io/worker
                  operator: Exists
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: k8s-app
                    operator: In
                    values:
                    - kube-dns
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: rancher/mirrored-coredns-coredns:1.9.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          beta.kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoExecute
          operator: Exists
        - effect: NoSchedule
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            name: coredns
          name: config-volume
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-01-26T07:18:49Z"
    generation: 1
    labels:
      k8s-app: coredns-autoscaler
      pod-template-hash: 5567d8c485
    name: coredns-autoscaler-5567d8c485
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: coredns-autoscaler
      uid: 774d0f20-d8cd-42f8-b9b6-164c79e30167
    resourceVersion: "13085734"
    uid: 4acbbd70-8ca3-4d1a-a033-7a4fa61a2e94
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: coredns-autoscaler
        pod-template-hash: 5567d8c485
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: coredns-autoscaler
          pod-template-hash: 5567d8c485
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: node-role.kubernetes.io/worker
                  operator: Exists
        containers:
        - command:
          - /cluster-proportional-autoscaler
          - --namespace=kube-system
          - --configmap=coredns-autoscaler
          - --target=Deployment/coredns
          - --default-params={"linear":{"coresPerReplica":128,"nodesPerReplica":4,"min":1,"preventSinglePointFailure":true}}
          - --nodelabels=node-role.kubernetes.io/worker=true,beta.kubernetes.io/os=linux
          - --logtostderr=true
          - --v=2
          image: rancher/mirrored-cluster-proportional-autoscaler:1.8.6
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: autoscaler
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 20m
              memory: 10Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns-autoscaler
        serviceAccountName: coredns-autoscaler
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          operator: Exists
        - effect: NoSchedule
          operator: Exists
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2024-03-13T09:26:23Z"
    generation: 3
    labels:
      k8s-app: kube-dns
      pod-template-hash: b9597578
    name: coredns-b9597578
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: coredns
      uid: 3e7ffa92-455d-46b1-bfd1-4137b69ee321
    resourceVersion: "13078558"
    uid: d36b48bb-6047-405f-9897-bae16c4dc40b
  spec:
    replicas: 2
    selector:
      matchLabels:
        k8s-app: kube-dns
        pod-template-hash: b9597578
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2024-03-13T10:26:22+01:00"
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
          pod-template-hash: b9597578
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: node-role.kubernetes.io/worker
                  operator: Exists
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: k8s-app
                    operator: In
                    values:
                    - kube-dns
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: rancher/mirrored-coredns-coredns:1.9.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          beta.kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoExecute
          operator: Exists
        - effect: NoSchedule
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            name: coredns
          name: config-volume
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 3
    readyReplicas: 2
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-01-26T07:18:59Z"
    generation: 1
    labels:
      k8s-app: metrics-server
      pod-template-hash: 7886b5f87c
    name: metrics-server-7886b5f87c
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: metrics-server
      uid: 12f5dfc2-781a-49be-aab8-16489455b46f
    resourceVersion: "13078452"
    uid: f17d2efc-8224-4688-afac-06810a75d711
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: metrics-server
        pod-template-hash: 7886b5f87c
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
          pod-template-hash: 7886b5f87c
        name: metrics-server
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: beta.kubernetes.io/os
                  operator: NotIn
                  values:
                  - windows
                - key: node-role.kubernetes.io/worker
                  operator: Exists
        containers:
        - args:
          - --cert-dir=/tmp
          - --secure-port=4443
          - --kubelet-insecure-tls
          - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
          - --metric-resolution=15s
          image: rancher/mirrored-metrics-server:v0.6.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: metrics-server
          ports:
          - containerPort: 4443
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: https
              scheme: HTTPS
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-dir
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          operator: Exists
        - effect: NoSchedule
          operator: Exists
        volumes:
        - emptyDir: {}
          name: tmp-dir
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "12"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-05T06:20:13Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/name: grafana
      pod-template-hash: "5777877675"
    name: kube-prometheus-stack-grafana-5777877675
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-grafana
      uid: 60a86144-080d-4d57-9783-9241f5375a03
    resourceVersion: "10780437"
    uid: 8d28305d-f255-44e9-be5b-af4f4098f314
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: grafana
        pod-template-hash: "5777877675"
    template:
      metadata:
        annotations:
          checksum/config: 5e6dc46683f6da93ac57508aa43dfbc12b8a264349d2c6cdf874bc04a0120dd0
          checksum/sc-dashboard-provider-config: 593c0a8778b83f11fe80ccb21dfb20bc46705e2be3178df1dc4c89d164c8cd9c
          checksum/secret: 52393145826148a5c4d0489c34fdfe1036fe8275afda36f83c7eee420dd9eb7d
          kubectl.kubernetes.io/default-container: grafana
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/name: grafana
          pod-template-hash: "5777877675"
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:10.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        initContainers:
        - command:
          - chown
          - -R
          - 472:472
          - /var/lib/grafana
          image: docker.io/library/busybox:1.31.1
          imagePullPolicy: IfNotPresent
          name: init-chown-data
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/grafana
            name: storage
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: kube-prometheus-stack-grafana
        serviceAccountName: kube-prometheus-stack-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana
          name: config
        - name: storage
          persistentVolumeClaim:
            claimName: kube-prometheus-stack-grafana
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "6"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-02-20T05:00:17Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/name: grafana
      pod-template-hash: 5994988b65
    name: kube-prometheus-stack-grafana-5994988b65
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-grafana
      uid: 60a86144-080d-4d57-9783-9241f5375a03
    resourceVersion: "6492576"
    uid: 1b74d468-685b-4ded-ab94-3b671574c90d
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: grafana
        pod-template-hash: 5994988b65
    template:
      metadata:
        annotations:
          checksum/config: 1916f6725b1417f536e843e808bc7c46aa6d73474d452fd31b4d2369c151e5ec
          checksum/sc-dashboard-provider-config: 593c0a8778b83f11fe80ccb21dfb20bc46705e2be3178df1dc4c89d164c8cd9c
          checksum/secret: 52393145826148a5c4d0489c34fdfe1036fe8275afda36f83c7eee420dd9eb7d
          kubectl.kubernetes.io/default-container: grafana
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/name: grafana
          pod-template-hash: 5994988b65
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:10.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: kube-prometheus-stack-grafana
        serviceAccountName: kube-prometheus-stack-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana
          name: config
        - emptyDir: {}
          name: storage
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-02-19T06:59:02Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/name: grafana
      pod-template-hash: "6756768884"
    name: kube-prometheus-stack-grafana-6756768884
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-grafana
      uid: 60a86144-080d-4d57-9783-9241f5375a03
    resourceVersion: "6213722"
    uid: 0e5a68d9-d738-4c07-b2d2-513bc997430a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: grafana
        pod-template-hash: "6756768884"
    template:
      metadata:
        annotations:
          checksum/config: b5fa991debf6a4ed4674d6f03168386ed522002c1190ba1f7d51975f6e0bc4c8
          checksum/sc-dashboard-provider-config: 593c0a8778b83f11fe80ccb21dfb20bc46705e2be3178df1dc4c89d164c8cd9c
          checksum/secret: 52393145826148a5c4d0489c34fdfe1036fe8275afda36f83c7eee420dd9eb7d
          kubectl.kubernetes.io/default-container: grafana
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/name: grafana
          pod-template-hash: "6756768884"
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:10.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: kube-prometheus-stack-grafana
        serviceAccountName: kube-prometheus-stack-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana
          name: config
        - emptyDir: {}
          name: storage
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "8"
      deployment.kubernetes.io/revision-history: "4"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-02-20T04:31:56Z"
    generation: 4
    labels:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/name: grafana
      pod-template-hash: 6c498db6dd
    name: kube-prometheus-stack-grafana-6c498db6dd
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-grafana
      uid: 60a86144-080d-4d57-9783-9241f5375a03
    resourceVersion: "7396878"
    uid: 791fed8a-5c4c-4b89-9319-4254722d0c97
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: grafana
        pod-template-hash: 6c498db6dd
    template:
      metadata:
        annotations:
          checksum/config: c8e876853580e8b0d807a852e9813e06a752130be7ce716cc267fb1881ca7e39
          checksum/sc-dashboard-provider-config: 593c0a8778b83f11fe80ccb21dfb20bc46705e2be3178df1dc4c89d164c8cd9c
          checksum/secret: 52393145826148a5c4d0489c34fdfe1036fe8275afda36f83c7eee420dd9eb7d
          kubectl.kubernetes.io/default-container: grafana
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/name: grafana
          pod-template-hash: 6c498db6dd
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:10.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: kube-prometheus-stack-grafana
        serviceAccountName: kube-prometheus-stack-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana
          name: config
        - emptyDir: {}
          name: storage
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "13"
      deployment.kubernetes.io/revision-history: "9"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-02-23T07:31:50Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/name: grafana
      pod-template-hash: 7c77d9b866
    name: kube-prometheus-stack-grafana-7c77d9b866
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-grafana
      uid: 60a86144-080d-4d57-9783-9241f5375a03
    resourceVersion: "13086987"
    uid: e018a8ec-e321-4c87-b2cc-a12541c3ee1a
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: grafana
        pod-template-hash: 7c77d9b866
    template:
      metadata:
        annotations:
          checksum/config: c8e876853580e8b0d807a852e9813e06a752130be7ce716cc267fb1881ca7e39
          checksum/sc-dashboard-provider-config: 593c0a8778b83f11fe80ccb21dfb20bc46705e2be3178df1dc4c89d164c8cd9c
          checksum/secret: 52393145826148a5c4d0489c34fdfe1036fe8275afda36f83c7eee420dd9eb7d
          kubectl.kubernetes.io/default-container: grafana
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/name: grafana
          pod-template-hash: 7c77d9b866
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:10.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        initContainers:
        - command:
          - chown
          - -R
          - 472:472
          - /var/lib/grafana
          image: docker.io/library/busybox:1.31.1
          imagePullPolicy: IfNotPresent
          name: init-chown-data
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/grafana
            name: storage
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: kube-prometheus-stack-grafana
        serviceAccountName: kube-prometheus-stack-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana
          name: config
        - name: storage
          persistentVolumeClaim:
            claimName: kube-prometheus-stack-grafana
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "10"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-05T06:05:23Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/name: grafana
      pod-template-hash: 7f6cc65689
    name: kube-prometheus-stack-grafana-7f6cc65689
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-grafana
      uid: 60a86144-080d-4d57-9783-9241f5375a03
    resourceVersion: "10714931"
    uid: ab98ac81-892f-457c-b7ca-801ef062e993
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: grafana
        pod-template-hash: 7f6cc65689
    template:
      metadata:
        annotations:
          checksum/config: 068f8cdfc5416f833b0fb8e8fb7641d8afa562e993a31f2b1b1630b57b78aef7
          checksum/sc-dashboard-provider-config: 593c0a8778b83f11fe80ccb21dfb20bc46705e2be3178df1dc4c89d164c8cd9c
          checksum/secret: 52393145826148a5c4d0489c34fdfe1036fe8275afda36f83c7eee420dd9eb7d
          kubectl.kubernetes.io/default-container: grafana
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/name: grafana
          pod-template-hash: 7f6cc65689
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:10.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        initContainers:
        - command:
          - chown
          - -R
          - 472:472
          - /var/lib/grafana
          image: docker.io/library/busybox:1.31.1
          imagePullPolicy: IfNotPresent
          name: init-chown-data
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/grafana
            name: storage
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: kube-prometheus-stack-grafana
        serviceAccountName: kube-prometheus-stack-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana
          name: config
        - name: storage
          persistentVolumeClaim:
            claimName: kube-prometheus-stack-grafana
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "7"
      deployment.kubernetes.io/revision-history: 1,3,5
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-02-02T10:31:12Z"
    generation: 6
    labels:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/name: grafana
      pod-template-hash: 9d46b9bcd
    name: kube-prometheus-stack-grafana-9d46b9bcd
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-grafana
      uid: 60a86144-080d-4d57-9783-9241f5375a03
    resourceVersion: "6536180"
    uid: cd452f14-7c5c-4a69-9945-a7f087af3665
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: grafana
        pod-template-hash: 9d46b9bcd
    template:
      metadata:
        annotations:
          checksum/config: 66f0043c27435681c8baf268e9ff1d0314b33d72eff8c3810d856983c7c0869f
          checksum/sc-dashboard-provider-config: 593c0a8778b83f11fe80ccb21dfb20bc46705e2be3178df1dc4c89d164c8cd9c
          checksum/secret: 52393145826148a5c4d0489c34fdfe1036fe8275afda36f83c7eee420dd9eb7d
          kubectl.kubernetes.io/default-container: grafana
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/name: grafana
          pod-template-hash: 9d46b9bcd
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:10.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: kube-prometheus-stack-grafana
        serviceAccountName: kube-prometheus-stack-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana
          name: config
        - emptyDir: {}
          name: storage
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    observedGeneration: 6
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "11"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-05T06:14:55Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/name: grafana
      pod-template-hash: b6dbf77fb
    name: kube-prometheus-stack-grafana-b6dbf77fb
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-grafana
      uid: 60a86144-080d-4d57-9783-9241f5375a03
    resourceVersion: "10716108"
    uid: bfc05837-51ed-4504-8540-3b058a2fb71d
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: grafana
        pod-template-hash: b6dbf77fb
    template:
      metadata:
        annotations:
          checksum/config: ccdfbd53f00ca3b9754190680c64570f8617200ab484dfce5b1b344534048a3f
          checksum/sc-dashboard-provider-config: 593c0a8778b83f11fe80ccb21dfb20bc46705e2be3178df1dc4c89d164c8cd9c
          checksum/secret: 52393145826148a5c4d0489c34fdfe1036fe8275afda36f83c7eee420dd9eb7d
          kubectl.kubernetes.io/default-container: grafana
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/name: grafana
          pod-template-hash: b6dbf77fb
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:10.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        initContainers:
        - command:
          - chown
          - -R
          - 472:472
          - /var/lib/grafana
          image: docker.io/library/busybox:1.31.1
          imagePullPolicy: IfNotPresent
          name: init-chown-data
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/grafana
            name: storage
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: kube-prometheus-stack-grafana
        serviceAccountName: kube-prometheus-stack-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana
          name: config
        - name: storage
          persistentVolumeClaim:
            claimName: kube-prometheus-stack-grafana
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-02-02T10:31:12Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.10.1
      helm.sh/chart: kube-state-metrics-5.15.2
      pod-template-hash: d68548445
      release: kube-prometheus-stack
    name: kube-prometheus-stack-kube-state-metrics-d68548445
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-kube-state-metrics
      uid: cde9c5d0-5146-4685-b32e-83e8819f65a9
    resourceVersion: "13075253"
    uid: 9cd8bc32-21a8-4368-be2e-5275564fa971
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: kube-state-metrics
        pod-template-hash: d68548445
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.10.1
          helm.sh/chart: kube-state-metrics-5.15.2
          pod-template-hash: d68548445
          release: kube-prometheus-stack
      spec:
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.10.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: kube-prometheus-stack-kube-state-metrics
        serviceAccountName: kube-prometheus-stack-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-02-02T10:31:12Z"
    generation: 1
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 55.5.1
      chart: kube-prometheus-stack-55.5.1
      heritage: Helm
      pod-template-hash: 79b4dd5d7c
      release: kube-prometheus-stack
    name: kube-prometheus-stack-operator-79b4dd5d7c
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-operator
      uid: d3f6343f-4180-49fb-8026-9a6b9956f0e9
    resourceVersion: "13075177"
    uid: 25988fa9-c9b4-4fe1-b5a7-157582b85d80
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: kube-prometheus-stack-operator
        pod-template-hash: 79b4dd5d7c
        release: kube-prometheus-stack
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: kube-prometheus-stack-operator
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/part-of: kube-prometheus-stack
          app.kubernetes.io/version: 55.5.1
          chart: kube-prometheus-stack-55.5.1
          heritage: Helm
          pod-template-hash: 79b4dd5d7c
          release: kube-prometheus-stack
      spec:
        containers:
        - args:
          - --kubelet-service=kube-system/kube-prometheus-stack-kubelet
          - --localhost=127.0.0.1
          - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
          - --config-reloader-cpu-request=0
          - --config-reloader-cpu-limit=0
          - --config-reloader-memory-request=0
          - --config-reloader-memory-limit=0
          - --thanos-default-base-image=quay.io/thanos/thanos:v0.32.5
          - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
          - --web.enable-tls=true
          - --web.cert-file=/cert/cert
          - --web.key-file=/cert/key
          - --web.listen-address=:10250
          - --web.tls-min-version=VersionTLS13
          env:
          - name: GOGC
            value: "30"
          image: quay.io/prometheus-operator/prometheus-operator:v0.70.0
          imagePullPolicy: IfNotPresent
          name: kube-prometheus-stack
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /cert
            name: tls-secret
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: kube-prometheus-stack-operator
        serviceAccountName: kube-prometheus-stack-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - name: tls-secret
          secret:
            defaultMode: 420
            secretName: kube-prometheus-stack-admission
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: UEsDBBQACAAIAAAAAAAAAAAAAAAAAAAAAAAIAAAAb3JpZ2luYWzcV11v2zYU/S9EHzpAVGzno6kAPwS1NxhbHCMZ9lIEBU1e2ZwpkiUpNVqg/z5cSo7lxGmTbRjWPRiWqMPLy3PO5cc9YVb+Bs5Lo0lGmLX+qBqShGykFiQjE7DK1AXoQBJSQGCCBUaye6JZASQj3EvKwa5zb1W5kppaZyqJ0cCRJKK8ZRyhzphNxJKEmC8a3DXk4EBz8CT7+GwiS2X45grxE1AQ4uecKQ8J4UYHZ5QCR7LgSjicdZfpw/DUWHAsGMyvlAh/x455PshzesJGnJ6Ik/f0XAwGNF/mZ2I5eD9YDs9Jc9skxFvgOHsHVknOPMlGCfGggGO87J4ULPD1L2wJyuMrs/ZbJDUYNjgWYFVjl1BbTPcauAMWgDQJCVBYhc84QE8D9ZpxWrqY1P4QsoDgJPdtNt0kWZ5LLUPMShsBF7v3JiHWiAsdZK+ROPhcSgdiUjqpVzd8DaJUUq9mK20emqd3wMtWxo/dFG4eEzi9sw48Jt5aYwN16wm0zla9jMw0SUjFVBkd9PXp30b9grFGmVX9cwy4KZfgNATwqTRHa+ND9Epziyx0bIHr3OlWcRBKq/GAJIRSHI4JgYmO37y9mEyupzc3P8RPCpgAR+O0pNFjNGf8EGQBpgzjUXE88Ieg9KFixv16eYpTwDxQUToWRxgevzscz4GGL1QAE0pqGA8Hz+KCq6kFJ40Yj848uU0I6CrOvaugbopbzklGjriX+OsznXrDN6S5TYgs2CpWHqykD65ON+eRaC9X1Afj2ApiZxYC42twWXWSnqQj0vVclEotjJIcpZrlcxMWDvxeTfd7k4Q48KZ0cT25J0oWMsQnbkuSkdFgUMQ1rDAOI45Ozy4lVhe6FnwfOnwEHY7OEdokpDKqLODSlDq0tijwccHCumNjlxvSAIEK6eLS0XfQV5yzNdc3jbLz1H/WQQhaMy0U0JY3KnXpgYJzxo3jGv4il5Va3mVHR/+M2Rx4+Qd6bZi+f7XXus7/d6u1SkodwFVMUR+YC+PTwaD4np0Id8Ex2m6qdLuLtrn/6ybsdc6q4/Ts1Ubc39i/FzPiIUKK8Zu386vJ9NNs0voQDzzjdt9uhdLCGqnD+M3bDzezT9P5ZHE1m/+6b9rd0c+Dq8DtTCicrMBhMjvXpW34lHuZtu+miGArReRrTIfxPTeOAyI2eDBQXEnQ4TmTLK4mn2aLrUd+dKZAKnMJSlxD/vDcseQDC6VPrRGzBWkaZKaL07Hx4kAWeIpUzuNhpR8JM5pfXE5vFhcfpi+Nt62FdHdY3wvaF+GvFsTnktVYCEht7ND+o/mHg3T4Svf3D3nfsv/pY0//9Iz5R6f7wNPh6O+ZP3kE9LXfAfGwSbHlCUzJ5VFhRKmgB1dySXeNDpi40qpubz1PIgioHg2ELU9gEHjUIe4R3Ohcro56PO9/edo9FDZqv4H+vPCNhsJu6/82IdZJ42SoPyjm/bzjqfYBCspV6QM4yp0MkjNF8DLlKsnhgnMcbN6/uz0o3/ca9YxsJWrVgcKGeiLb2wQIWRYkI5etrM1zSiFN7czuie0J1hxW7BC8L1xzULnnerbyHFSsZf+SWcTLAEX/QhQZ2VIYgfR3b7AkushtYxobUYonV+E9gb8m/ctIfax+e7fFdQ/vjM2fAQAA//9QSwcIEPx1W9QEAABtEAAAUEsBAhQAFAAIAAgAAAAAABD8dVvUBAAAbRAAAAgAAAAAAAAAAAAAAAAAAAAAAG9yaWdpbmFsUEsFBgAAAAABAAEANgAAAAoFAAAAAA==
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-02-12T11:44:41Z"
    generation: 1
    labels:
      app: csi-cephfsplugin-provisioner
      contains: csi-cephfsplugin-metrics
      pod-template-hash: 774788f85
    name: csi-cephfsplugin-provisioner-774788f85
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: csi-cephfsplugin-provisioner
      uid: a5d18ec9-f5e7-4e72-a83a-bb496f8d18f1
    resourceVersion: "13085160"
    uid: 7d259d28-6418-483a-961d-3dde90aad44c
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: csi-cephfsplugin-provisioner
        pod-template-hash: 774788f85
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: csi-cephfsplugin-provisioner
          contains: csi-cephfsplugin-metrics
          pod-template-hash: 774788f85
      spec:
        affinity:
          nodeAffinity: {}
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - csi-cephfsplugin-provisioner
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - --v=0
          - --csi-address=$(ADDRESS)
          - --leader-election=true
          - --timeout=2m30s
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=137s
          - --leader-election-renew-deadline=107s
          - --leader-election-retry-period=26s
          env:
          - name: ADDRESS
            value: /csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-attacher:v4.4.2
          imagePullPolicy: IfNotPresent
          name: csi-attacher
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=137s
          - --leader-election-renew-deadline=107s
          - --leader-election-retry-period=26s
          - --handle-volume-inuse-error=false
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-resizer:v1.9.2
          imagePullPolicy: IfNotPresent
          name: csi-resizer
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --retry-interval-start=500ms
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=137s
          - --leader-election-renew-deadline=107s
          - --leader-election-retry-period=26s
          - --extra-create-metadata=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-provisioner:v3.6.2
          imagePullPolicy: IfNotPresent
          name: csi-provisioner
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --nodeid=$(NODE_ID)
          - --type=cephfs
          - --endpoint=$(CSI_ENDPOINT)
          - --v=0
          - --controllerserver=true
          - --drivername=rook-ceph.cephfs.csi.ceph.com
          - --pidlimit=-1
          - --forcecephkernelclient=true
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CSI_ENDPOINT
            value: unix:///csi/csi-provisioner.sock
          image: quay.io/cephcsi/cephcsi:v3.10.1
          imagePullPolicy: IfNotPresent
          name: csi-cephfsplugin
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
          - mountPath: /sys
            name: host-sys
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /dev
            name: host-dev
          - mountPath: /etc/ceph-csi-config/
            name: ceph-csi-config
          - mountPath: /tmp/csi/keys
            name: keys-tmp-dir
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-csi-cephfs-provisioner-sa
        serviceAccountName: rook-csi-cephfs-provisioner-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: socket-dir
        - hostPath:
            path: /sys
            type: ""
          name: host-sys
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - hostPath:
            path: /dev
            type: ""
          name: host-dev
        - configMap:
            defaultMode: 420
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
          name: ceph-csi-config
        - emptyDir:
            medium: Memory
          name: keys-tmp-dir
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-01-31T12:41:09Z"
    generation: 1
    labels:
      app: csi-rbdplugin-provisioner
      contains: csi-rbdplugin-metrics
      pod-template-hash: 56f7bf6d4d
    name: csi-rbdplugin-provisioner-56f7bf6d4d
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: csi-rbdplugin-provisioner
      uid: f8fd6120-dd82-4955-9d9d-af715e126301
    resourceVersion: "13085218"
    uid: 9a0f4d8d-835c-4750-8876-53b3797c0357
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: csi-rbdplugin-provisioner
        pod-template-hash: 56f7bf6d4d
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: csi-rbdplugin-provisioner
          contains: csi-rbdplugin-metrics
          pod-template-hash: 56f7bf6d4d
      spec:
        affinity:
          nodeAffinity: {}
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - csi-rbdplugin-provisioner
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --retry-interval-start=500ms
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=137s
          - --leader-election-renew-deadline=107s
          - --leader-election-retry-period=26s
          - --default-fstype=ext4
          - --extra-create-metadata=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-provisioner:v3.6.2
          imagePullPolicy: IfNotPresent
          name: csi-provisioner
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=137s
          - --leader-election-renew-deadline=107s
          - --leader-election-retry-period=26s
          - --handle-volume-inuse-error=false
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-resizer:v1.9.2
          imagePullPolicy: IfNotPresent
          name: csi-resizer
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --v=0
          - --timeout=2m30s
          - --csi-address=$(ADDRESS)
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=137s
          - --leader-election-renew-deadline=107s
          - --leader-election-retry-period=26s
          - --default-fstype=ext4
          env:
          - name: ADDRESS
            value: /csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-attacher:v4.4.2
          imagePullPolicy: IfNotPresent
          name: csi-attacher
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=137s
          - --leader-election-renew-deadline=107s
          - --leader-election-retry-period=26s
          - --extra-create-metadata=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-snapshotter:v6.3.2
          imagePullPolicy: IfNotPresent
          name: csi-snapshotter
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --nodeid=$(NODE_ID)
          - --endpoint=$(CSI_ENDPOINT)
          - --v=0
          - --type=rbd
          - --controllerserver=true
          - --drivername=rook-ceph.rbd.csi.ceph.com
          - --pidlimit=-1
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CSI_ENDPOINT
            value: unix:///csi/csi-provisioner.sock
          image: quay.io/cephcsi/cephcsi:v3.10.1
          imagePullPolicy: IfNotPresent
          name: csi-rbdplugin
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
          - mountPath: /dev
            name: host-dev
          - mountPath: /sys
            name: host-sys
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /etc/ceph-csi-config/
            name: ceph-csi-configs
          - mountPath: /tmp/csi/keys
            name: keys-tmp-dir
          - mountPath: /run/secrets/tokens
            name: oidc-token
            readOnly: true
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-csi-rbd-provisioner-sa
        serviceAccountName: rook-csi-rbd-provisioner-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /dev
            type: ""
          name: host-dev
        - hostPath:
            path: /sys
            type: ""
          name: host-sys
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - emptyDir:
            medium: Memory
          name: socket-dir
        - name: ceph-csi-configs
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: csi-cluster-config-json
                  path: config.json
                name: rook-ceph-csi-config
            - configMap:
                items:
                - key: csi-mapping-config-json
                  path: cluster-mapping.json
                name: rook-ceph-csi-mapping-config
        - emptyDir:
            medium: Memory
          name: keys-tmp-dir
        - name: oidc-token
          projected:
            defaultMode: 420
            sources:
            - serviceAccountToken:
                audience: ceph-csi-kms
                expirationSeconds: 3600
                path: oidc-token
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2024-03-13T16:49:57Z"
    generation: 1
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 17.2.6-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: worker-1
      node_name: worker-1
      pod-template-hash: 78679dd88c
      rook_cluster: rook-ceph
    name: rook-ceph-crashcollector-worker-1-78679dd88c
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-crashcollector-worker-1
      uid: 3b7b01c7-3c71-44a7-b57c-b355efbfe57e
    resourceVersion: "13085390"
    uid: fa2e9266-45c2-42d5-9fcb-149f4bd6061b
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-crashcollector
        kubernetes.io/hostname: worker-1
        node_name: worker-1
        pod-template-hash: 78679dd88c
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-crashcollector
          ceph-version: 17.2.6-0
          ceph_daemon_id: crash
          crashcollector: crash
          kubernetes.io/hostname: worker-1
          node_name: worker-1
          pod-template-hash: 78679dd88c
          rook_cluster: rook-ceph
      spec:
        containers:
        - command:
          - ceph-crash
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: ceph-crash
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
            runAsGroup: 167
            runAsUser: 167
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /etc/ceph/crash-collector-keyring-store/
            name: rook-ceph-crash-collector-keyring
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - /var/lib/ceph/crash/posted
          command:
          - mkdir
          - -p
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: make-container-crash-dir
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        nodeSelector:
          kubernetes.io/hostname: worker-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - name: rook-ceph-crash-collector-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-crash-collector-keyring
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-03-13T16:45:38Z"
    generation: 2
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 17.2.6-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: worker-1
      node_name: worker-1
      pod-template-hash: 7b57758c4f
      rook_cluster: rook-ceph
    name: rook-ceph-crashcollector-worker-1-7b57758c4f
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-crashcollector-worker-1
      uid: 3b7b01c7-3c71-44a7-b57c-b355efbfe57e
    resourceVersion: "13085400"
    uid: 8e927492-252f-4776-8a7b-a40080dfdf49
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-crashcollector
        kubernetes.io/hostname: worker-1
        node_name: worker-1
        pod-template-hash: 7b57758c4f
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-crashcollector
          ceph-version: 17.2.6-0
          ceph_daemon_id: crash
          crashcollector: crash
          kubernetes.io/hostname: worker-1
          node_name: worker-1
          pod-template-hash: 7b57758c4f
          rook_cluster: rook-ceph
      spec:
        containers:
        - command:
          - ceph-crash
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: ceph-crash
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
            runAsGroup: 167
            runAsUser: 167
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /etc/ceph/crash-collector-keyring-store/
            name: rook-ceph-crash-collector-keyring
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - /var/lib/ceph/crash/posted
          command:
          - mkdir
          - -p
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: make-container-crash-dir
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        nodeSelector:
          kubernetes.io/hostname: worker-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 300
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - name: rook-ceph-crash-collector-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-crash-collector-keyring
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "7"
      deployment.kubernetes.io/revision-history: 1,3,5
    creationTimestamp: "2024-02-01T09:39:41Z"
    generation: 7
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 17.2.6-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: worker-2
      node_name: worker-2
      pod-template-hash: 686d8549dd
      rook_cluster: rook-ceph
    name: rook-ceph-crashcollector-worker-2-686d8549dd
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-crashcollector-worker-2
      uid: 34a796cf-b7dd-4fd4-935c-35ca648f60a2
    resourceVersion: "13085560"
    uid: d71e0b82-e612-4cea-84fb-45cc7629bcce
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-crashcollector
        kubernetes.io/hostname: worker-2
        node_name: worker-2
        pod-template-hash: 686d8549dd
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-crashcollector
          ceph-version: 17.2.6-0
          ceph_daemon_id: crash
          crashcollector: crash
          kubernetes.io/hostname: worker-2
          node_name: worker-2
          pod-template-hash: 686d8549dd
          rook_cluster: rook-ceph
      spec:
        containers:
        - command:
          - ceph-crash
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: ceph-crash
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
            runAsGroup: 167
            runAsUser: 167
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /etc/ceph/crash-collector-keyring-store/
            name: rook-ceph-crash-collector-keyring
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - /var/lib/ceph/crash/posted
          command:
          - mkdir
          - -p
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: make-container-crash-dir
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        nodeSelector:
          kubernetes.io/hostname: worker-2
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 300
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - name: rook-ceph-crash-collector-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-crash-collector-keyring
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 7
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "6"
      deployment.kubernetes.io/revision-history: 2,4
    creationTimestamp: "2024-02-01T09:40:48Z"
    generation: 6
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 17.2.6-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: worker-2
      node_name: worker-2
      pod-template-hash: b876cdb9d
      rook_cluster: rook-ceph
    name: rook-ceph-crashcollector-worker-2-b876cdb9d
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-crashcollector-worker-2
      uid: 34a796cf-b7dd-4fd4-935c-35ca648f60a2
    resourceVersion: "13085570"
    uid: 4d87b7ec-c3be-4e5d-aa70-632e00c84628
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-crashcollector
        kubernetes.io/hostname: worker-2
        node_name: worker-2
        pod-template-hash: b876cdb9d
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-crashcollector
          ceph-version: 17.2.6-0
          ceph_daemon_id: crash
          crashcollector: crash
          kubernetes.io/hostname: worker-2
          node_name: worker-2
          pod-template-hash: b876cdb9d
          rook_cluster: rook-ceph
      spec:
        containers:
        - command:
          - ceph-crash
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: ceph-crash
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
            runAsGroup: 167
            runAsUser: 167
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /etc/ceph/crash-collector-keyring-store/
            name: rook-ceph-crash-collector-keyring
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - /var/lib/ceph/crash/posted
          command:
          - mkdir
          - -p
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: make-container-crash-dir
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        nodeSelector:
          kubernetes.io/hostname: worker-2
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - name: rook-ceph-crash-collector-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-crash-collector-keyring
  status:
    observedGeneration: 6
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "7"
      deployment.kubernetes.io/revision-history: 1,3,5
    creationTimestamp: "2024-01-31T12:42:17Z"
    generation: 3
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 17.2.6-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: worker-3
      node_name: worker-3
      pod-template-hash: "5845867477"
      rook_cluster: rook-ceph
    name: rook-ceph-crashcollector-worker-3-5845867477
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-crashcollector-worker-3
      uid: b03bf9eb-96b7-47fe-8e6b-bfa0a6eae32a
    resourceVersion: "13085406"
    uid: 3bf658e7-148a-49e0-96c1-9ec09cd43c20
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-crashcollector
        kubernetes.io/hostname: worker-3
        node_name: worker-3
        pod-template-hash: "5845867477"
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-crashcollector
          ceph-version: 17.2.6-0
          ceph_daemon_id: crash
          crashcollector: crash
          kubernetes.io/hostname: worker-3
          node_name: worker-3
          pod-template-hash: "5845867477"
          rook_cluster: rook-ceph
      spec:
        containers:
        - command:
          - ceph-crash
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: ceph-crash
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
            runAsGroup: 167
            runAsUser: 167
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /etc/ceph/crash-collector-keyring-store/
            name: rook-ceph-crash-collector-keyring
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - /var/lib/ceph/crash/posted
          command:
          - mkdir
          - -p
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: make-container-crash-dir
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        nodeSelector:
          kubernetes.io/hostname: worker-3
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - name: rook-ceph-crash-collector-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-crash-collector-keyring
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "6"
      deployment.kubernetes.io/revision-history: 2,4
    creationTimestamp: "2024-02-28T07:25:02Z"
    generation: 6
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 17.2.6-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: worker-3
      node_name: worker-3
      pod-template-hash: 7444bf5fcf
      rook_cluster: rook-ceph
    name: rook-ceph-crashcollector-worker-3-7444bf5fcf
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-crashcollector-worker-3
      uid: b03bf9eb-96b7-47fe-8e6b-bfa0a6eae32a
    resourceVersion: "13085413"
    uid: 56ecf956-f4ab-45a1-941a-09e31e1b90a1
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-crashcollector
        kubernetes.io/hostname: worker-3
        node_name: worker-3
        pod-template-hash: 7444bf5fcf
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-crashcollector
          ceph-version: 17.2.6-0
          ceph_daemon_id: crash
          crashcollector: crash
          kubernetes.io/hostname: worker-3
          node_name: worker-3
          pod-template-hash: 7444bf5fcf
          rook_cluster: rook-ceph
      spec:
        containers:
        - command:
          - ceph-crash
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: ceph-crash
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
            runAsGroup: 167
            runAsUser: 167
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /etc/ceph/crash-collector-keyring-store/
            name: rook-ceph-crash-collector-keyring
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - /var/lib/ceph/crash/posted
          command:
          - mkdir
          - -p
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: make-container-crash-dir
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        nodeSelector:
          kubernetes.io/hostname: worker-3
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 300
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - name: rook-ceph-crash-collector-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-crash-collector-keyring
  status:
    observedGeneration: 6
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "6"
      deployment.kubernetes.io/revision-history: 2,4
    creationTimestamp: "2024-03-13T10:11:59Z"
    generation: 5
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 17.2.6-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: worker-4
      node_name: worker-4
      pod-template-hash: 66d7cd88d8
      rook_cluster: rook-ceph
    name: rook-ceph-crashcollector-worker-4-66d7cd88d8
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-crashcollector-worker-4
      uid: 5e0a49dd-6c8f-4fa9-95cc-c2825c7a8bf1
    resourceVersion: "13085362"
    uid: b6db64ba-a63b-496a-b3ef-f041a9123f8f
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-crashcollector
        kubernetes.io/hostname: worker-4
        node_name: worker-4
        pod-template-hash: 66d7cd88d8
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-crashcollector
          ceph-version: 17.2.6-0
          ceph_daemon_id: crash
          crashcollector: crash
          kubernetes.io/hostname: worker-4
          node_name: worker-4
          pod-template-hash: 66d7cd88d8
          rook_cluster: rook-ceph
      spec:
        containers:
        - command:
          - ceph-crash
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: ceph-crash
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
            runAsGroup: 167
            runAsUser: 167
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /etc/ceph/crash-collector-keyring-store/
            name: rook-ceph-crash-collector-keyring
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - /var/lib/ceph/crash/posted
          command:
          - mkdir
          - -p
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: make-container-crash-dir
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        nodeSelector:
          kubernetes.io/hostname: worker-4
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - name: rook-ceph-crash-collector-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-crash-collector-keyring
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 5
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "5"
      deployment.kubernetes.io/revision-history: 1,3
    creationTimestamp: "2024-02-28T07:51:18Z"
    generation: 4
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 17.2.6-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: worker-4
      node_name: worker-4
      pod-template-hash: 86c6ffc4b
      rook_cluster: rook-ceph
    name: rook-ceph-crashcollector-worker-4-86c6ffc4b
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-crashcollector-worker-4
      uid: 5e0a49dd-6c8f-4fa9-95cc-c2825c7a8bf1
    resourceVersion: "13085369"
    uid: 19fe1d15-8be9-43d7-9664-20247ba57b36
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-crashcollector
        kubernetes.io/hostname: worker-4
        node_name: worker-4
        pod-template-hash: 86c6ffc4b
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-crashcollector
          ceph-version: 17.2.6-0
          ceph_daemon_id: crash
          crashcollector: crash
          kubernetes.io/hostname: worker-4
          node_name: worker-4
          pod-template-hash: 86c6ffc4b
          rook_cluster: rook-ceph
      spec:
        containers:
        - command:
          - ceph-crash
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: ceph-crash
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
            runAsGroup: 167
            runAsUser: 167
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /etc/ceph/crash-collector-keyring-store/
            name: rook-ceph-crash-collector-keyring
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - /var/lib/ceph/crash/posted
          command:
          - mkdir
          - -p
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: make-container-crash-dir
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 500m
              memory: 60Mi
            requests:
              cpu: 100m
              memory: 60Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        nodeSelector:
          kubernetes.io/hostname: worker-4
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 300
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - name: rook-ceph-crash-collector-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-crash-collector-keyring
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-01-31T12:45:42Z"
    generation: 1
    labels:
      app: rook-ceph-mds
      app.kubernetes.io/component: cephfilesystems.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: ceph-filesystem-a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mds
      app.kubernetes.io/part-of: ceph-filesystem
      ceph_daemon_id: ceph-filesystem-a
      ceph_daemon_type: mds
      mds: ceph-filesystem-a
      pod-template-hash: 6d784b75dd
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      rook_file_system: ceph-filesystem
    name: rook-ceph-mds-ceph-filesystem-a-6d784b75dd
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mds-ceph-filesystem-a
      uid: 0959fe1f-d474-409f-aebf-bbefbb7cdc60
    resourceVersion: "13085136"
    uid: bad5d37a-663b-4bc1-be76-e2f1093cd851
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-mds
        ceph_daemon_id: ceph-filesystem-a
        mds: ceph-filesystem-a
        pod-template-hash: 6d784b75dd
        rook_cluster: rook-ceph
        rook_file_system: ceph-filesystem
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mds
          app.kubernetes.io/component: cephfilesystems.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: ceph-filesystem-a
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mds
          app.kubernetes.io/part-of: ceph-filesystem
          ceph_daemon_id: ceph-filesystem-a
          ceph_daemon_type: mds
          mds: ceph-filesystem-a
          pod-template-hash: 6d784b75dd
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          rook_file_system: ceph-filesystem
        name: rook-ceph-mds-ceph-filesystem-a
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=ceph-filesystem-a
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --public-addr=$(ROOK_POD_IP)
          command:
          - ceph-mds
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - timeout
              - "20"
              - sh
              - -c
              - |
                #!/usr/bin/env bash
                # do not use 'set -e -u' etc. because it is important to only fail this probe when failure is certain
                # spurious failures risk destabilizing ceph or the filesystem

                MDS_ID="ceph-filesystem-a"
                FILESYSTEM_NAME="ceph-filesystem"
                KEYRING="/etc/ceph/keyring-store/keyring"

                outp="$(ceph fs dump --mon-host="$ROOK_CEPH_MON_HOST" --mon-initial-members="$ROOK_CEPH_MON_INITIAL_MEMBERS" --keyring "$KEYRING" --format json)"
                rc=$?
                if [ $rc -ne 0 ]; then
                    echo "ceph MDS dump check failed with the following output:"
                    echo "$outp"
                    echo "passing probe to avoid restarting MDS. cannot determine if MDS is unhealthy. restarting MDS risks destabilizing ceph/filesystem, which is likely unreachable or in error state"
                    exit 0
                fi

                # get the active and standby MDS in the fs map
                standbyMds=$(echo "$outp" | jq ".standbys | map(.name) | any(.[]; . == \"$MDS_ID\")")
                activeMds=$(echo "$outp" | jq ".filesystems[] | select(.mdsmap.fs_name == \"$FILESYSTEM_NAME\") | .mdsmap.info | map(.name) | any(.[]; . == \"$MDS_ID\")")

                if [[ $standbyMds == true || $activeMds == true ]]; then
                    echo "MDS ID present in MDS map, no need to re-start the container"
                    exit 0
                fi

                echo "Error: MDS ID not present in MDS map"
                exit 1
            failureThreshold: 5
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 25
          name: mds
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 256Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mds-ceph-filesystem-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mds/ceph-ceph-filesystem-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        - command:
          - /bin/bash
          - -x
          - -e
          - -m
          - -c
          - "\nCEPH_CLIENT_ID=ceph-mds.ceph-filesystem-a\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
            edit the logrotate file to only rotate a specific daemon log\n# otherwise
            we will logrotate log files without reloading certain daemons\n# this
            might happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
            \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
            --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
            rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed
            --in-place \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif
            [ \"$LOG_MAX_SIZE\" != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE
            at the 4th line of the logrotate config file with 4 spaces to maintain
            indentation\n\tsed --in-place \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\"
            \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile true; do\n\t# we don't force the
            logrorate but we let the logrotate binary handle the rotation based on
            user's input for periodicity and size\n\tlogrotate --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep
            15m\ndone\n"
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: log-collector
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mds/ceph-ceph-filesystem-a
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 256Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mds-ceph-filesystem-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mds/ceph-ceph-filesystem-a
            name: ceph-daemon-data
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        shareProcessNamespace: true
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mds-ceph-filesystem-a-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mds-ceph-filesystem-a-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: ceph-daemon-data
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-01-31T12:45:51Z"
    generation: 1
    labels:
      app: rook-ceph-mds
      app.kubernetes.io/component: cephfilesystems.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: ceph-filesystem-b
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mds
      app.kubernetes.io/part-of: ceph-filesystem
      ceph_daemon_id: ceph-filesystem-b
      ceph_daemon_type: mds
      mds: ceph-filesystem-b
      pod-template-hash: 8459d7cc6
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      rook_file_system: ceph-filesystem
    name: rook-ceph-mds-ceph-filesystem-b-8459d7cc6
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mds-ceph-filesystem-b
      uid: a958ce97-63eb-495e-bd88-78f2b26dcd34
    resourceVersion: "13084735"
    uid: 2ff40c2b-cea0-49ca-9bcc-e0cdb82a7b8f
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-mds
        ceph_daemon_id: ceph-filesystem-b
        mds: ceph-filesystem-b
        pod-template-hash: 8459d7cc6
        rook_cluster: rook-ceph
        rook_file_system: ceph-filesystem
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mds
          app.kubernetes.io/component: cephfilesystems.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: ceph-filesystem-b
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mds
          app.kubernetes.io/part-of: ceph-filesystem
          ceph_daemon_id: ceph-filesystem-b
          ceph_daemon_type: mds
          mds: ceph-filesystem-b
          pod-template-hash: 8459d7cc6
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          rook_file_system: ceph-filesystem
        name: rook-ceph-mds-ceph-filesystem-b
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=ceph-filesystem-b
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --public-addr=$(ROOK_POD_IP)
          command:
          - ceph-mds
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - timeout
              - "20"
              - sh
              - -c
              - |
                #!/usr/bin/env bash
                # do not use 'set -e -u' etc. because it is important to only fail this probe when failure is certain
                # spurious failures risk destabilizing ceph or the filesystem

                MDS_ID="ceph-filesystem-b"
                FILESYSTEM_NAME="ceph-filesystem"
                KEYRING="/etc/ceph/keyring-store/keyring"

                outp="$(ceph fs dump --mon-host="$ROOK_CEPH_MON_HOST" --mon-initial-members="$ROOK_CEPH_MON_INITIAL_MEMBERS" --keyring "$KEYRING" --format json)"
                rc=$?
                if [ $rc -ne 0 ]; then
                    echo "ceph MDS dump check failed with the following output:"
                    echo "$outp"
                    echo "passing probe to avoid restarting MDS. cannot determine if MDS is unhealthy. restarting MDS risks destabilizing ceph/filesystem, which is likely unreachable or in error state"
                    exit 0
                fi

                # get the active and standby MDS in the fs map
                standbyMds=$(echo "$outp" | jq ".standbys | map(.name) | any(.[]; . == \"$MDS_ID\")")
                activeMds=$(echo "$outp" | jq ".filesystems[] | select(.mdsmap.fs_name == \"$FILESYSTEM_NAME\") | .mdsmap.info | map(.name) | any(.[]; . == \"$MDS_ID\")")

                if [[ $standbyMds == true || $activeMds == true ]]; then
                    echo "MDS ID present in MDS map, no need to re-start the container"
                    exit 0
                fi

                echo "Error: MDS ID not present in MDS map"
                exit 1
            failureThreshold: 5
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 25
          name: mds
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 256Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mds-ceph-filesystem-b-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mds/ceph-ceph-filesystem-b
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        - command:
          - /bin/bash
          - -x
          - -e
          - -m
          - -c
          - "\nCEPH_CLIENT_ID=ceph-mds.ceph-filesystem-b\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
            edit the logrotate file to only rotate a specific daemon log\n# otherwise
            we will logrotate log files without reloading certain daemons\n# this
            might happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
            \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
            --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
            rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed
            --in-place \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif
            [ \"$LOG_MAX_SIZE\" != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE
            at the 4th line of the logrotate config file with 4 spaces to maintain
            indentation\n\tsed --in-place \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\"
            \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile true; do\n\t# we don't force the
            logrorate but we let the logrotate binary handle the rotation based on
            user's input for periodicity and size\n\tlogrotate --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep
            15m\ndone\n"
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: log-collector
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mds/ceph-ceph-filesystem-b
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 256Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mds-ceph-filesystem-b-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mds/ceph-ceph-filesystem-b
            name: ceph-daemon-data
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        shareProcessNamespace: true
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mds-ceph-filesystem-b-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mds-ceph-filesystem-b-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: ceph-daemon-data
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-01-31T12:42:17Z"
    generation: 1
    labels:
      app: rook-ceph-mgr
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mgr
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: a
      ceph_daemon_type: mgr
      instance: a
      mgr: a
      mgr_role: active
      pod-template-hash: 88759fcf8
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mgr-a-88759fcf8
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mgr-a
      uid: 5b06e356-b81e-412e-9514-0e71294809fc
    resourceVersion: "13085653"
    uid: 6ab2404d-0e0a-4d0a-8f3f-f17ffe6ede21
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-mgr
        ceph_daemon_id: a
        instance: a
        mgr: a
        pod-template-hash: 88759fcf8
        rook_cluster: rook-ceph
    template:
      metadata:
        annotations:
          prometheus.io/port: "9283"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: rook-ceph-mgr
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: a
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mgr
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: a
          ceph_daemon_type: mgr
          instance: a
          mgr: a
          mgr_role: active
          pod-template-hash: 88759fcf8
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mgr-a
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app: rook-ceph-mgr
                  rook_cluster: rook-ceph
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --client-mount-uid=0
          - --client-mount-gid=0
          - --foreground
          - --public-addr=$(ROOK_POD_IP)
          command:
          - ceph-mgr
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_OPERATOR_NAMESPACE
            value: rook-ceph
          - name: ROOK_CEPH_CLUSTER_CRD_VERSION
            value: v1
          - name: ROOK_CEPH_CLUSTER_CRD_NAME
            value: rook-ceph
          - name: CEPH_ARGS
            value: --mon-host $(ROOK_CEPH_MON_HOST) --keyring /etc/ceph/keyring-store/keyring
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: mgr
          ports:
          - containerPort: 6800
            name: mgr
            protocol: TCP
          - containerPort: 9283
            name: http-metrics
            protocol: TCP
          - containerPort: 8443
            name: dashboard
            protocol: TCP
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        - args:
          - ceph
          - mgr
          - watch-active
          env:
          - name: ROOK_CLUSTER_ID
            value: 02274ff6-97d3-4d09-b941-31edb9bdad99
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: ROOK_DASHBOARD_ENABLED
            value: "true"
          - name: ROOK_MONITORING_ENABLED
            value: "false"
          - name: ROOK_UPDATE_INTERVAL
            value: 15s
          - name: ROOK_DAEMON_NAME
            value: a
          - name: ROOK_CEPH_VERSION
            value: ceph version 17.2.6-0 quincy
          image: rook/ceph:v1.13.1
          imagePullPolicy: IfNotPresent
          name: watch-active
          resources:
            limits:
              cpu: 500m
              memory: 100Mi
            requests:
              cpu: 100m
              memory: 40Mi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook-ceph-mon
            name: ceph-admin-secret
            readOnly: true
        - command:
          - /bin/bash
          - -x
          - -e
          - -m
          - -c
          - "\nCEPH_CLIENT_ID=ceph-mgr.a\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
            edit the logrotate file to only rotate a specific daemon log\n# otherwise
            we will logrotate log files without reloading certain daemons\n# this
            might happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
            \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
            --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
            rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed
            --in-place \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif
            [ \"$LOG_MAX_SIZE\" != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE
            at the 4th line of the logrotate config file with 4 spaces to maintain
            indentation\n\tsed --in-place \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\"
            \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile true; do\n\t# we don't force the
            logrorate but we let the logrotate binary handle the rotation based on
            user's input for periodicity and size\n\tlogrotate --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep
            15m\ndone\n"
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: log-collector
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mgr/ceph-a
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-mgr
        serviceAccountName: rook-ceph-mgr
        shareProcessNamespace: true
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mgr-a-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mgr-a-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: ceph-daemon-data
        - name: ceph-admin-secret
          secret:
            defaultMode: 420
            items:
            - key: ceph-secret
              path: secret.keyring
            secretName: rook-ceph-mon
  status:
    availableReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-01-31T12:42:18Z"
    generation: 1
    labels:
      app: rook-ceph-mgr
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: b
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mgr
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: b
      ceph_daemon_type: mgr
      instance: b
      mgr: b
      mgr_role: active
      pod-template-hash: 684bb98899
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mgr-b-684bb98899
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mgr-b
      uid: 2225d5d3-6b85-4b05-8de8-2584400f56aa
    resourceVersion: "13085521"
    uid: 30e58d80-cae0-4fe6-bd3f-c6c0cefe4ce9
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-mgr
        ceph_daemon_id: b
        instance: b
        mgr: b
        pod-template-hash: 684bb98899
        rook_cluster: rook-ceph
    template:
      metadata:
        annotations:
          prometheus.io/port: "9283"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: rook-ceph-mgr
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: b
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mgr
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: b
          ceph_daemon_type: mgr
          instance: b
          mgr: b
          mgr_role: active
          pod-template-hash: 684bb98899
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mgr-b
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app: rook-ceph-mgr
                  rook_cluster: rook-ceph
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=b
          - --setuser=ceph
          - --setgroup=ceph
          - --client-mount-uid=0
          - --client-mount-gid=0
          - --foreground
          - --public-addr=$(ROOK_POD_IP)
          command:
          - ceph-mgr
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_OPERATOR_NAMESPACE
            value: rook-ceph
          - name: ROOK_CEPH_CLUSTER_CRD_VERSION
            value: v1
          - name: ROOK_CEPH_CLUSTER_CRD_NAME
            value: rook-ceph
          - name: CEPH_ARGS
            value: --mon-host $(ROOK_CEPH_MON_HOST) --keyring /etc/ceph/keyring-store/keyring
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.b.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: mgr
          ports:
          - containerPort: 6800
            name: mgr
            protocol: TCP
          - containerPort: 9283
            name: http-metrics
            protocol: TCP
          - containerPort: 8443
            name: dashboard
            protocol: TCP
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.b.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-b-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-b
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        - args:
          - ceph
          - mgr
          - watch-active
          env:
          - name: ROOK_CLUSTER_ID
            value: 02274ff6-97d3-4d09-b941-31edb9bdad99
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: ROOK_DASHBOARD_ENABLED
            value: "true"
          - name: ROOK_MONITORING_ENABLED
            value: "false"
          - name: ROOK_UPDATE_INTERVAL
            value: 15s
          - name: ROOK_DAEMON_NAME
            value: b
          - name: ROOK_CEPH_VERSION
            value: ceph version 17.2.6-0 quincy
          image: rook/ceph:v1.13.1
          imagePullPolicy: IfNotPresent
          name: watch-active
          resources:
            limits:
              cpu: 500m
              memory: 100Mi
            requests:
              cpu: 100m
              memory: 40Mi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook-ceph-mon
            name: ceph-admin-secret
            readOnly: true
        - command:
          - /bin/bash
          - -x
          - -e
          - -m
          - -c
          - "\nCEPH_CLIENT_ID=ceph-mgr.b\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
            edit the logrotate file to only rotate a specific daemon log\n# otherwise
            we will logrotate log files without reloading certain daemons\n# this
            might happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
            \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
            --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
            rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed
            --in-place \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif
            [ \"$LOG_MAX_SIZE\" != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE
            at the 4th line of the logrotate config file with 4 spaces to maintain
            indentation\n\tsed --in-place \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\"
            \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile true; do\n\t# we don't force the
            logrorate but we let the logrotate binary handle the rotation based on
            user's input for periodicity and size\n\tlogrotate --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep
            15m\ndone\n"
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: log-collector
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mgr/ceph-b
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-b-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-b
            name: ceph-daemon-data
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-mgr
        serviceAccountName: rook-ceph-mgr
        shareProcessNamespace: true
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mgr-b-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mgr-b-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: ceph-daemon-data
        - name: ceph-admin-secret
          secret:
            defaultMode: 420
            items:
            - key: ceph-secret
              path: secret.keyring
            secretName: rook-ceph-mon
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-01-31T12:41:16Z"
    generation: 1
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: a
      ceph_daemon_type: mon
      mon: a
      mon_cluster: rook-ceph
      pod-template-hash: 5c7dcd9676
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-a-5c7dcd9676
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mon-a
      uid: f6b4f854-7cf9-488c-966b-ab8c121abe41
    resourceVersion: "13085451"
    uid: 6aae3a45-fcf5-458a-9f3d-0f2122ed061c
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: a
        mon: a
        mon_cluster: rook-ceph
        pod-template-hash: 5c7dcd9676
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: a
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mon
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: a
          ceph_daemon_type: mon
          mon: a
          mon_cluster: rook-ceph
          pod-template-hash: 5c7dcd9676
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mon-a
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --public-addr=10.43.110.147
          - --setuser-match-path=/var/lib/ceph/mon/ceph-a/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: mon
          ports:
          - containerPort: 3300
            name: tcp-msgr2
            protocol: TCP
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources:
            limits:
              cpu: 1500m
              memory: 1536Mi
            requests:
              cpu: 350m
              memory: 512Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        - command:
          - /bin/bash
          - -x
          - -e
          - -m
          - -c
          - "\nCEPH_CLIENT_ID=ceph-mon.a\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
            edit the logrotate file to only rotate a specific daemon log\n# otherwise
            we will logrotate log files without reloading certain daemons\n# this
            might happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
            \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
            --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
            rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed
            --in-place \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif
            [ \"$LOG_MAX_SIZE\" != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE
            at the 4th line of the logrotate config file with 4 spaces to maintain
            indentation\n\tsed --in-place \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\"
            \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile true; do\n\t# we don't force the
            logrorate but we let the logrotate binary handle the rotation based on
            user's input for periodicity and size\n\tlogrotate --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep
            15m\ndone\n"
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: log-collector
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mon/ceph-a
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 1500m
              memory: 1536Mi
            requests:
              cpu: 350m
              memory: 512Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        - args:
          - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --public-addr=10.43.110.147
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources:
            limits:
              cpu: 1500m
              memory: 1536Mi
            requests:
              cpu: 350m
              memory: 512Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: worker-3
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        shareProcessNamespace: true
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-a/data
            type: ""
          name: ceph-daemon-data
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-01-31T12:41:56Z"
    generation: 1
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: c
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: c
      ceph_daemon_type: mon
      mon: c
      mon_cluster: rook-ceph
      pod-template-hash: 59cdc6599b
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-c-59cdc6599b
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mon-c
      uid: a0997286-60f7-4343-9fd6-2497b161761d
    resourceVersion: "13085379"
    uid: 5c071ef8-7a7b-456b-8c8a-97a77bb945b4
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: c
        mon: c
        mon_cluster: rook-ceph
        pod-template-hash: 59cdc6599b
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: c
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mon
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: c
          ceph_daemon_type: mon
          mon: c
          mon_cluster: rook-ceph
          pod-template-hash: 59cdc6599b
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mon-c
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=c
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --public-addr=10.43.221.137
          - --setuser-match-path=/var/lib/ceph/mon/ceph-c/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.c.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: mon
          ports:
          - containerPort: 3300
            name: tcp-msgr2
            protocol: TCP
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources:
            limits:
              cpu: 1500m
              memory: 1536Mi
            requests:
              cpu: 350m
              memory: 512Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.c.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-c
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        - command:
          - /bin/bash
          - -x
          - -e
          - -m
          - -c
          - "\nCEPH_CLIENT_ID=ceph-mon.c\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
            edit the logrotate file to only rotate a specific daemon log\n# otherwise
            we will logrotate log files without reloading certain daemons\n# this
            might happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
            \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
            --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
            rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed
            --in-place \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif
            [ \"$LOG_MAX_SIZE\" != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE
            at the 4th line of the logrotate config file with 4 spaces to maintain
            indentation\n\tsed --in-place \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\"
            \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile true; do\n\t# we don't force the
            logrorate but we let the logrotate binary handle the rotation based on
            user's input for periodicity and size\n\tlogrotate --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep
            15m\ndone\n"
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: log-collector
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mon/ceph-c
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 1500m
              memory: 1536Mi
            requests:
              cpu: 350m
              memory: 512Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-c
            name: ceph-daemon-data
        - args:
          - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=c
          - --setuser=ceph
          - --setgroup=ceph
          - --public-addr=10.43.221.137
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources:
            limits:
              cpu: 1500m
              memory: 1536Mi
            requests:
              cpu: 350m
              memory: 512Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-c
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: worker-1
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        shareProcessNamespace: true
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-c/data
            type: ""
          name: ceph-daemon-data
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-02-28T07:57:57Z"
    generation: 1
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: e
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: e
      ceph_daemon_type: mon
      mon: e
      mon_cluster: rook-ceph
      pod-template-hash: 76cdf858cf
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-e-76cdf858cf
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mon-e
      uid: 97915027-b2f3-46a7-b38a-e1d0ecc7d148
    resourceVersion: "13085476"
    uid: ea06ad40-5e4a-4076-9e75-45e550b5083e
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: e
        mon: e
        mon_cluster: rook-ceph
        pod-template-hash: 76cdf858cf
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: e
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mon
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: e
          ceph_daemon_type: mon
          mon: e
          mon_cluster: rook-ceph
          pod-template-hash: 76cdf858cf
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mon-e
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=e
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --public-addr=10.43.19.85
          - --setuser-match-path=/var/lib/ceph/mon/ceph-e/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.e.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: mon
          ports:
          - containerPort: 3300
            name: tcp-msgr2
            protocol: TCP
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources:
            limits:
              cpu: 1500m
              memory: 1536Mi
            requests:
              cpu: 350m
              memory: 512Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.e.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-e
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        - command:
          - /bin/bash
          - -x
          - -e
          - -m
          - -c
          - "\nCEPH_CLIENT_ID=ceph-mon.e\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
            edit the logrotate file to only rotate a specific daemon log\n# otherwise
            we will logrotate log files without reloading certain daemons\n# this
            might happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
            \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
            --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
            rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed
            --in-place \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif
            [ \"$LOG_MAX_SIZE\" != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE
            at the 4th line of the logrotate config file with 4 spaces to maintain
            indentation\n\tsed --in-place \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\"
            \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile true; do\n\t# we don't force the
            logrorate but we let the logrotate binary handle the rotation based on
            user's input for periodicity and size\n\tlogrotate --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep
            15m\ndone\n"
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: log-collector
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mon/ceph-e
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 1500m
              memory: 1536Mi
            requests:
              cpu: 350m
              memory: 512Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-e
            name: ceph-daemon-data
        - args:
          - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=e
          - --setuser=ceph
          - --setgroup=ceph
          - --public-addr=10.43.19.85
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources:
            limits:
              cpu: 1500m
              memory: 1536Mi
            requests:
              cpu: 350m
              memory: 512Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-e
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: worker-4
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        shareProcessNamespace: true
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-e/data
            type: ""
          name: ceph-daemon-data
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: rook-ceph
      meta.helm.sh/release-namespace: rook-ceph
    creationTimestamp: "2024-01-31T12:38:04Z"
    generation: 1
    labels:
      app: rook-ceph-operator
      helm.sh/chart: rook-ceph-v1.13.1
      pod-template-hash: 54bbffd5dd
    name: rook-ceph-operator-54bbffd5dd
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-operator
      uid: 7a3cf0ff-4a2c-4d49-8d00-fbf6db090b18
    resourceVersion: "13085213"
    uid: 569a024d-fd89-4b27-b746-af72a9bbdcfa
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-operator
        pod-template-hash: 54bbffd5dd
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-operator
          helm.sh/chart: rook-ceph-v1.13.1
          pod-template-hash: 54bbffd5dd
      spec:
        containers:
        - args:
          - ceph
          - operator
          env:
          - name: ROOK_CURRENT_NAMESPACE_ONLY
            value: "false"
          - name: ROOK_HOSTPATH_REQUIRES_PRIVILEGED
            value: "false"
          - name: ROOK_DISABLE_DEVICE_HOTPLUG
            value: "false"
          - name: ROOK_DISCOVER_DEVICES_INTERVAL
            value: 60m
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rook/ceph:v1.13.1
          imagePullPolicy: IfNotPresent
          name: rook-ceph-operator
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext:
            capabilities:
              drop:
              - ALL
            runAsGroup: 2016
            runAsNonRoot: true
            runAsUser: 2016
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-config
          - mountPath: /etc/ceph
            name: default-config-dir
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-system
        serviceAccountName: rook-ceph-system
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - emptyDir: {}
          name: rook-config
        - emptyDir: {}
          name: default-config-dir
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: UEsDBBQACAAIAAAAAAAAAAAAAAAAAAAAAAAIAAAAb3JpZ2luYWzsW41v27iS/1e4agEne5Y/8tE0XhgHb+x2jU3iPCfp3bsqZ9DSyOaGIrUkZdevm//9MKRky19J9r3iUNwtEAQ2ORzODOfjN5T81UvA0Iga6rW+epyOgWv8RNPUa3lKykc/hHTqSx15VRytPWZjUAIM6BqT9VAmqRQgjNfykDDkmTagdA2/1HB9jcndKxVQA5E/XqxvlIKiRqqda5jQhooQvJbX2EmQUEEnf5KpoAnk0u9VM6XK+DIuM/WqyyU+i3KJ7MgMlGZSeC2veVY7qr3zi5lRRCGRYrRGXgyaRYpiOAkimLEQ/JBTrb2Wp+1gTBnPFPiRTChD9nOpHkH5x17VLnM8UR5tpEJmY56B+1z1UqkMHXMcjinXOGR1WUk7a9aax7VmPoFqF3bz0UY6pdbyZQvg51F+5htTRqaSy8nC5zKkhknhT6U261Jv0ygpkSaCmGbceE9VLz+eNVe0Ft0nk5wLUEOIQYEIQXutz+jO7NNSz7Jr1meo75jL8HGA67rAwVgyozKoeqEURknOUT038sgEmvoC0ulFrvi2kF7Vy9wpHx2dncTxO//8LDr2T6LGuT8+P2n6x02IxufjiEbn597Tw1PV0ymEGHkKUs5Cqr1Ws+pp4BCi47a+egk14fTyhQjddsm9R/SEmxpFDUwWyDB3wCG4yETbG0hSjp9bf+WJb54nvsds8D0HvVcKEhrHTDCDfvvkgpQyASoPdjXBD57vx1LBRMlMoMF8NH/VWsX3Y22/vIuP3x03Q+rH9DzyT8an4I/paeifvzuF8N3pUfMkPLH0Gkymbajnmtoh5J2Wx0KV6elSsTYq1s7VImiJdskOvp/P+FxOfCN9bSJQqo1pZm0alHpuOpHCz0/jJU447eb8VEHMvrQjGGcTskuamHFoF77x7G4blIn2OVAlfBpFyo+VTPwUQOUUD3hcSUIxiX5eBdND1QMxs8eXn/1wMPh1dD3o9kbXnaueV/VmlGdQdqWn6jrxxeX97V1vOOp3S9SvS8B7OG3sXMqdGwtuhv1PnbveqH9TkH9QMkFPjRnwaAjx8vMNNVMMYkNNpmupjPo33tPTFsP7ny/7F9+E382gaxW5velcvE6bq8H1qHfdvRn0r+9uNwUIpYjZ5Iqmv8Iil+MRMI3a4rAjdNFjQESpZMLoHapeDK4/9D+Ouv1hSbj6jKo6Z+M68tk+n97NL8W6wafecNjvljWrgwntwroTti5noBSLoIbfy9y2/OtFM6cQ1oSM4BoZ7FBmeH/7y1XnZjQcDO5KIq1y2x76Xwa3d69wdKv5p8Hl/VVv1O39fP+xRN/cR3j7a/9mNOzd3g2GvYvB9b4l3atRt3/b+fmyN7rv9j7tI7NyD267t6Ob3nDU7X3qr7nVuhSD67tO/xqD8qrzsUz2e0YXtrxDOrX/WjOHlr0drvva0ykwSk1snQ6yuupdDYZ/H132r/p3mywVaJmpED6UWEdsxjSCLwekcgqv5XGWMKNrCSRSLfbuM+z97b53+6/tpOB3LNr797q4uf9nFGruVChMs907fFNVtnaxvnp/2xsNO9fdwdXoenC95lK2ju3wQOvTneHdqH991xt+6lyW8/7OBff3a8XhfHx21Gw0z/xx9P69f9II3/nvT8KxfwzxOT07h2PaCHfyWS8xu/MTZlGM6k2jaQTYZjN7IuizWGlHBnVZbIfNOsOPtyVB/IS8PdgW4HBLvp8vBxe/jm46d7+Us2YEs7qOxtvafBpdDdYSrKLznWZxyWB0cdm5LculLXjbKHCD7jepbq5g3X4cHpU2TPREHY0s4BiBCNUiRTiWD2ArokDr5Yj35NCHE+JzqcTlkmwdCKJqEDO/qCsItO0WlLs28QlZsoROnk91Oc1NxvmN5CxER+jH19LcKNDYLFU9zmYgQOsbJce2D4MvDgKXUBQip6rnM6/qaYtDQ6/qBUJmJm0H3tsD3JP4Po0SJnzXYJC6ysRKItSo1qhRLR+JszU5CrJG4xjw/9G75mHgBUKF7bf/HggWk8/krQqJL4A0yANxNO4/+Ux+IH5M6iZJ67b74AApefiJmCmIQAQGwqkkgYV+JBdmCpSbKQmnED4SbGwgInNmpriGxJJzOWdiQlChzLRQlCWbtzgYeOQPoiEiPpCKrv933clO6pOKJf3CDAociJgFwsNuG7sHRnkXOF3cQihFhA13o+oZloDMzHLsdNWNuGasyGi2/XV50x5HmnktD50wz9Qt7+gjw56lyHsrqtNGIykTnjaPrpjrxyHMFDOLCykMfDG4JFVsxjhMICpuIBTQaCD4Yiil+cA46IU2kHgt68xVT2Wio++xWWk1bItPlcnSv/znm/pP3n3fTRXoqeSR1zo7ajzjVikoJqMXPW0meZbAlcwQKmMqSvBjngTXMfF6nbDgG9Pi2gLEwXlvupbDbH4rZ6/CpfLstcmnOOsVH3vozvra1zJ89COmtgWwEsvJTiGQA5eTPYvYOPcuRfXOpW5ia3EEsxW1uzjR21SoULZGqjLh25HnxZE6cg7fWC2loWEzasBdUjxULXBnYtJlassGyL0UefUxE/Wx09D/gv9sD52sgtC1Opf93vXdqN9tr4ItEDe9YX/Q7V/07/7ejijji0BcDj6OhoM7bEXtwg/9y17bugGXEyUNNVBzCjjaq85/jm77/9VrnzYaV4FwS9tngQjEGwIRMzaClmsJtvrESCIFX5B8jBJsiljMwiIcuZzgemmmoOZMA5kDmTPOS4y4nFhm2sapzAxRwCWNMExDUIYykXPTyMpMmSYJm0wNmdI0BUHmUxAkybhhKYeClKhMECms0JomQBIaTpmAQNjgZiTw9B8/1ric/PF23bB2LPAwIeyyoU0Z4g1RkHIaAinudKzZXaqZYKEmmQZFmEgzk+/pM+G7NYGn65a+/rZ0cvXJ63fNbReid1aXMpyRWCpCOSellKid9YmZUkHUOPITppRUO4XK2Z4VH946OV6WzCbynKLwpMAjP7RJ4DUCr5y03xAa2cNN6BfN/gFkbRGhztFOzJRwJoDIeMPxXMpy/mfNfULs1aRGb0wosxeAhIkIhLF3b7jplq4njATB6m+nLM8rjQUgEPOpjQOVwU8kkk6/OZBIiorB0whhJb5C8ceZQQIOmwE1ZoKqBZlSEXG3yE4wKciYovzS+VRFO6+yZ+2KCQuZWRAqIoJKoBArtr4/AzWWGp49QONqa/M0CUQkMUy8bwVb89TI5cQPJc8fYrwAnjZhUXMffmpuEjYar8NPFiE9VT1jFgWYer7i/j8ooA+2VRHMXKzfou+rUq4saTDElwSUQnC0/J6yFBAaLQeEzOzHNxaWERaTGVWMjjHzUwXEzi6pvxhFQ3Rl4ZpsRJ+lnjso6uGH2363/Zq7e8cIu/72a3p9R27vyUYfLjsf24Hn+8vHJri97XI7d51Rtz9s74EGgfe2JK9rntuKzgPh+uOlVqs2vMj0Dtbaa8rAI0mmsd7NXGIIvFizKPDIhMsx5XlKzJTLFkaSAoeQvOWFiAxuu7Z+0tjYYlBGwxeQTiua3AwJ1lZQE4hqSDs1JtWten3CzDQb10KZrFJAHb2CmfrR6bvTUzil798fnUOj0aRRfPLu7LTZpO/GEbxvnhyNG/T07BT53WHttrHuJLVIYAxEQSJnEBFODaga6eQiUlPFVBli3bIUtvgjI1SfMPEbhJZPKCOoEexr6JgvCCUTKSOCqBoVsvjAyILH3zImwgXRWZpKZfbpae+M7b8047zebBwfH7+JmA4zd12g3p8fvW+evT87C0SYEt8X0k8x56kZtBMZAbEZw0CS+kub2dMky1SyGgtEujBTKY6JHxL0AJagcPnBplRpUOgWee1rr03ULuyXG/vl4LCgqmEiOqjs2K1ymJdsIYstalOqR9qZ86Di3Kpy2AoEISQn+VwMP5A2+fq0m4W7+1hyqFbwpPYy+uymkWHl7TKcK7auYmWXKYjdGlRJZV45JFTnPBENrG1Smytm4GA1izrnoeWeIxMBc/IIC8XEJBAWMPmChJyBMDXb4BKaIZoD40vl54sQcJeimiDIrVAMJJIqaUGJ1FGFJBO1c1zqqBj/sUL8x5I3uKY6l8g9qq0v5bOhi1HtMgCysSayacZVLofCLAzLU03gkTYiMD5LEIOVQNjd1Y3NW28Pkkf0UeJH1iksPCuSh9QRjpS2IHyWrOat07srh4jkRlllTIeelobKP2MCdueAW4UyXVh9TJLGmkRMWXiwsDmMoGBSIShaTrhltgdgmgiACCIyhpBm2FvYQJ8CwRpGlo+CCdYlXS3tM5FYc+bUYSZmrBcZEMbZFKvYchuboxRoxNfMsYckNYuIKZRynH91m9MwBI1YbezUQjhqz2opjDVoWkJl1ogRqOItlcJQRWkJvPqPOJafWODVC+Nlwpb6kl6aIYMl/ETzEGYIFYsEi5YITOaWbO2xdiBjGj5aroVVcgzuNmHCSOeDik2YoNwquHZCLyq4UubH/bIosEjevsSjpyxFMVyjmAtgYxYhcd0+fbc7T+VcrG2uEAdqjBsLXO2aZ7bMywzscz6VrPG0LxZs6BQI4Bocw0o5eBSdE860qZCD+ZSF0yKSsXCkCIF+k2PUR7tqZSsAlm1E/kJGoA8dU7RMpoS2uvuxYiAiviDuioMgusOihxtg6QynVEzy4HDBqomCsZSmRu7yjuURwf/gtktSGWkSK5kQiwkRHChI7Ws3WFhtYab5clucrY8ZmV+4Mesojqd9k6oQag4OvTBNtMGICqVCq1ZdBMbopkyj0y4nVswdP43KYG+LVouYfrRmipmIXLNkrwSkgBqS4/Fe9m/v7KWlS3GHxTHHmXCwAdeOnIAHh+QrzuFGv/1O5jLjmFdIat0WQWrVdm7u4qEEWbAJm1IVgYDI4ihiGyJdMCvqup5alvCFaZtHbJO+hNlonAKIuZW78YBe6Cr5TWNPixuPqoSJWCI/HKxxSaMDvdA1bSImDmsMD/sAq29g0MYilp8rUkcjV3TbJE/PrXxPxYQ5cFTOLJWHKgERtSsWMixJKrHMRJSfbYtUyL+RzVX2TZBcFFDqkLyxFz3F2eHpOiZGEkfiNsAlmK8PGoeEvMkTmXXPVElMrvb2VuRttWXFtGMViOXiipBuzu1hszq6d79bqFwp8IB5cl6xK1AxrB1QxwLmLoLz3IHO5daH1GwNOnv/UPYwtz5cJ10V5H0C7Nl077aBWXUWBy9t70LC2JsM8/kz8f+xpvHDxn38V5LflKN1LdfAIz/Z8kqa5CfyVKTRfkwoeQQlgJezUpGK3L2dtqFP1/MEuo3jkddaPLUVLBhDKDG7Jcw6QtWVaKTBnlI7Z6LKFMVMGCYymWm+qLnmI1VyzCGxidFGsZZ85uq1hYoqcWjLVn8gM8pZxMyiqIAlKa30yr0hiT6JCSwmAtBHqVoss9Cqs9vXJOYeWbeGyF3nM/Evnauu1hOf4vEcIK7nTDxuzh+6C7f8/MgDKXuXSjbJlyfvbLXhfbaFsXe7eaOkybL7JVpugAwjSUq1JpQ4gpjTya6gKkHHwi1L/raGJ+0XiziKZy1b74V9L2/A/N96xn/Vu+s4ZOTe59mc/4/O5b6pXe9DfMcP1Dce2rx0Kfm/80R3/ZHt654Chq97HvXPPSBbu8b4V28/n212dwXDGt02/wfcoXjLFxNUrP1xBDMfvqQ0f9c3xb2r+6214z3UZarzjZT8W1+FO9l8J+5fXme9bu0gl/2bPb9lt5W/Xt3KHXHzRnz3Zffqwn3jnLFT/NYna5n6S1xvn8Xb2/zv4pD/1Msir/OBv94/+J7fP8BKKWQEt6XfDq3/SgUxSL629I4xOo+0/sSp1tdu3rmLj/z8UDHDQsqdXyPiXgZGh8/pArOaBoUqd0L7kPp6R523LxlRBTeus7te/cik/EDOeR8K6szw1Ut3OeHTHi983ttSJX+D0GCQfPWWAVrGKThh+2g77NBbjtCqHqJkr3V0flQthAqLK2qLfJ7b/OnpwSK8lzSrwxfE3/YXJ/lvgbpFQzRQF8tfaL0YDi9utDycuguG/UHyJ1gVwfFc2Oxi5wJgV/jsol5G0dPuMPoTAr/mYeLoNY8QX3deW8XyRQjkOopVb7EVWusLrJ+5t/9MhtXk6el/AgAA//9QSwcIJ5asp/cRAADrOwAAUEsBAhQAFAAIAAgAAAAAACeWrKf3EQAA6zsAAAgAAAAAAAAAAAAAAAAAAAAAAG9yaWdpbmFsUEsFBgAAAAABAAEANgAAAC0SAAAAAA==
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-02-01T09:38:50Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "0"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "0"
      ceph_daemon_id: "0"
      ceph_daemon_type: osd
      device-class: ssd
      failure-domain: worker-3
      osd: "0"
      osd-store: bluestore
      pod-template-hash: 7b44966dc6
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: worker-3
      topology-location-root: default
    name: rook-ceph-osd-0-7b44966dc6
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-0
      uid: 420a6c3c-fd4c-4123-b244-9fe7e3e54002
    resourceVersion: "13085849"
    uid: d2f34659-5ba4-4d6a-8fcd-6cfa687f8feb
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "0"
        pod-template-hash: 7b44966dc6
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "0"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "0"
          ceph_daemon_id: "0"
          ceph_daemon_type: osd
          device-class: ssd
          failure-domain: worker-3
          osd: "0"
          osd-store: bluestore
          pod-template-hash: 7b44966dc6
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: worker-3
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "0"
          - --fsid
          - 6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=worker-3
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: worker-3
          - name: ROOK_CLUSTER_ID
            value: 02274ff6-97d3-4d09-b941-31edb9bdad99
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: worker-3
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: 9b721017-bd88-40c6-84cb-3ef9a79e3a0c
          - name: ROOK_OSD_ID
            value: "0"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdb
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: ssd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.0.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: osd
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.0.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          workingDir: /var/log/ceph
        - command:
          - /bin/bash
          - -x
          - -e
          - -m
          - -c
          - "\nCEPH_CLIENT_ID=ceph-osd.0\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
            edit the logrotate file to only rotate a specific daemon log\n# otherwise
            we will logrotate log files without reloading certain daemons\n# this
            might happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
            \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
            --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
            rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed
            --in-place \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif
            [ \"$LOG_MAX_SIZE\" != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE
            at the 4th line of the logrotate config file with 4 spaces to maintain
            indentation\n\tsed --in-place \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\"
            \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile true; do\n\t# we don't force the
            logrorate but we let the logrotate binary handle the rotation based on
            user's input for periodicity and size\n\tlogrotate --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep
            15m\ndone\n"
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: log-collector
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4\nOSD_UUID=9b721017-bd88-40c6-84cb-3ef9a79e3a0c\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tceph-volume raw list \"$DEVICE\" >
            \"$OSD_LIST\"\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device < \"$OSD_LIST\";
            then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device
            < \"$OSD_LIST\")\"\n\tfi\n\t[[ -z \"$DEVICE\" ]] && { echo \"no device\"
            ; exit 1 ; }\n\n\t# If a kernel device name change happens and a block
            device file\n\t# in the OSD directory becomes missing, this OSD fails
            to start\n\t# continuously. This problem can be resolved by confirming\n\t#
            the validity of the device file and recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdb
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "0"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: activate
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-0
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: worker-3
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        shareProcessNamespace: true
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/6f3631ca-fa9d-4b5e-ba5c-965ec65214c4_9b721017-bd88-40c6-84cb-3ef9a79e3a0c
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-02-01T09:38:53Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "1"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "1"
      ceph_daemon_id: "1"
      ceph_daemon_type: osd
      device-class: ssd
      failure-domain: worker-1
      osd: "1"
      osd-store: bluestore
      pod-template-hash: 78f8d45768
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: worker-1
      topology-location-root: default
    name: rook-ceph-osd-1-78f8d45768
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-1
      uid: e12e6541-dcd1-48ab-a657-6a6ab6ccd0a1
    resourceVersion: "13085853"
    uid: f1a3c3e2-9068-4b2c-b0c3-6339ec9e38e4
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "1"
        pod-template-hash: 78f8d45768
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "1"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "1"
          ceph_daemon_id: "1"
          ceph_daemon_type: osd
          device-class: ssd
          failure-domain: worker-1
          osd: "1"
          osd-store: bluestore
          pod-template-hash: 78f8d45768
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: worker-1
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "1"
          - --fsid
          - 6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=worker-1
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: worker-1
          - name: ROOK_CLUSTER_ID
            value: 02274ff6-97d3-4d09-b941-31edb9bdad99
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: worker-1
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: 098de1f8-d466-4e51-aef3-874dee214609
          - name: ROOK_OSD_ID
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdb
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: ssd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.1.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: osd
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.1.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          workingDir: /var/log/ceph
        - command:
          - /bin/bash
          - -x
          - -e
          - -m
          - -c
          - "\nCEPH_CLIENT_ID=ceph-osd.1\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
            edit the logrotate file to only rotate a specific daemon log\n# otherwise
            we will logrotate log files without reloading certain daemons\n# this
            might happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
            \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
            --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
            rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed
            --in-place \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif
            [ \"$LOG_MAX_SIZE\" != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE
            at the 4th line of the logrotate config file with 4 spaces to maintain
            indentation\n\tsed --in-place \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\"
            \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile true; do\n\t# we don't force the
            logrorate but we let the logrotate binary handle the rotation based on
            user's input for periodicity and size\n\tlogrotate --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep
            15m\ndone\n"
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: log-collector
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4\nOSD_UUID=098de1f8-d466-4e51-aef3-874dee214609\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tceph-volume raw list \"$DEVICE\" >
            \"$OSD_LIST\"\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device < \"$OSD_LIST\";
            then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device
            < \"$OSD_LIST\")\"\n\tfi\n\t[[ -z \"$DEVICE\" ]] && { echo \"no device\"
            ; exit 1 ; }\n\n\t# If a kernel device name change happens and a block
            device file\n\t# in the OSD directory becomes missing, this OSD fails
            to start\n\t# continuously. This problem can be resolved by confirming\n\t#
            the validity of the device file and recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdb
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "1"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: activate
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-1
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: worker-1
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        shareProcessNamespace: true
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/6f3631ca-fa9d-4b5e-ba5c-965ec65214c4_098de1f8-d466-4e51-aef3-874dee214609
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-02-01T09:38:52Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "2"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "2"
      ceph_daemon_id: "2"
      ceph_daemon_type: osd
      device-class: ssd
      failure-domain: worker-4
      osd: "2"
      osd-store: bluestore
      pod-template-hash: 77f4cc765
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: worker-4
      topology-location-root: default
    name: rook-ceph-osd-2-77f4cc765
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-2
      uid: 8853349f-b64d-40c2-aeea-46c79d5784ba
    resourceVersion: "13085790"
    uid: 26f9962c-e9dd-43a9-9ced-4ec774c34ccd
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "2"
        pod-template-hash: 77f4cc765
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "2"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "2"
          ceph_daemon_id: "2"
          ceph_daemon_type: osd
          device-class: ssd
          failure-domain: worker-4
          osd: "2"
          osd-store: bluestore
          pod-template-hash: 77f4cc765
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: worker-4
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "2"
          - --fsid
          - 6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=worker-4
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: worker-4
          - name: ROOK_CLUSTER_ID
            value: 02274ff6-97d3-4d09-b941-31edb9bdad99
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: worker-4
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: 4dfb6650-7784-48ca-8101-ef967416f326
          - name: ROOK_OSD_ID
            value: "2"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdb
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: ssd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.2.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: osd
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.2.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          workingDir: /var/log/ceph
        - command:
          - /bin/bash
          - -x
          - -e
          - -m
          - -c
          - "\nCEPH_CLIENT_ID=ceph-osd.2\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
            edit the logrotate file to only rotate a specific daemon log\n# otherwise
            we will logrotate log files without reloading certain daemons\n# this
            might happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
            \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
            --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
            rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed
            --in-place \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif
            [ \"$LOG_MAX_SIZE\" != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE
            at the 4th line of the logrotate config file with 4 spaces to maintain
            indentation\n\tsed --in-place \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\"
            \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile true; do\n\t# we don't force the
            logrorate but we let the logrotate binary handle the rotation based on
            user's input for periodicity and size\n\tlogrotate --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep
            15m\ndone\n"
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: log-collector
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4\nOSD_UUID=4dfb6650-7784-48ca-8101-ef967416f326\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tceph-volume raw list \"$DEVICE\" >
            \"$OSD_LIST\"\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device < \"$OSD_LIST\";
            then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device
            < \"$OSD_LIST\")\"\n\tfi\n\t[[ -z \"$DEVICE\" ]] && { echo \"no device\"
            ; exit 1 ; }\n\n\t# If a kernel device name change happens and a block
            device file\n\t# in the OSD directory becomes missing, this OSD fails
            to start\n\t# continuously. This problem can be resolved by confirming\n\t#
            the validity of the device file and recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdb
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "2"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: activate
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-2
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: worker-4
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        shareProcessNamespace: true
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/6f3631ca-fa9d-4b5e-ba5c-965ec65214c4_4dfb6650-7784-48ca-8101-ef967416f326
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-02-01T09:39:41Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "3"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "3"
      ceph_daemon_id: "3"
      ceph_daemon_type: osd
      device-class: ssd
      failure-domain: worker-2
      osd: "3"
      osd-store: bluestore
      pod-template-hash: 665577cf9d
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: worker-2
      topology-location-root: default
    name: rook-ceph-osd-3-665577cf9d
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-3
      uid: 3f639ac3-9697-44d7-b967-6b2a1adbbe24
    resourceVersion: "13085661"
    uid: 651937da-d641-4914-929d-309013c10ab7
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "3"
        pod-template-hash: 665577cf9d
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "3"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "3"
          ceph_daemon_id: "3"
          ceph_daemon_type: osd
          device-class: ssd
          failure-domain: worker-2
          osd: "3"
          osd-store: bluestore
          pod-template-hash: 665577cf9d
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: worker-2
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "3"
          - --fsid
          - 6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=worker-2
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: worker-2
          - name: ROOK_CLUSTER_ID
            value: 02274ff6-97d3-4d09-b941-31edb9bdad99
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: worker-2
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: 3e4616df-9857-4b48-8919-f1f41a479006
          - name: ROOK_OSD_ID
            value: "3"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdb
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: ssd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.3.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: osd
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.3.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-3
            name: activate-osd
          workingDir: /var/log/ceph
        - command:
          - /bin/bash
          - -x
          - -e
          - -m
          - -c
          - "\nCEPH_CLIENT_ID=ceph-osd.3\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
            edit the logrotate file to only rotate a specific daemon log\n# otherwise
            we will logrotate log files without reloading certain daemons\n# this
            might happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
            \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
            --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
            rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed
            --in-place \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif
            [ \"$LOG_MAX_SIZE\" != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE
            at the 4th line of the logrotate config file with 4 spaces to maintain
            indentation\n\tsed --in-place \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\"
            \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile true; do\n\t# we don't force the
            logrorate but we let the logrotate binary handle the rotation based on
            user's input for periodicity and size\n\tlogrotate --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep
            15m\ndone\n"
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: log-collector
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4\nOSD_UUID=3e4616df-9857-4b48-8919-f1f41a479006\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tceph-volume raw list \"$DEVICE\" >
            \"$OSD_LIST\"\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device < \"$OSD_LIST\";
            then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device
            < \"$OSD_LIST\")\"\n\tfi\n\t[[ -z \"$DEVICE\" ]] && { echo \"no device\"
            ; exit 1 ; }\n\n\t# If a kernel device name change happens and a block
            device file\n\t# in the OSD directory becomes missing, this OSD fails
            to start\n\t# continuously. This problem can be resolved by confirming\n\t#
            the validity of the device file and recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdb
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "3"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: activate
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-3
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-3
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-3
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-3
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: worker-2
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        shareProcessNamespace: true
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/6f3631ca-fa9d-4b5e-ba5c-965ec65214c4_3e4616df-9857-4b48-8919-f1f41a479006
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-02-01T09:40:48Z"
    generation: 1
    labels:
      app: rook-ceph-rgw
      app.kubernetes.io/component: cephobjectstores.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: ceph-objectstore
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-rgw
      app.kubernetes.io/part-of: ceph-objectstore
      ceph_daemon_id: ceph-objectstore
      ceph_daemon_type: rgw
      pod-template-hash: 7cb6c484c4
      rgw: ceph-objectstore
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      rook_object_store: ceph-objectstore
    name: rook-ceph-rgw-ceph-objectstore-a-7cb6c484c4
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-rgw-ceph-objectstore-a
      uid: a4edfa15-76e5-4947-a89a-9250fb0d82d6
    resourceVersion: "13086170"
    uid: e8253e36-ea2c-4a1a-a05b-44b000818a7a
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-rgw
        ceph_daemon_id: ceph-objectstore
        pod-template-hash: 7cb6c484c4
        rgw: ceph-objectstore
        rook_cluster: rook-ceph
        rook_object_store: ceph-objectstore
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-rgw
          app.kubernetes.io/component: cephobjectstores.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: ceph-objectstore
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-rgw
          app.kubernetes.io/part-of: ceph-objectstore
          ceph_daemon_id: ceph-objectstore
          ceph_daemon_type: rgw
          pod-template-hash: 7cb6c484c4
          rgw: ceph-objectstore
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          rook_object_store: ceph-objectstore
        name: rook-ceph-rgw-ceph-objectstore-a
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: rook-ceph-rgw
                    ceph_daemon_id: ceph-objectstore
                    rgw: ceph-objectstore
                    rook_cluster: rook-ceph
                    rook_object_store: ceph-objectstore
                topologyKey: kubernetes.io/hostname
              weight: 50
        containers:
        - args:
          - --fsid=6f3631ca-fa9d-4b5e-ba5c-965ec65214c4
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=rgw.ceph.objectstore.a
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --rgw-frontends=beast port=8080
          - --host=$(POD_NAME)
          - --rgw-mime-types-file=/etc/ceph/rgw/mime.types
          - --rgw-realm=ceph-objectstore
          - --rgw-zonegroup=ceph-objectstore
          - --rgw-zone=ceph-objectstore
          command:
          - radosgw
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v17.2.6
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_MSGR2
            value: msgr2_false_encryption_false_compression_false
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: rgw
          readinessProbe:
            exec:
              command:
              - bash
              - -c
              - |
                #!/usr/bin/env bash

                PROBE_TYPE="readiness"
                PROBE_PORT="8080"
                PROBE_PROTOCOL="HTTP"

                # standard bash codes start at 126 and progress upward. pick error codes from 125 downward for
                # script as to allow curl to output new error codes and still return a distinctive number.
                USAGE_ERR_CODE=125
                PROBE_ERR_CODE=124
                # curl error codes: 1-123

                STARTUP_TYPE='startup'
                READINESS_TYPE='readiness'

                RGW_URL="$PROBE_PROTOCOL://0.0.0.0:$PROBE_PORT"

                function check() {
                  local URL="$1"
                  # --insecure - don't validate ssl if using secure port only
                  # --silent - don't output progress info
                  # --output /dev/stderr - output HTML header to stdout (good for debugging)
                  # --write-out '%{response_code}' - print the HTTP response code to stdout
                  curl --insecure --silent --output /dev/stderr --write-out '%{response_code}' "$URL"
                }

                http_response="$(check "$RGW_URL")"
                retcode=$?

                if [[ $retcode -ne 0 ]]; then
                  # if this is the startup probe, always returning failure. if startup probe passes, all subsequent
                  # probes can rely on the assumption that the health check was once succeeding without errors.
                  # if this is the readiness probe, we know that curl was previously working correctly in the
                  # startup probe, so curl error most likely means some new error with the RGW.
                  echo "RGW health check failed with error code: $retcode. the RGW likely cannot be reached by clients" >/dev/stderr
                  exit $retcode
                fi

                RGW_RATE_LIMITING_RESPONSE=503
                RGW_MISCONFIGURATION_RESPONSE=500

                if [[ $http_response -ge 200 ]] && [[ $http_response -lt 400 ]]; then
                  # 200-399 are successful responses. same behavior as Kubernetes' HTTP probe
                  exit 0

                elif [[ $http_response -eq $RGW_RATE_LIMITING_RESPONSE ]]; then
                  # S3's '503: slow down' code is not an error but an indication that RGW is throttling client
                  # traffic. failing the readiness check here would only cause an increase in client connections on
                  # other RGWs and likely cause those to fail also in a cascade. i.e., a special healthy response.
                  echo "INFO: RGW is rate limiting" 2>/dev/stderr
                  exit 0

                elif [[ $http_response -eq $RGW_MISCONFIGURATION_RESPONSE ]]; then
                  # can't specifically determine if the RGW is running or not. most likely a misconfiguration.
                  case "$PROBE_TYPE" in
                  "$STARTUP_TYPE")
                    # fail until we can accurately get a valid healthy response when runtime starts.
                    echo 'FAIL: HTTP code 500 suggests an RGW misconfiguration.' >/dev/stderr
                    exit $PROBE_ERR_CODE
                    ;;
                  "$READINESS_TYPE")
                    # config likely modified at runtime which could result in all RGWs failing this check.
                    # occasional client failures are still better than total failure, so ignore this
                    echo 'WARN: HTTP code 500 suggests an RGW misconfiguration' >/dev/stderr
                    exit 0
                    ;;
                  *)
                    # prior arg validation means this path should never be activated, but keep to be safe
                    echo "ERROR: probe type is unknown: $PROBE_TYPE" >/dev/stderr
                    exit $USAGE_ERR_CODE
                    ;;
                  esac

                else
                  # anything else is a failing response. same behavior as Kubernetes' HTTP probe
                  echo "FAIL: received an HTTP error code: $http_response"
                  exit $PROBE_ERR_CODE

                fi
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 3
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 256Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - bash
              - -c
              - |
                #!/usr/bin/env bash

                PROBE_TYPE="startup"
                PROBE_PORT="8080"
                PROBE_PROTOCOL="HTTP"

                # standard bash codes start at 126 and progress upward. pick error codes from 125 downward for
                # script as to allow curl to output new error codes and still return a distinctive number.
                USAGE_ERR_CODE=125
                PROBE_ERR_CODE=124
                # curl error codes: 1-123

                STARTUP_TYPE='startup'
                READINESS_TYPE='readiness'

                RGW_URL="$PROBE_PROTOCOL://0.0.0.0:$PROBE_PORT"

                function check() {
                  local URL="$1"
                  # --insecure - don't validate ssl if using secure port only
                  # --silent - don't output progress info
                  # --output /dev/stderr - output HTML header to stdout (good for debugging)
                  # --write-out '%{response_code}' - print the HTTP response code to stdout
                  curl --insecure --silent --output /dev/stderr --write-out '%{response_code}' "$URL"
                }

                http_response="$(check "$RGW_URL")"
                retcode=$?

                if [[ $retcode -ne 0 ]]; then
                  # if this is the startup probe, always returning failure. if startup probe passes, all subsequent
                  # probes can rely on the assumption that the health check was once succeeding without errors.
                  # if this is the readiness probe, we know that curl was previously working correctly in the
                  # startup probe, so curl error most likely means some new error with the RGW.
                  echo "RGW health check failed with error code: $retcode. the RGW likely cannot be reached by clients" >/dev/stderr
                  exit $retcode
                fi

                RGW_RATE_LIMITING_RESPONSE=503
                RGW_MISCONFIGURATION_RESPONSE=500

                if [[ $http_response -ge 200 ]] && [[ $http_response -lt 400 ]]; then
                  # 200-399 are successful responses. same behavior as Kubernetes' HTTP probe
                  exit 0

                elif [[ $http_response -eq $RGW_RATE_LIMITING_RESPONSE ]]; then
                  # S3's '503: slow down' code is not an error but an indication that RGW is throttling client
                  # traffic. failing the readiness check here would only cause an increase in client connections on
                  # other RGWs and likely cause those to fail also in a cascade. i.e., a special healthy response.
                  echo "INFO: RGW is rate limiting" 2>/dev/stderr
                  exit 0

                elif [[ $http_response -eq $RGW_MISCONFIGURATION_RESPONSE ]]; then
                  # can't specifically determine if the RGW is running or not. most likely a misconfiguration.
                  case "$PROBE_TYPE" in
                  "$STARTUP_TYPE")
                    # fail until we can accurately get a valid healthy response when runtime starts.
                    echo 'FAIL: HTTP code 500 suggests an RGW misconfiguration.' >/dev/stderr
                    exit $PROBE_ERR_CODE
                    ;;
                  "$READINESS_TYPE")
                    # config likely modified at runtime which could result in all RGWs failing this check.
                    # occasional client failures are still better than total failure, so ignore this
                    echo 'WARN: HTTP code 500 suggests an RGW misconfiguration' >/dev/stderr
                    exit 0
                    ;;
                  *)
                    # prior arg validation means this path should never be activated, but keep to be safe
                    echo "ERROR: probe type is unknown: $PROBE_TYPE" >/dev/stderr
                    exit $USAGE_ERR_CODE
                    ;;
                  esac

                else
                  # anything else is a failing response. same behavior as Kubernetes' HTTP probe
                  echo "FAIL: received an HTTP error code: $http_response"
                  exit $PROBE_ERR_CODE

                fi
            failureThreshold: 33
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-rgw-ceph-objectstore-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/rgw/ceph-ceph-objectstore
            name: ceph-daemon-data
          - mountPath: /etc/ceph/rgw
            name: rook-ceph-rgw-ceph-objectstore-mime-types
            readOnly: true
          workingDir: /var/log/ceph
        - command:
          - /bin/bash
          - -x
          - -e
          - -m
          - -c
          - "\nCEPH_CLIENT_ID=ceph-client.rgw.ceph.objectstore.a\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
            edit the logrotate file to only rotate a specific daemon log\n# otherwise
            we will logrotate log files without reloading certain daemons\n# this
            might happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
            \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
            --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
            rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed
            --in-place \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif
            [ \"$LOG_MAX_SIZE\" != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE
            at the 4th line of the logrotate config file with 4 spaces to maintain
            indentation\n\tsed --in-place \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\"
            \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile true; do\n\t# we don't force the
            logrorate but we let the logrotate binary handle the rotation based on
            user's input for periodicity and size\n\tlogrotate --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep
            15m\ndone\n"
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: log-collector
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/rgw/ceph-ceph-objectstore
          command:
          - chown
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 256Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-rgw-ceph-objectstore-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/rgw/ceph-ceph-objectstore
            name: ceph-daemon-data
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-rgw
        serviceAccountName: rook-ceph-rgw
        shareProcessNamespace: true
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-rgw-ceph-objectstore-a-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-rgw-ceph-objectstore-a-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: ceph-daemon-data
        - configMap:
            defaultMode: 420
            name: rook-ceph-rgw-ceph-objectstore-mime-types
          name: rook-ceph-rgw-ceph-objectstore-mime-types
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-02-01T03:54:45Z"
    generation: 1
    labels:
      app: rook-ceph-tools
      pod-template-hash: 6d6d694fb9
    name: rook-ceph-tools-6d6d694fb9
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-tools
      uid: 220a73a0-4a54-47f2-ba1f-876f6fb99a4b
    resourceVersion: "13085044"
    uid: aae30c56-fcc3-400b-b138-85f447de9eaf
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-tools
        pod-template-hash: 6d6d694fb9
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-tools
          pod-template-hash: 6d6d694fb9
      spec:
        containers:
        - command:
          - /bin/bash
          - -c
          - |
            # Replicate the script from toolbox.sh inline so the ceph image
            # can be run directly, instead of requiring the rook toolbox
            CEPH_CONFIG="/etc/ceph/ceph.conf"
            MON_CONFIG="/etc/rook/mon-endpoints"
            KEYRING_FILE="/etc/ceph/keyring"

            # create a ceph config file in its default location so ceph/rados tools can be used
            # without specifying any arguments
            write_endpoints() {
              endpoints=$(cat ${MON_CONFIG})

              # filter out the mon names
              # external cluster can have numbers or hyphens in mon names, handling them in regex
              # shellcheck disable=SC2001
              mon_endpoints=$(echo "${endpoints}"| sed 's/[a-z0-9_-]\+=//g')

              DATE=$(date)
              echo "$DATE writing mon endpoints to ${CEPH_CONFIG}: ${endpoints}"
                cat <<EOF > ${CEPH_CONFIG}
            [global]
            mon_host = ${mon_endpoints}

            [client.admin]
            keyring = ${KEYRING_FILE}
            EOF
            }

            # watch the endpoints config file and update if the mon endpoints ever change
            watch_endpoints() {
              # get the timestamp for the target of the soft link
              real_path=$(realpath ${MON_CONFIG})
              initial_time=$(stat -c %Z "${real_path}")
              while true; do
                real_path=$(realpath ${MON_CONFIG})
                latest_time=$(stat -c %Z "${real_path}")

                if [[ "${latest_time}" != "${initial_time}" ]]; then
                  write_endpoints
                  initial_time=${latest_time}
                fi

                sleep 10
              done
            }

            # read the secret from an env var (for backward compatibility), or from the secret file
            ceph_secret=${ROOK_CEPH_SECRET}
            if [[ "$ceph_secret" == "" ]]; then
              ceph_secret=$(cat /var/lib/rook-ceph-mon/secret.keyring)
            fi

            # create the keyring file
            cat <<EOF > ${KEYRING_FILE}
            [${ROOK_CEPH_USERNAME}]
            key = ${ceph_secret}
            EOF

            # write the initial config file
            write_endpoints

            # continuously update the mon endpoints if they fail over
            watch_endpoints
          env:
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          image: quay.io/ceph/ceph:v18.2.1
          imagePullPolicy: IfNotPresent
          name: rook-ceph-tools
          resources: {}
          securityContext:
            capabilities:
              drop:
              - ALL
            runAsGroup: 2016
            runAsNonRoot: true
            runAsUser: 2016
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: ceph-config
          - mountPath: /etc/rook
            name: mon-endpoint-volume
          - mountPath: /var/lib/rook-ceph-mon
            name: ceph-admin-secret
            readOnly: true
        dnsPolicy: ClusterFirstWithHostNet
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: ceph-admin-secret
          secret:
            defaultMode: 420
            items:
            - key: ceph-secret
              path: secret.keyring
            optional: false
            secretName: rook-ceph-mon
        - configMap:
            defaultMode: 420
            items:
            - key: data
              path: mon-endpoints
            name: rook-ceph-mon-endpoints
          name: mon-endpoint-volume
        - emptyDir: {}
          name: ceph-config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: tigera-operator
      meta.helm.sh/release-namespace: tigera-operator
    creationTimestamp: "2024-03-13T16:11:38Z"
    generation: 1
    labels:
      k8s-app: tigera-operator
      name: tigera-operator
      pod-template-hash: 5cc9b4b697
    name: tigera-operator-5cc9b4b697
    namespace: tigera-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: tigera-operator
      uid: f8bd4cac-b919-4c74-bfd5-c052a3e70e25
    resourceVersion: "13072670"
    uid: feafa0d3-4666-44c3-920d-6da969025ffb
  spec:
    replicas: 1
    selector:
      matchLabels:
        name: tigera-operator
        pod-template-hash: 5cc9b4b697
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: tigera-operator
          name: tigera-operator
          pod-template-hash: 5cc9b4b697
      spec:
        containers:
        - command:
          - operator
          env:
          - name: WATCH_NAMESPACE
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: OPERATOR_NAME
            value: tigera-operator
          - name: TIGERA_OPERATOR_INIT_IMAGE_VERSION
            value: master
          envFrom:
          - configMapRef:
              name: kubernetes-services-endpoint
              optional: true
          image: quay.io/tigera/operator:master
          imagePullPolicy: IfNotPresent
          name: tigera-operator
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/calico
            name: var-lib-calico
            readOnly: true
        dnsPolicy: ClusterFirstWithHostNet
        hostNetwork: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: tigera-operator
        serviceAccountName: tigera-operator
        terminationGracePeriodSeconds: 60
        tolerations:
        - effect: NoExecute
          operator: Exists
        - effect: NoSchedule
          operator: Exists
        volumes:
        - hostPath:
            path: /var/lib/calico
            type: ""
          name: var-lib-calico
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: postgresql-ha
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-02-13T12:27:05Z"
    generation: 1
    labels:
      app.kubernetes.io/component: postgresql
      app.kubernetes.io/instance: postgresql-ha
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql-ha
      app.kubernetes.io/version: 16.2.0
      helm.sh/chart: postgresql-ha-13.2.3
      role: data
    name: postgresql-ha-postgresql
    namespace: airflow
    resourceVersion: "13087270"
    uid: 6b9ab1b3-c15a-4c34-9379-7698e6a5fce2
  spec:
    podManagementPolicy: Parallel
    replicas: 3
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: postgresql
        app.kubernetes.io/instance: postgresql-ha
        app.kubernetes.io/name: postgresql-ha
        role: data
    serviceName: postgresql-ha-postgresql-headless
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: postgresql
          app.kubernetes.io/instance: postgresql-ha
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: postgresql-ha
          app.kubernetes.io/version: 16.2.0
          helm.sh/chart: postgresql-ha-13.2.3
          role: data
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/component: postgresql
                  app.kubernetes.io/instance: postgresql-ha
                  app.kubernetes.io/name: postgresql-ha
              topologyKey: kubernetes.io/hostname
        automountServiceAccountToken: false
        containers:
        - env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: POSTGRESQL_VOLUME_DIR
            value: /bitnami/postgresql
          - name: PGDATA
            value: /bitnami/postgresql/data
          - name: POSTGRES_USER
            value: postgres
          - name: POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: postgresql-ha-postgresql
          - name: POSTGRES_DB
            value: airflow
          - name: POSTGRESQL_LOG_HOSTNAME
            value: "true"
          - name: POSTGRESQL_LOG_CONNECTIONS
            value: "false"
          - name: POSTGRESQL_LOG_DISCONNECTIONS
            value: "false"
          - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
            value: "off"
          - name: POSTGRESQL_CLIENT_MIN_MESSAGES
            value: error
          - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
            value: pgaudit, repmgr
          - name: POSTGRESQL_ENABLE_TLS
            value: "no"
          - name: POSTGRESQL_PORT_NUMBER
            value: "5432"
          - name: REPMGR_PORT_NUMBER
            value: "5432"
          - name: REPMGR_PRIMARY_PORT
            value: "5432"
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: REPMGR_UPGRADE_EXTENSION
            value: "no"
          - name: REPMGR_PGHBA_TRUST_ALL
            value: "no"
          - name: REPMGR_MOUNTED_CONF_DIR
            value: /bitnami/repmgr/conf
          - name: REPMGR_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: REPMGR_PARTNER_NODES
            value: postgresql-ha-postgresql-0.postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local,postgresql-ha-postgresql-1.postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local,postgresql-ha-postgresql-2.postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local,
          - name: REPMGR_PRIMARY_HOST
            value: postgresql-ha-postgresql-0.postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local
          - name: REPMGR_NODE_NAME
            value: $(MY_POD_NAME)
          - name: REPMGR_NODE_NETWORK_NAME
            value: $(MY_POD_NAME).postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local
          - name: REPMGR_NODE_TYPE
            value: data
          - name: REPMGR_LOG_LEVEL
            value: NOTICE
          - name: REPMGR_CONNECT_TIMEOUT
            value: "5"
          - name: REPMGR_RECONNECT_ATTEMPTS
            value: "2"
          - name: REPMGR_RECONNECT_INTERVAL
            value: "3"
          - name: REPMGR_USERNAME
            value: repmgr
          - name: REPMGR_PASSWORD
            valueFrom:
              secretKeyRef:
                key: repmgr-password
                name: postgresql-ha-postgresql
          - name: REPMGR_DATABASE
            value: repmgr
          - name: REPMGR_FENCE_OLD_PRIMARY
            value: "no"
          - name: REPMGR_CHILD_NODES_CHECK_INTERVAL
            value: "5"
          - name: REPMGR_CHILD_NODES_CONNECTED_MIN_COUNT
            value: "1"
          - name: REPMGR_CHILD_NODES_DISCONNECT_TIMEOUT
            value: "30"
          image: docker.io/bitnami/postgresql-repmgr:16.2.0-debian-11-r1
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /pre-stop.sh
                - "25"
          livenessProbe:
            exec:
              command:
              - bash
              - -ec
              - PGPASSWORD=$POSTGRES_PASSWORD psql -w -U "postgres" -d "airflow" -h
                127.0.0.1 -p 5432 -c "SELECT 1"
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: postgresql
          ports:
          - containerPort: 5432
            name: postgresql
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - bash
              - -ec
              - PGPASSWORD=$POSTGRES_PASSWORD psql -w -U "postgres" -d "airflow" -h
                127.0.0.1 -p 5432 -c "SELECT 1"
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /bitnami/postgresql
            name: data
          - mountPath: /pre-stop.sh
            name: hooks-scripts
            subPath: pre-stop.sh
          - mountPath: /readiness-probe.sh
            name: hooks-scripts
            subPath: readiness-probe.sh
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          fsGroupChangePolicy: Always
        serviceAccount: postgresql-ha
        serviceAccountName: postgresql-ha
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 493
            name: postgresql-ha-postgresql-hooks-scripts
          name: hooks-scripts
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 8Gi
        storageClassName: ceph-block
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 3
    collisionCount: 0
    currentReplicas: 3
    currentRevision: postgresql-ha-postgresql-5bf76cd696
    observedGeneration: 1
    readyReplicas: 3
    replicas: 3
    updateRevision: postgresql-ha-postgresql-5bf76cd696
    updatedReplicas: 3
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
      prometheus-operator-input-hash: "1452430108220233800"
    creationTimestamp: "2024-02-02T10:31:24Z"
    generation: 1
    labels:
      app: kube-prometheus-stack-alertmanager
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 55.5.1
      chart: kube-prometheus-stack-55.5.1
      heritage: Helm
      release: kube-prometheus-stack
    name: alertmanager-kube-prometheus-stack-alertmanager
    namespace: monitoring
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      blockOwnerDeletion: true
      controller: true
      kind: Alertmanager
      name: kube-prometheus-stack-alertmanager
      uid: 8f669cdb-5756-4c52-8339-056f5c5dbf22
    resourceVersion: "13086224"
    uid: 516570fd-990e-48d4-9110-8801a42180e0
  spec:
    podManagementPolicy: Parallel
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        alertmanager: kube-prometheus-stack-alertmanager
        app.kubernetes.io/instance: kube-prometheus-stack-alertmanager
        app.kubernetes.io/managed-by: prometheus-operator
        app.kubernetes.io/name: alertmanager
    serviceName: alertmanager-operated
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/default-container: alertmanager
        creationTimestamp: null
        labels:
          alertmanager: kube-prometheus-stack-alertmanager
          app.kubernetes.io/instance: kube-prometheus-stack-alertmanager
          app.kubernetes.io/managed-by: prometheus-operator
          app.kubernetes.io/name: alertmanager
          app.kubernetes.io/version: 0.26.0
      spec:
        containers:
        - args:
          - --config.file=/etc/alertmanager/config_out/alertmanager.env.yaml
          - --storage.path=/alertmanager
          - --data.retention=120h
          - --cluster.listen-address=
          - --web.listen-address=:9093
          - --web.external-url=http://kube-prometheus-stack-alertmanager.monitoring:9093
          - --web.route-prefix=/alertmanager
          - --cluster.label=monitoring/kube-prometheus-stack-alertmanager
          - --cluster.peer=alertmanager-kube-prometheus-stack-alertmanager-0.alertmanager-operated:9094
          - --cluster.reconnect-timeout=5m
          - --web.config.file=/etc/alertmanager/web_config/web-config.yaml
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/prometheus/alertmanager:v0.26.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /alertmanager/-/healthy
              port: http-web
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
          name: alertmanager
          ports:
          - containerPort: 9093
            name: http-web
            protocol: TCP
          - containerPort: 9094
            name: mesh-tcp
            protocol: TCP
          - containerPort: 9094
            name: mesh-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 10
            httpGet:
              path: /alertmanager/-/ready
              port: http-web
              scheme: HTTP
            initialDelaySeconds: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            requests:
              memory: 200Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/alertmanager/config
            name: config-volume
          - mountPath: /etc/alertmanager/config_out
            name: config-out
            readOnly: true
          - mountPath: /etc/alertmanager/certs
            name: tls-assets
            readOnly: true
          - mountPath: /alertmanager
            name: alertmanager-kube-prometheus-stack-alertmanager-db
            subPath: alertmanager-db
          - mountPath: /etc/alertmanager/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
        - args:
          - --listen-address=:8080
          - --reload-url=http://127.0.0.1:9093/alertmanager/-/reload
          - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
          - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
          - --watched-dir=/etc/alertmanager/config
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "-1"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
          imagePullPolicy: IfNotPresent
          name: config-reloader
          ports:
          - containerPort: 8080
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/alertmanager/config
            name: config-volume
            readOnly: true
          - mountPath: /etc/alertmanager/config_out
            name: config-out
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --watch-interval=0
          - --listen-address=:8080
          - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
          - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
          - --watched-dir=/etc/alertmanager/config
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "-1"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
          imagePullPolicy: IfNotPresent
          name: init-config-reloader
          ports:
          - containerPort: 8080
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/alertmanager/config
            name: config-volume
            readOnly: true
          - mountPath: /etc/alertmanager/config_out
            name: config-out
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 2000
          runAsGroup: 2000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: kube-prometheus-stack-alertmanager
        serviceAccountName: kube-prometheus-stack-alertmanager
        terminationGracePeriodSeconds: 120
        volumes:
        - name: config-volume
          secret:
            defaultMode: 420
            secretName: alertmanager-kube-prometheus-stack-alertmanager-generated
        - name: tls-assets
          projected:
            defaultMode: 420
            sources:
            - secret:
                name: alertmanager-kube-prometheus-stack-alertmanager-tls-assets-0
        - emptyDir:
            medium: Memory
          name: config-out
        - name: web-config
          secret:
            defaultMode: 420
            secretName: alertmanager-kube-prometheus-stack-alertmanager-web-config
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: alertmanager-kube-prometheus-stack-alertmanager-db
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
        storageClassName: ceph-block
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: alertmanager-kube-prometheus-stack-alertmanager-6c5bff765
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: alertmanager-kube-prometheus-stack-alertmanager-6c5bff765
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
      prometheus-operator-input-hash: "11685362654000569107"
    creationTimestamp: "2024-02-02T10:31:24Z"
    generation: 1
    labels:
      app: kube-prometheus-stack-prometheus
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 55.5.1
      chart: kube-prometheus-stack-55.5.1
      heritage: Helm
      operator.prometheus.io/mode: server
      operator.prometheus.io/name: kube-prometheus-stack-prometheus
      operator.prometheus.io/shard: "0"
      release: kube-prometheus-stack
    name: prometheus-kube-prometheus-stack-prometheus
    namespace: monitoring
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      blockOwnerDeletion: true
      controller: true
      kind: Prometheus
      name: kube-prometheus-stack-prometheus
      uid: 98d1fc63-6335-4d02-9f54-749c6a7870f8
    resourceVersion: "13087072"
    uid: 34962cd4-23e7-4568-a91d-2dc616931919
  spec:
    podManagementPolicy: Parallel
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack-prometheus
        app.kubernetes.io/managed-by: prometheus-operator
        app.kubernetes.io/name: prometheus
        operator.prometheus.io/name: kube-prometheus-stack-prometheus
        operator.prometheus.io/shard: "0"
        prometheus: kube-prometheus-stack-prometheus
    serviceName: prometheus-operated
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/default-container: prometheus
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kube-prometheus-stack-prometheus
          app.kubernetes.io/managed-by: prometheus-operator
          app.kubernetes.io/name: prometheus
          app.kubernetes.io/version: 2.48.1
          operator.prometheus.io/name: kube-prometheus-stack-prometheus
          operator.prometheus.io/shard: "0"
          prometheus: kube-prometheus-stack-prometheus
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --web.console.templates=/etc/prometheus/consoles
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --config.file=/etc/prometheus/config_out/prometheus.env.yaml
          - --web.enable-lifecycle
          - --web.external-url=/prometheus
          - --web.route-prefix=/prometheus
          - --storage.tsdb.retention.time=14d
          - --storage.tsdb.path=/prometheus
          - --storage.tsdb.wal-compression
          - --web.config.file=/etc/prometheus/web_config/web-config.yaml
          image: quay.io/prometheus/prometheus:v2.48.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /prometheus/-/healthy
              port: http-web
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          name: prometheus
          ports:
          - containerPort: 9090
            name: http-web
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /prometheus/-/ready
              port: http-web
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /prometheus/-/ready
              port: http-web
              scheme: HTTP
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 3
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/prometheus/config_out
            name: config-out
            readOnly: true
          - mountPath: /etc/prometheus/certs
            name: tls-assets
            readOnly: true
          - mountPath: /prometheus
            name: prometheus-kube-prometheus-stack-prometheus-db
            subPath: prometheus-db
          - mountPath: /etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0
            name: prometheus-kube-prometheus-stack-prometheus-rulefiles-0
          - mountPath: /etc/prometheus/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
        - args:
          - --listen-address=:8080
          - --reload-url=http://127.0.0.1:9090/prometheus/-/reload
          - --config-file=/etc/prometheus/config/prometheus.yaml.gz
          - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
          - --watched-dir=/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "0"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
          imagePullPolicy: IfNotPresent
          name: config-reloader
          ports:
          - containerPort: 8080
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/prometheus/config
            name: config
          - mountPath: /etc/prometheus/config_out
            name: config-out
          - mountPath: /etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0
            name: prometheus-kube-prometheus-stack-prometheus-rulefiles-0
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --watch-interval=0
          - --listen-address=:8080
          - --config-file=/etc/prometheus/config/prometheus.yaml.gz
          - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
          - --watched-dir=/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "0"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
          imagePullPolicy: IfNotPresent
          name: init-config-reloader
          ports:
          - containerPort: 8080
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/prometheus/config
            name: config
          - mountPath: /etc/prometheus/config_out
            name: config-out
          - mountPath: /etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0
            name: prometheus-kube-prometheus-stack-prometheus-rulefiles-0
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 2000
          runAsGroup: 2000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: kube-prometheus-stack-prometheus
        serviceAccountName: kube-prometheus-stack-prometheus
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 600
        volumes:
        - name: config
          secret:
            defaultMode: 420
            secretName: prometheus-kube-prometheus-stack-prometheus
        - name: tls-assets
          projected:
            defaultMode: 420
            sources:
            - secret:
                name: prometheus-kube-prometheus-stack-prometheus-tls-assets-0
        - emptyDir:
            medium: Memory
          name: config-out
        - configMap:
            defaultMode: 420
            name: prometheus-kube-prometheus-stack-prometheus-rulefiles-0
          name: prometheus-kube-prometheus-stack-prometheus-rulefiles-0
        - name: web-config
          secret:
            defaultMode: 420
            secretName: prometheus-kube-prometheus-stack-prometheus-web-config
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: prometheus-kube-prometheus-stack-prometheus-db
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 30Gi
        storageClassName: ceph-block
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: prometheus-kube-prometheus-stack-prometheus-76dc8d49f4
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: prometheus-kube-prometheus-stack-prometheus-76dc8d49f4
    updatedReplicas: 1
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/job-tracking: ""
    creationTimestamp: "2024-01-26T07:18:47Z"
    generation: 1
    labels:
      controller-uid: 24e08827-0037-4738-b671-7734805b7c26
      job-name: rke-coredns-addon-deploy-job
    name: rke-coredns-addon-deploy-job
    namespace: kube-system
    resourceVersion: "513"
    uid: 24e08827-0037-4738-b671-7734805b7c26
  spec:
    backoffLimit: 10
    completionMode: NonIndexed
    completions: 1
    parallelism: 1
    selector:
      matchLabels:
        controller-uid: 24e08827-0037-4738-b671-7734805b7c26
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          controller-uid: 24e08827-0037-4738-b671-7734805b7c26
          job-name: rke-coredns-addon-deploy-job
        name: rke-deploy
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/os
                  operator: NotIn
                  values:
                  - windows
        containers:
        - command:
          - kubectl
          - apply
          - -f
          - /etc/config/rke-coredns-addon.yaml
          image: rancher/hyperkube:v1.26.9-rancher1
          imagePullPolicy: IfNotPresent
          name: rke-coredns-addon-pod
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeName: master-1
        restartPolicy: Never
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rke-job-deployer
        serviceAccountName: rke-job-deployer
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: rke-coredns-addon
              path: rke-coredns-addon.yaml
            name: rke-coredns-addon
          name: config-volume
  status:
    completionTime: "2024-01-26T07:18:53Z"
    conditions:
    - lastProbeTime: "2024-01-26T07:18:53Z"
      lastTransitionTime: "2024-01-26T07:18:53Z"
      status: "True"
      type: Complete
    ready: 0
    startTime: "2024-01-26T07:18:47Z"
    succeeded: 1
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/job-tracking: ""
    creationTimestamp: "2024-01-26T07:18:57Z"
    generation: 1
    labels:
      controller-uid: a5ef1f8d-3373-49fc-a2c2-27fefb6d62c6
      job-name: rke-metrics-addon-deploy-job
    name: rke-metrics-addon-deploy-job
    namespace: kube-system
    resourceVersion: "603"
    uid: a5ef1f8d-3373-49fc-a2c2-27fefb6d62c6
  spec:
    backoffLimit: 10
    completionMode: NonIndexed
    completions: 1
    parallelism: 1
    selector:
      matchLabels:
        controller-uid: a5ef1f8d-3373-49fc-a2c2-27fefb6d62c6
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          controller-uid: a5ef1f8d-3373-49fc-a2c2-27fefb6d62c6
          job-name: rke-metrics-addon-deploy-job
        name: rke-deploy
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/os
                  operator: NotIn
                  values:
                  - windows
        containers:
        - command:
          - kubectl
          - apply
          - -f
          - /etc/config/rke-metrics-addon.yaml
          image: rancher/hyperkube:v1.26.9-rancher1
          imagePullPolicy: IfNotPresent
          name: rke-metrics-addon-pod
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeName: master-1
        restartPolicy: Never
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rke-job-deployer
        serviceAccountName: rke-job-deployer
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: rke-metrics-addon
              path: rke-metrics-addon.yaml
            name: rke-metrics-addon
          name: config-volume
  status:
    completionTime: "2024-01-26T07:19:03Z"
    conditions:
    - lastProbeTime: "2024-01-26T07:19:03Z"
      lastTransitionTime: "2024-01-26T07:19:03Z"
      status: "True"
      type: Complete
    ready: 0
    startTime: "2024-01-26T07:18:57Z"
    succeeded: 1
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/job-tracking: ""
    creationTimestamp: "2024-03-25T09:16:59Z"
    generation: 1
    labels:
      app: rook-ceph-osd-prepare
      ceph-version: 17.2.6-0
      rook-version: v1.13.1
      rook_cluster: rook-ceph
    name: rook-ceph-osd-prepare-worker-1
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 02274ff6-97d3-4d09-b941-31edb9bdad99
    resourceVersion: "16380841"
    uid: 09eabab4-05b4-4416-9540-2e62b38a70af
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    parallelism: 1
    selector:
      matchLabels:
        controller-uid: 09eabab4-05b4-4416-9540-2e62b38a70af
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd-prepare
          ceph.rook.io/pvc: ""
          controller-uid: 09eabab4-05b4-4416-9540-2e62b38a70af
          job-name: rook-ceph-osd-prepare-worker-1
          rook_cluster: rook-ceph
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - ceph
          - osd
          - provision
          command:
          - /rook/rook
          env:
          - name: ROOK_NODE_NAME
            value: worker-1
          - name: ROOK_CLUSTER_ID
            value: 02274ff6-97d3-4d09-b941-31edb9bdad99
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: ROOK_OSD_STORE_TYPE
            value: bluestore
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: worker-1
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: ROOK_LOG_LEVEL
            value: DEBUG
          - name: ROOK_DATA_DEVICE_FILTER
            value: ^sd.
          - name: ROOK_CEPH_VERSION
            value: ceph version 17.2.6-0 quincy
          - name: ROOK_OSD_CRUSH_DEVICE_CLASS
          - name: ROOK_OSD_CRUSH_INITIAL_WEIGHT
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: provision
          resources:
            requests:
              cpu: 500m
              memory: 50Mi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsNonRoot: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: ceph-conf-emptydir
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: udev
          - mountPath: /rook
            name: rook-binaries
          - mountPath: /var/lib/rook-ceph-mon
            name: ceph-admin-secret
            readOnly: true
          - mountPath: /rootfs
            name: rootfs
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --archive
          - --force
          - --verbose
          - /usr/local/bin/rook
          - /rook
          command:
          - cp
          image: rook/ceph:v1.13.1
          imagePullPolicy: IfNotPresent
          name: copy-bins
          resources:
            requests:
              cpu: 500m
              memory: 50Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /rook
            name: rook-binaries
        nodeSelector:
          kubernetes.io/hostname: worker-1
        priorityClassName: system-node-critical
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - emptyDir: {}
          name: ceph-conf-emptydir
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: rook-binaries
        - hostPath:
            path: /run/udev
            type: ""
          name: udev
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - name: ceph-admin-secret
          secret:
            defaultMode: 420
            items:
            - key: ceph-secret
              path: secret.keyring
            secretName: rook-ceph-mon
        - hostPath:
            path: /
            type: ""
          name: rootfs
  status:
    completionTime: "2024-03-25T09:17:07Z"
    conditions:
    - lastProbeTime: "2024-03-25T09:17:07Z"
      lastTransitionTime: "2024-03-25T09:17:07Z"
      status: "True"
      type: Complete
    ready: 0
    startTime: "2024-03-25T09:16:59Z"
    succeeded: 1
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/job-tracking: ""
    creationTimestamp: "2024-03-25T09:16:59Z"
    generation: 1
    labels:
      app: rook-ceph-osd-prepare
      ceph-version: 17.2.6-0
      rook-version: v1.13.1
      rook_cluster: rook-ceph
    name: rook-ceph-osd-prepare-worker-2
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 02274ff6-97d3-4d09-b941-31edb9bdad99
    resourceVersion: "16380893"
    uid: 8a24f70b-fecd-41c1-9238-e046792991a0
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    parallelism: 1
    selector:
      matchLabels:
        controller-uid: 8a24f70b-fecd-41c1-9238-e046792991a0
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd-prepare
          ceph.rook.io/pvc: ""
          controller-uid: 8a24f70b-fecd-41c1-9238-e046792991a0
          job-name: rook-ceph-osd-prepare-worker-2
          rook_cluster: rook-ceph
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - ceph
          - osd
          - provision
          command:
          - /rook/rook
          env:
          - name: ROOK_NODE_NAME
            value: worker-2
          - name: ROOK_CLUSTER_ID
            value: 02274ff6-97d3-4d09-b941-31edb9bdad99
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: ROOK_OSD_STORE_TYPE
            value: bluestore
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: worker-2
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: ROOK_LOG_LEVEL
            value: DEBUG
          - name: ROOK_DATA_DEVICE_FILTER
            value: ^sd.
          - name: ROOK_CEPH_VERSION
            value: ceph version 17.2.6-0 quincy
          - name: ROOK_OSD_CRUSH_DEVICE_CLASS
          - name: ROOK_OSD_CRUSH_INITIAL_WEIGHT
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: provision
          resources:
            requests:
              cpu: 500m
              memory: 50Mi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsNonRoot: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: ceph-conf-emptydir
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: udev
          - mountPath: /rook
            name: rook-binaries
          - mountPath: /var/lib/rook-ceph-mon
            name: ceph-admin-secret
            readOnly: true
          - mountPath: /rootfs
            name: rootfs
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --archive
          - --force
          - --verbose
          - /usr/local/bin/rook
          - /rook
          command:
          - cp
          image: rook/ceph:v1.13.1
          imagePullPolicy: IfNotPresent
          name: copy-bins
          resources:
            requests:
              cpu: 500m
              memory: 50Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /rook
            name: rook-binaries
        nodeSelector:
          kubernetes.io/hostname: worker-2
        priorityClassName: system-node-critical
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - emptyDir: {}
          name: ceph-conf-emptydir
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: rook-binaries
        - hostPath:
            path: /run/udev
            type: ""
          name: udev
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - name: ceph-admin-secret
          secret:
            defaultMode: 420
            items:
            - key: ceph-secret
              path: secret.keyring
            secretName: rook-ceph-mon
        - hostPath:
            path: /
            type: ""
          name: rootfs
  status:
    completionTime: "2024-03-25T09:17:12Z"
    conditions:
    - lastProbeTime: "2024-03-25T09:17:12Z"
      lastTransitionTime: "2024-03-25T09:17:12Z"
      status: "True"
      type: Complete
    ready: 0
    startTime: "2024-03-25T09:16:59Z"
    succeeded: 1
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/job-tracking: ""
    creationTimestamp: "2024-03-25T09:17:00Z"
    generation: 1
    labels:
      app: rook-ceph-osd-prepare
      ceph-version: 17.2.6-0
      rook-version: v1.13.1
      rook_cluster: rook-ceph
    name: rook-ceph-osd-prepare-worker-3
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 02274ff6-97d3-4d09-b941-31edb9bdad99
    resourceVersion: "16380901"
    uid: d5b4f8b6-4ded-47ee-8bf3-25296a1deff2
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    parallelism: 1
    selector:
      matchLabels:
        controller-uid: d5b4f8b6-4ded-47ee-8bf3-25296a1deff2
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd-prepare
          ceph.rook.io/pvc: ""
          controller-uid: d5b4f8b6-4ded-47ee-8bf3-25296a1deff2
          job-name: rook-ceph-osd-prepare-worker-3
          rook_cluster: rook-ceph
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - ceph
          - osd
          - provision
          command:
          - /rook/rook
          env:
          - name: ROOK_NODE_NAME
            value: worker-3
          - name: ROOK_CLUSTER_ID
            value: 02274ff6-97d3-4d09-b941-31edb9bdad99
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: ROOK_OSD_STORE_TYPE
            value: bluestore
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: worker-3
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: ROOK_LOG_LEVEL
            value: DEBUG
          - name: ROOK_DATA_DEVICE_FILTER
            value: ^sd.
          - name: ROOK_CEPH_VERSION
            value: ceph version 17.2.6-0 quincy
          - name: ROOK_OSD_CRUSH_DEVICE_CLASS
          - name: ROOK_OSD_CRUSH_INITIAL_WEIGHT
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: provision
          resources:
            requests:
              cpu: 500m
              memory: 50Mi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsNonRoot: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: ceph-conf-emptydir
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: udev
          - mountPath: /rook
            name: rook-binaries
          - mountPath: /var/lib/rook-ceph-mon
            name: ceph-admin-secret
            readOnly: true
          - mountPath: /rootfs
            name: rootfs
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --archive
          - --force
          - --verbose
          - /usr/local/bin/rook
          - /rook
          command:
          - cp
          image: rook/ceph:v1.13.1
          imagePullPolicy: IfNotPresent
          name: copy-bins
          resources:
            requests:
              cpu: 500m
              memory: 50Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /rook
            name: rook-binaries
        nodeSelector:
          kubernetes.io/hostname: worker-3
        priorityClassName: system-node-critical
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - emptyDir: {}
          name: ceph-conf-emptydir
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: rook-binaries
        - hostPath:
            path: /run/udev
            type: ""
          name: udev
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - name: ceph-admin-secret
          secret:
            defaultMode: 420
            items:
            - key: ceph-secret
              path: secret.keyring
            secretName: rook-ceph-mon
        - hostPath:
            path: /
            type: ""
          name: rootfs
  status:
    completionTime: "2024-03-25T09:17:13Z"
    conditions:
    - lastProbeTime: "2024-03-25T09:17:13Z"
      lastTransitionTime: "2024-03-25T09:17:13Z"
      status: "True"
      type: Complete
    ready: 0
    startTime: "2024-03-25T09:17:00Z"
    succeeded: 1
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/job-tracking: ""
    creationTimestamp: "2024-03-25T09:17:04Z"
    generation: 1
    labels:
      app: rook-ceph-osd-prepare
      ceph-version: 17.2.6-0
      rook-version: v1.13.1
      rook_cluster: rook-ceph
    name: rook-ceph-osd-prepare-worker-4
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 02274ff6-97d3-4d09-b941-31edb9bdad99
    resourceVersion: "16380939"
    uid: a0de7042-62d3-4acf-9fb9-2b5143a4a700
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    parallelism: 1
    selector:
      matchLabels:
        controller-uid: a0de7042-62d3-4acf-9fb9-2b5143a4a700
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd-prepare
          ceph.rook.io/pvc: ""
          controller-uid: a0de7042-62d3-4acf-9fb9-2b5143a4a700
          job-name: rook-ceph-osd-prepare-worker-4
          rook_cluster: rook-ceph
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - ceph
          - osd
          - provision
          command:
          - /rook/rook
          env:
          - name: ROOK_NODE_NAME
            value: worker-4
          - name: ROOK_CLUSTER_ID
            value: 02274ff6-97d3-4d09-b941-31edb9bdad99
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: ROOK_OSD_STORE_TYPE
            value: bluestore
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: worker-4
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: ROOK_LOG_LEVEL
            value: DEBUG
          - name: ROOK_DATA_DEVICE_FILTER
            value: ^sd.
          - name: ROOK_CEPH_VERSION
            value: ceph version 17.2.6-0 quincy
          - name: ROOK_OSD_CRUSH_DEVICE_CLASS
          - name: ROOK_OSD_CRUSH_INITIAL_WEIGHT
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v17.2.6
          imagePullPolicy: IfNotPresent
          name: provision
          resources:
            requests:
              cpu: 500m
              memory: 50Mi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsNonRoot: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: ceph-conf-emptydir
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: udev
          - mountPath: /rook
            name: rook-binaries
          - mountPath: /var/lib/rook-ceph-mon
            name: ceph-admin-secret
            readOnly: true
          - mountPath: /rootfs
            name: rootfs
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --archive
          - --force
          - --verbose
          - /usr/local/bin/rook
          - /rook
          command:
          - cp
          image: rook/ceph:v1.13.1
          imagePullPolicy: IfNotPresent
          name: copy-bins
          resources:
            requests:
              cpu: 500m
              memory: 50Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /rook
            name: rook-binaries
        nodeSelector:
          kubernetes.io/hostname: worker-4
        priorityClassName: system-node-critical
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - emptyDir: {}
          name: ceph-conf-emptydir
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: rook-binaries
        - hostPath:
            path: /run/udev
            type: ""
          name: udev
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - name: ceph-admin-secret
          secret:
            defaultMode: 420
            items:
            - key: ceph-secret
              path: secret.keyring
            secretName: rook-ceph-mon
        - hostPath:
            path: /
            type: ""
          name: rootfs
  status:
    completionTime: "2024-03-25T09:17:20Z"
    conditions:
    - lastProbeTime: "2024-03-25T09:17:20Z"
      lastTransitionTime: "2024-03-25T09:17:20Z"
      status: "True"
      type: Complete
    ready: 0
    startTime: "2024-03-25T09:17:04Z"
    succeeded: 1
    uncountedTerminatedPods: {}
kind: List
metadata:
  resourceVersion: ""
